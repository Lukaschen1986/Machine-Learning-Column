{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U --user datasets accelerate peft trl tensorboard bitsandbytes langchain sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "# import torch_npu as th_npu\n",
    "import transformers\n",
    "\n",
    "from pprint import pp\n",
    "from datasets import (load_dataset, load_from_disk, Dataset)\n",
    "from transformers import (AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoModel, \n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, \n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForSeq2Seq, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from peft import (LoraConfig, get_peft_model, PeftModel, TaskType, get_peft_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "torch version = 2.5.1+cu121\n",
      "cuda version = 12.1\n",
      "transformers version = 4.49.0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# device = th.device(\"npu\" if th.npu.is_available() else \"cpu\")\n",
    "devive_cnt = th.cuda.device_count()\n",
    "# devive_cnt = th.npu.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(f\"torch version = {th.__version__}\")\n",
    "print(f\"cuda version = {th.version.cuda}\")\n",
    "print(f\"transformers version = {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_model = \"F:/LLM\"\n",
    "path_output = os.path.join(os.path.dirname(path_project), \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"alpaca/train-00000-of-00001-a09b74b3ef9c3b56.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"parquet\",\n",
    "    data_files=os.path.join(path_data, filename),\n",
    "    split=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(2000))  # 预研\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0)\n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'List 3 possible reasons why the given website is not '\n",
      "                'performing as expected.',\n",
      " 'input': 'A website for an e-commerce store',\n",
      " 'output': '1. The website has a slow loading time. \\n'\n",
      "           '2. The website has a weak user interface and design. \\n'\n",
      "           '3. The website is lacking in SEO optimization.',\n",
      " 'text': 'Below is an instruction that describes a task, paired with an input '\n",
      "         'that provides further context. Write a response that appropriately '\n",
      "         'completes the request.\\n'\n",
      "         '\\n'\n",
      "         '### Instruction:\\n'\n",
      "         'List 3 possible reasons why the given website is not performing as '\n",
      "         'expected.\\n'\n",
      "         '\\n'\n",
      "         '### Input:\\n'\n",
      "         'A website for an e-commerce store\\n'\n",
      "         '\\n'\n",
      "         '### Response:\\n'\n",
      "         '1. The website has a slow loading time. \\n'\n",
      "         '2. The website has a weak user interface and design. \\n'\n",
      "         '3. The website is lacking in SEO optimization.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n注：这是一个指令微调数据集，要用作预训练可以只对 output 做预研\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(train_dataset[2])\n",
    "'''\n",
    "注：这是一个指令微调数据集，要用作预训练可以只对 output 做预研\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Qwen/Qwen2.5-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|im_start|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bos_token = <|im_start|>'\n",
      "'eos_token = <|im_end|>'\n",
      "'pad_token = <|endoftext|>'\n",
      "'padding_side = right'\n"
     ]
    }
   ],
   "source": [
    "pp(f\"bos_token = {tokenizer.bos_token}\")\n",
    "pp(f\"eos_token = {tokenizer.eos_token}\")\n",
    "pp(f\"pad_token = {tokenizer.pad_token}\")\n",
    "pp(f\"padding_side = {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: 量化参数（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")  # QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-4: 载入基模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=th.bfloat16,\n",
    "    # attn_implementation=\"sdpa\",  # flash_attention_2, sdpa\n",
    "    # quantization_config=config_bnb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, parm) in enumerate(base_model.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "if th.cuda.device_count() > 1:\n",
    "    base_model.is_parallelizable = True\n",
    "    base_model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已分配的GPU内存：0.93G, 已缓存的GPU内存：0.97G\n"
     ]
    }
   ],
   "source": [
    "allocated_memory = th.cuda.memory_allocated()\n",
    "cached_memory = th.cuda.memory_cached()\n",
    "print(f\"已分配的GPU内存：{allocated_memory / 1024**3:.2f}G, 已缓存的GPU内存：{cached_memory / 1024**3:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_size = len(tokenizer)\n",
    "embedding_size = base_model.get_input_embeddings().weight.shape[0]\n",
    "if tokenizer_size > embedding_size:\n",
    "    base_model.resize_token_embeddings(tokenizer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-5: 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"rank\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"use_rslora\": True,\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_steps\": 1,\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_seq_length\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-6: LoRA参数（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pp(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA: Low-Rank Adaptation of Large Language Models\n",
    "# config_lora = LoraConfig(target_modules=[\"0\"])\n",
    "# config_lora = LoraConfig(target_modules=[\"query_key_value\", \"dense_4h_to_h\"])\n",
    "# config_lora = LoraConfig(target_modules=[\".*\\.1.*query_key_value\"])\n",
    "# config_lora = LoraConfig(target_modules=[\"query_key_value\"], modules_to_save=[\"word_embeddings\"])\n",
    "config_lora = LoraConfig(\n",
    "    r=config_model.get(\"rank\"),\n",
    "    lora_alpha=config_model.get(\"lora_alpha\"),\n",
    "    lora_dropout=config_model.get(\"lora_dropout\"),\n",
    "    use_rslora=config_model.get(\"use_rslora\"),\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        # self_attn\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        # mlp\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        # lm_head\n",
    "        # \"lm_head\"\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model=base_model, peft_config=config_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_trainable_parameters - 1\n",
    "# print(model_lora.print_trainable_parameters())\n",
    "\n",
    "# print_trainable_parameters - 2\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for param in lora_model.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "    all_params += param.numel()\n",
    "\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peft_model_state_dict(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-7: 整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对 PT，进行数据结构整理; SFT 使用 tokenizer.apply_chat_template\n",
    "def apply_pretrain_template(sample):\n",
    "    sample[\"output\"] += tokenizer.pad_token\n",
    "    return sample\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    inputs = tokenizer(text=sample[\"output\"], max_length=128, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0955c8ead43119a88e6b3b03f4ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848886c1bc9142229ea4eda1b82f2f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(apply_pretrain_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c240148377694481acf1858bc0e37beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72680bf938d442ecae4194031246656e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_t = dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_t = dataset_t[\"train\"]\n",
    "test_dataset_t = dataset_t[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-8: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train = TrainingArguments(\n",
    "    output_dir=os.path.join(path_output, \"model_pt\"),\n",
    "    num_train_epochs=config_model.get(\"epochs\"),\n",
    "    per_device_train_batch_size=config_model.get(\"batch_size\"),\n",
    "    per_device_eval_batch_size=config_model.get(\"batch_size\"),\n",
    "    gradient_accumulation_steps=config_model.get(\"gradient_steps\"),\n",
    "    gradient_checkpointing=True, \n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=config_model.get(\"learning_rate\"),\n",
    "    weight_decay=config_model.get(\"weight_decay\"),\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer, mlm=False) \n",
    "# collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "# collate_fn = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "# collate_fn = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args_train,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset_t,\n",
    "    eval_dataset=test_dataset_t,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-9: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_result = trainer.evaluate()\n",
    "# testing_result = trainer.evaluate(dataset_test)\n",
    "pp(evaluating_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-10: 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - 使用 Trainer 训练时保存整个训练模型(包含训练状态（模型权重、配置文件、优化器等）)\n",
    "trainer.save_model(output_dir=os.path.join(path_output, \"model_pt_1\"))\n",
    "\n",
    "# 2 - 通常用于非 Trainer 环境下保存模型(只保存模型权重、配置文件和分词器等)\n",
    "base_model.save_pretrained(save_directory=os.path.join(path_output, \"model_pt_2\"), max_shard_size=\"4GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
