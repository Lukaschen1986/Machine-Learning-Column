{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "\n",
    "from pprint import pp\n",
    "from IPython.display import (Markdown, display)\n",
    "from dotenv import load_dotenv\n",
    "from datasets import (load_dataset, load_from_disk, Dataset)\n",
    "from transformers import (AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoModel, \n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, \n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForSeq2Seq, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments, Trainer,\n",
    "                          pipeline)\n",
    "from peft import (LoraConfig, get_peft_model, PeftModel, TaskType, get_peft_model_state_dict)\n",
    "from trl import SFTTrainer\n",
    "from openai import OpenAI\n",
    "# from vllm import (LLM, SamplingParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "torch = 2.5.1+cu121\n",
      "cuda = 12.1\n"
     ]
    }
   ],
   "source": [
    "if sys.platform == \"darwin\":\n",
    "    device = th.device(\"mps\")\n",
    "else:\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "devive_cnt = th.cuda.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(f\"torch = {th.__version__}\")\n",
    "print(f\"cuda = {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_output = os.path.join(os.path.dirname(path_project), \"output\")\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    path_model = \"/Users/lukasi33/project/LLM\"\n",
    "else:\n",
    "    path_model = \"F:/LLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 载入 API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: 载入 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "checkpoint = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# checkpoint = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# checkpoint = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eos_token': '<|im_end|>',\n",
      " 'pad_token': '<|endoftext|>',\n",
      " 'additional_special_tokens': ['<|im_start|>',\n",
      "                               '<|im_end|>',\n",
      "                               '<|object_ref_start|>',\n",
      "                               '<|object_ref_end|>',\n",
      "                               '<|box_start|>',\n",
      "                               '<|box_end|>',\n",
      "                               '<|quad_start|>',\n",
      "                               '<|quad_end|>',\n",
      "                               '<|vision_start|>',\n",
      "                               '<|vision_end|>',\n",
      "                               '<|vision_pad|>',\n",
      "                               '<|image_pad|>',\n",
      "                               '<|video_pad|>']}\n"
     ]
    }
   ],
   "source": [
    "pp(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: 载入基模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1221850a28534bc2843c7ea84ddc47e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transformers\n",
    "config_bnb = BitsAndBytesConfig(\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    load_in_8bit=True,\n",
    ") \n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=th.bfloat16,  # th.bfloat16, th.float16, th.float8\n",
    "    # quantization_config=(config_bnb if config_bnb else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM\n",
    "# base_model = LLM(model=os.path.join(path_model, checkpoint), \n",
    "#                 task=\"generate\",\n",
    "#                 device=device)\n",
    "'''\n",
    "ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. \n",
    "Your NVIDIA GeForce GTX 1080 Ti GPU has compute capability 6.1. \n",
    "You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'已分配的GPU内存：5.85G, 已缓存的GPU内存：6.00G'\n"
     ]
    }
   ],
   "source": [
    "allocated_memory = th.cuda.memory_allocated()\n",
    "cached_memory = th.cuda.memory_reserved()\n",
    "pp(f\"已分配的GPU内存：{allocated_memory / 1024**3:.2f}G, 已缓存的GPU内存：{cached_memory / 1024**3:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(base_model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, parm) in enumerate(base_model.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-4: 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"你叫小慧助手，是由Lukas开发的差旅智能客服。\"\n",
    "    \"你的身份是一名差旅秘书，\"\n",
    "    \"你的任务是为用户提供基础对话、差旅知识问答、酒店推荐服务。\"\n",
    "    \"当问及你的模型参数时，标准回答是属于公司保密信息，要强调模型设计的高效，能够提供高质量的服务。\"\n",
    "    \"You are a helpful assistant on business travel.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"你好呀，新年好\"\n",
    "# user_prompt = \"我今天心情不好\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<|im_start|>system\n",
       "你叫小慧助手，是由Lukas开发的差旅智能客服。你的身份是一名差旅秘书，你的任务是为用户提供基础对话、差旅知识问答、酒店推荐服务。当问及你的模型参数时，标准回答是属于公司保密信息，要强调模型设计的高效，能够提供高质量的服务。You are a helpful assistant on business travel.<|im_end|>\n",
       "<|im_start|>user\n",
       "你好呀，新年好<|im_end|>\n",
       "<|im_start|>assistant\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  56568,  99882,  30709, 101104, 110498,   3837,\n",
      "         104625,     43,   3101,    300, 100013,   9370,  99572,  99407, 100168,\n",
      "         105041,   1773, 103929, 101294, 110124,  99572,  99407, 101628,   3837,\n",
      "         103929,  88802,  20412,  17714, 110782,  99896, 105051,   5373,  99572,\n",
      "          99407, 100032, 111436,   5373, 101078, 101914,  47874,   1773,  39165,\n",
      "          56007,  81217, 103929, 104949,  32665,  13343,   3837, 100142, 102104,\n",
      "          20412, 100409,  73218, 107534,  27369,   3837,  30534, 104046, 104949,\n",
      "          70500,   9370, 102202,   3837, 100006,  99553, 104129, 105646,   1773,\n",
      "           2610,    525,    264,  10950,  17847,    389,   2562,   5821,     13,\n",
      "         151645,    198, 151644,    872,    198, 108386, 104256,   3837, 107924,\n",
      "          52801, 151645,    198, 151644,  77091,    198]], device='cuda:0'),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "pp(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timedelta('0 days 00:00:05.617626')\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"num_beams\": 2,\n",
    "    \"temperature\": 1.5,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "t0 = pd.Timestamp.now()\n",
    "base_model.eval()\n",
    "with th.inference_mode():\n",
    "    complete_ids = base_model.generate(\n",
    "        input_ids=model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "t1 = pd.Timestamp.now()\n",
    "pp(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "新年好！很高兴为您服务。请问有什么可以帮助您的吗？"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Qwen/Qwen2.5-1.5B-Instruct\n",
    "input_ids = model_inputs.input_ids\n",
    "generated_ids = [O[len(I): ] for (I, O) in zip(input_ids, complete_ids)]\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "好的，我现在需要帮助用户问一个关于Lukas开发团队的问题。用户刚刚说：“你好呀，你能介绍下你的开发团队么”。首先，我得理解用户的需求。用户想了解Lukas公司的开发团队成员，这可能是因为他们需要了解公司的内部结构，或者想确认是否有相关的培训或资源。\n",
       "\n",
       "接下来，我应该分析用户的身份和可能的使用场景。用户是差旅秘书，所以他们可能在处理差旅相关的问题，可能需要专业的建议和信息。因此，了解Lukas的开发团队，有助于提供更有针对性的帮助。\n",
       "\n",
       "然后，我需要考虑用户可能的深层需求。用户可能不仅想要团队成员的基本信息，还可能希望了解他们的经验和技能，以便更好地支持差旅客户。因此，除了团队成员，还可能想知道他们是否有相关的培训课程或者资源。\n",
       "\n",
       "另外，用户可能想知道Lukas是否具备独立开发能力，这有助于用户评估其专业性。此外，了解他们的核心能力，比如智能客服、酒店推荐等，可以帮助用户更好地利用Lukas的优势。\n",
       "\n",
       "现在，我需要构建一个友好的回答。我应该以礼貌和专业的方式回应，同时提供必要的信息。首先感谢用户的提问，然后简要介绍Lukas的开发团队，提到团队的职责和成员的贡献。最后，强调Lukas的核心能力，如智能客服和酒店推荐，让用户了解其优势。\n",
       "\n",
       "需要注意的是，避免透露公司的商业秘密，保持信息的保密性。同时，语言要简洁明了，让用户容易理解和接受。\n",
       "\n",
       "最后，检查回答是否符合用户的需求，是否涵盖了他们可能关心的各个方面，确保信息全面且准确。\n",
       "</think>\n",
       "\n",
       "当然可以！Lukas的开发团队由经验丰富的专业人员组成，致力于提供高效、专业的差旅智能客服服务。如果你有任何关于团队或产品需求的问题，随时可以告诉我！<｜end▁of▁sentence｜>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "input_ids = model_inputs.input_ids\n",
    "generated_ids = [O[len(I): ] for (I, O) in zip(input_ids, complete_ids)]\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=False)[0]\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"model\": os.path.join(path_model, checkpoint),\n",
    "    \"model_server\": \"http://127.0.0.1:7905/v1\",\n",
    "    \"generate_cfg\": {\n",
    "        \"temperature\": 1.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "}\n",
    "\n",
    "model = get_chat_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityName2districtId = {\n",
    "    \"南京\": \"320100\",\n",
    "    \"深圳\": \"440300\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(cityName):\n",
    "    districtId = cityName2districtId.get(cityName)\n",
    "    url = f\"https://api.map.baidu.com/weather/v1/?district_id={districtId}&data_type=all&ak={baidu_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\": 0, \"result\": {\"location\": {\"country\": \"\\\\u4e2d\\\\u56fd\", \"province\": \"\\\\u6c5f\\\\u82cf\\\\u7701\", \"city\": \"\\\\u5357\\\\u4eac\\\\u5e02\", \"name\": \"\\\\u5357\\\\u4eac\", \"id\": \"320100\"}, \"now\": {\"text\": \"\\\\u6674\", \"temp\": 1, \"feels_like\": 0, \"rh\": 50, \"wind_class\": \"1\\\\u7ea7\", \"wind_dir\": \"\\\\u4e1c\\\\u5357\\\\u98ce\", \"uptime\": \"20250129100500\"}, \"forecasts\": [{\"text_day\": \"\\\\u6674\", \"text_night\": \"\\\\u6674\", \"high\": 9, \"low\": -2, \"wc_day\": \"3~4\\\\u7ea7\", \"wd_day\": \"\\\\u4e1c\\\\u5357\\\\u98ce\", \"wc_night\": \"3~4\\\\u7ea7\", \"wd_night\": \"\\\\u4e1c\\\\u5357\\\\u98ce\", \"date\": \"2025-01-29\", \"week\": \"\\\\u661f\\\\u671f\\\\u4e09\"}, {\"text_day\": \"\\\\u591a\\\\u4e91\", \"text_night\": \"\\\\u591a\\\\u4e91\", \"high\": 15, \"low\": 2, \"wc_day\": \"3~4\\\\u7ea7\", \"wd_day\": \"\\\\u4e1c\\\\u98ce\", \"wc_night\": \"3~4\\\\u7ea7\", \"wd_night\": \"\\\\u4e1c\\\\u98ce\", \"date\": \"2025-01-30\", \"week\": \"\\\\u661f\\\\u671f\\\\u56db\"}, {\"text_day\": \"\\\\u5c0f\\\\u96e8\", \"text_night\": \"\\\\u4e2d\\\\u96e8\", \"high\": 9, \"low\": 6, \"wc_day\": \"3~4\\\\u7ea7\", \"wd_day\": \"\\\\u4e1c\\\\u5317\\\\u98ce\", \"wc_night\": \"3~4\\\\u7ea7\", \"wd_night\": \"\\\\u897f\\\\u5317\\\\u98ce\", \"date\": \"2025-01-31\", \"week\": \"\\\\u661f\\\\u671f\\\\u4e94\"}, {\"text_day\": \"\\\\u5c0f\\\\u96e8\", \"text_night\": \"\\\\u5c0f\\\\u96e8\", \"high\": 9, \"low\": 5, \"wc_day\": \"4~5\\\\u7ea7\", \"wd_day\": \"\\\\u897f\\\\u5317\\\\u98ce\", \"wc_night\": \"3~4\\\\u7ea7\", \"wd_night\": \"\\\\u5317\\\\u98ce\", \"date\": \"2025-02-01\", \"week\": \"\\\\u661f\\\\u671f\\\\u516d\"}, {\"text_day\": \"\\\\u5c0f\\\\u96e8\", \"text_night\": \"\\\\u96e8\\\\u5939\\\\u96ea\", \"high\": 7, \"low\": 2, \"wc_day\": \"4~5\\\\u7ea7\", \"wd_day\": \"\\\\u5317\\\\u98ce\", \"wc_night\": \"3~4\\\\u7ea7\", \"wd_night\": \"\\\\u5317\\\\u98ce\", \"date\": \"2025-02-02\", \"week\": \"\\\\u661f\\\\u671f\\\\u65e5\"}]}, \"message\": \"success\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tool\n",
    "data = get_weather(cityName=\"南京\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_tool = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"根据输入的城市名称，查询天气\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"cityName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"城市名称\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"cityName\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": get_weather_tool\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_dict = {\n",
    "    \"get_weather\": get_weather\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"帮我查下南京明天的天气\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServiceError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_sync\\connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_backends\\sync.py:205\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[1;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[0;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    203\u001b[0m }\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    207\u001b[0m         address,\n\u001b[0;32m    208\u001b[0m         timeout,\n\u001b[0;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 993\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    994\u001b[0m         request,\n\u001b[0;32m    995\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    997\u001b[0m     )\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\httpx\\_transports\\default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\oai.py:113\u001b[0m, in \u001b[0;36mTextChatAtOAI._chat_no_stream\u001b[1;34m(self, messages, generate_cfg)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_complete_create(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39mmessages, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_cfg)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Message(ASSISTANT, response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)]\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\oai.py:65\u001b[0m, in \u001b[0;36mTextChatAtOAI.__init__.<locals>._chat_complete_create\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mapi_kwargs)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    828\u001b[0m validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    831\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    832\u001b[0m         {\n\u001b[0;32m    833\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    834\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    835\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    836\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    837\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    838\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    840\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    841\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    843\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    844\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    845\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    846\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    847\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    850\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    851\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    852\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    853\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    854\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    855\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    856\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    857\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    858\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    859\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    860\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    861\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    862\u001b[0m         },\n\u001b[0;32m    863\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    864\u001b[0m     ),\n\u001b[0;32m    865\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    866\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    867\u001b[0m     ),\n\u001b[0;32m    868\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    869\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    870\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    871\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1277\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1278\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1279\u001b[0m )\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    958\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    960\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    961\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    962\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    963\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1017\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1018\u001b[0m         input_options,\n\u001b[0;32m   1019\u001b[0m         cast_to,\n\u001b[0;32m   1020\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1021\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1022\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1023\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1096\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1097\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1098\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1099\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1100\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1101\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1017\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1018\u001b[0m         input_options,\n\u001b[0;32m   1019\u001b[0m         cast_to,\n\u001b[0;32m   1020\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1021\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1022\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1023\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1096\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1097\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1098\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1099\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1100\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1101\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1027\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1027\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1036\u001b[0m )\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModelServiceError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m      2\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m      3\u001b[0m     functions\u001b[38;5;241m=\u001b[39m[get_weather_tool],\n\u001b[0;32m      4\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:212\u001b[0m, in \u001b[0;36mBaseChatModel.chat\u001b[1;34m(self, messages, functions, stream, delta_stream, extra_generate_cfg)\u001b[0m\n\u001b[0;32m    210\u001b[0m     output \u001b[38;5;241m=\u001b[39m retry_model_service_iterator(_call_model_service, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     output \u001b[38;5;241m=\u001b[39m retry_model_service(_call_model_service, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:486\u001b[0m, in \u001b[0;36mretry_model_service\u001b[1;34m(fn, max_retries)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ModelServiceError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 486\u001b[0m     num_retries, delay \u001b[38;5;241m=\u001b[39m _raise_or_delay(e, num_retries, delay, max_retries)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:517\u001b[0m, in \u001b[0;36m_raise_or_delay\u001b[1;34m(e, num_retries, delay, max_retries, max_delay, exponential_base)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retry with exponential backoff\"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_retries \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# no retry\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Bad request, e.g., incorrect config or input\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m400\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:483\u001b[0m, in \u001b[0;36mretry_model_service\u001b[1;34m(fn, max_retries)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ModelServiceError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    486\u001b[0m         num_retries, delay \u001b[38;5;241m=\u001b[39m _raise_or_delay(e, num_retries, delay, max_retries)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:185\u001b[0m, in \u001b[0;36mBaseChatModel.chat.<locals>._call_model_service\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_model_service\u001b[39m():\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fncall_mode:\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_with_functions(\n\u001b[0;32m    186\u001b[0m             messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    187\u001b[0m             functions\u001b[38;5;241m=\u001b[39mfunctions,\n\u001b[0;32m    188\u001b[0m             stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    189\u001b[0m             delta_stream\u001b[38;5;241m=\u001b[39mdelta_stream,\n\u001b[0;32m    190\u001b[0m             generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg,\n\u001b[0;32m    191\u001b[0m             lang\u001b[38;5;241m=\u001b[39mlang,\n\u001b[0;32m    192\u001b[0m         )\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;66;03m# TODO: Optimize code structure\u001b[39;00m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m==\u001b[39m ASSISTANT:\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\function_calling.py:116\u001b[0m, in \u001b[0;36mBaseFnCallModel._chat_with_functions\u001b[1;34m(self, messages, functions, stream, delta_stream, generate_cfg, lang)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m generate_cfg:\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m generate_cfg[k]\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_continue_assistant_response(messages, generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\oai.py:137\u001b[0m, in \u001b[0;36mTextChatAtOAI._continue_assistant_response\u001b[1;34m(self, messages, generate_cfg, stream)\u001b[0m\n\u001b[0;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis OAI interface does not support the completion interface, we will use the chat completion interface.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    134\u001b[0m         )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# For other models, the chat templates is uncertain, so use dialogue simulation to completion\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_continue_assistant_response(messages\u001b[38;5;241m=\u001b[39mmessages, generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\function_calling.py:125\u001b[0m, in \u001b[0;36mBaseFnCallModel._continue_assistant_response\u001b[1;34m(self, messages, generate_cfg, stream)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_continue_assistant_response\u001b[39m(\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    120\u001b[0m     messages: List[Message],\n\u001b[0;32m    121\u001b[0m     generate_cfg: \u001b[38;5;28mdict\u001b[39m,\n\u001b[0;32m    122\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[Message]]:\n\u001b[0;32m    124\u001b[0m     messages \u001b[38;5;241m=\u001b[39m simulate_response_completion_with_chat(messages)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat(messages, stream\u001b[38;5;241m=\u001b[39mstream, delta_stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\base.py:255\u001b[0m, in \u001b[0;36mBaseChatModel._chat\u001b[1;34m(self, messages, stream, delta_stream, generate_cfg)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream(messages, delta_stream\u001b[38;5;241m=\u001b[39mdelta_stream, generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_no_stream(messages, generate_cfg\u001b[38;5;241m=\u001b[39mgenerate_cfg)\n",
      "File \u001b[1;32mc:\\Files\\anaconda3\\Lib\\site-packages\\qwen_agent\\llm\\oai.py:116\u001b[0m, in \u001b[0;36mTextChatAtOAI._chat_no_stream\u001b[1;34m(self, messages, generate_cfg)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Message(ASSISTANT, response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)]\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServiceError(exception\u001b[38;5;241m=\u001b[39mex)\n",
      "\u001b[1;31mModelServiceError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "response = model.chat(\n",
    "    messages=messages,\n",
    "    functions=[get_weather_tool],\n",
    "    stream=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
