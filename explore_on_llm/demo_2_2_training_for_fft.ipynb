{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U --user datasets accelerate peft trl tensorboard bitsandbytes langchain sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "# import torch_npu as th_npu\n",
    "import transformers\n",
    "\n",
    "from pprint import pp\n",
    "from datasets import (load_dataset, load_from_disk, Dataset)\n",
    "from transformers import (AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoModel, \n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, \n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForSeq2Seq, \n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from peft import (LoraConfig, get_peft_model, PeftModel, TaskType, get_peft_model_state_dict)\n",
    "from trl import (SFTConfig, SFTTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "torch version = 2.5.1+cu121\n",
      "cuda version = 12.1\n",
      "transformers version = 4.45.0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# device = th.device(\"npu\" if th.npu.is_available() else \"cpu\")\n",
    "devive_cnt = th.cuda.device_count()\n",
    "# devive_cnt = th.npu.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(f\"torch version = {th.__version__}\")\n",
    "print(f\"cuda version = {th.version.cuda}\")\n",
    "print(f\"transformers version = {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_model = \"F:/LLM\"\n",
    "path_output = os.path.join(path_model, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"alpaca/train-00000-of-00001-a09b74b3ef9c3b56.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"parquet\",\n",
    "    data_files=os.path.join(path_data, filename),\n",
    "    split=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(2000))  # 预研\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0)\n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'List 3 possible reasons why the given website is not '\n",
      "                'performing as expected.',\n",
      " 'input': 'A website for an e-commerce store',\n",
      " 'output': '1. The website has a slow loading time. \\n'\n",
      "           '2. The website has a weak user interface and design. \\n'\n",
      "           '3. The website is lacking in SEO optimization.',\n",
      " 'text': 'Below is an instruction that describes a task, paired with an input '\n",
      "         'that provides further context. Write a response that appropriately '\n",
      "         'completes the request.\\n'\n",
      "         '\\n'\n",
      "         '### Instruction:\\n'\n",
      "         'List 3 possible reasons why the given website is not performing as '\n",
      "         'expected.\\n'\n",
      "         '\\n'\n",
      "         '### Input:\\n'\n",
      "         'A website for an e-commerce store\\n'\n",
      "         '\\n'\n",
      "         '### Response:\\n'\n",
      "         '1. The website has a slow loading time. \\n'\n",
      "         '2. The website has a weak user interface and design. \\n'\n",
      "         '3. The website is lacking in SEO optimization.'}\n"
     ]
    }
   ],
   "source": [
    "pp(train_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Qwen/Qwen2.5-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|im_start|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bos_token = <|im_start|>'\n",
      "'eos_token = <|im_end|>'\n",
      "'pad_token = <|endoftext|>'\n",
      "'padding_side = right'\n"
     ]
    }
   ],
   "source": [
    "pp(f\"bos_token = {tokenizer.bos_token}\")\n",
    "pp(f\"eos_token = {tokenizer.eos_token}\")\n",
    "pp(f\"pad_token = {tokenizer.pad_token}\")\n",
    "pp(f\"padding_side = {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: 量化参数（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")  # QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-4: 载入基模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=th.bfloat16,\n",
    "    # attn_implementation=\"sdpa\",  # flash_attention_2, sdpa\n",
    "    # quantization_config=config_bnb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, parm) in enumerate(base_model.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "if th.cuda.device_count() > 1:\n",
    "    base_model.is_parallelizable = True\n",
    "    base_model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已分配的GPU内存：0.93G, 已缓存的GPU内存：0.97G\n"
     ]
    }
   ],
   "source": [
    "allocated_memory = th.cuda.memory_allocated()\n",
    "cached_memory = th.cuda.memory_cached()\n",
    "print(f\"已分配的GPU内存：{allocated_memory / 1024**3:.2f}G, 已缓存的GPU内存：{cached_memory / 1024**3:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_size = len(tokenizer)\n",
    "embedding_size = base_model.get_input_embeddings().weight.shape[0]\n",
    "if tokenizer_size > embedding_size:\n",
    "    base_model.resize_token_embeddings(tokenizer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-5: 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"use_rslora\": True,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_steps\": 4,\n",
    "    \"learning_rate\": 0.00002,\n",
    "    \"warmup_ratio\": 0.03,  # 3% of steps used for warmup\n",
    "    \"lr_scheduler_type\": \"cosine_with_min_lr\",  # use cosine decay\n",
    "    \"lr_scheduler_kwargs\": {\"min_lr\": 0.000002}, \n",
    "    \"weight_decay\": 0.0,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"packing\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = SFTConfig(\n",
    "    output_dir=os.path.join(path_output, \"model_fft\"),\n",
    "    num_train_epochs=model_config.get(\"epochs\"),\n",
    "    per_device_train_batch_size=model_config.get(\"batch_size\"),\n",
    "    per_device_eval_batch_size=model_config.get(\"batch_size\"),\n",
    "    gradient_accumulation_steps=model_config.get(\"gradient_steps\"),\n",
    "    gradient_checkpointing=False,  # True, False \n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=model_config.get(\"learning_rate\"),\n",
    "    warmup_ratio=model_config.get(\"warmup_ratio\"),  # 预热\n",
    "    lr_scheduler_type=model_config.get(\"lr_scheduler_type\"),  # 退火\n",
    "    lr_scheduler_kwargs=model_config.get(\"lr_scheduler_kwargs\"),  # 退火参数\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    # max_seq_length=model_config.get(\"max_seq_length\"),  # defaults to `1024`\n",
    "    # packing=model_config.get(\"packing\"),  # Whether to pack multiple sequences into a fixed-length format.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-6: LoRA参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不涉及"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-7: 整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Convert the given temperatures from Fahrenheit to Celsius.\n",
      "80°F<|im_end|>\n",
      "<|im_start|>assistant\n",
      "26.67°C<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 例子\n",
    "user_prompt = train_dataset[0][\"instruction\"] + \"\\n\" + train_dataset[0][\"input\"]\n",
    "assistant_prompt = train_dataset[0][\"output\"]\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False  # train\n",
    "    )\n",
    "# text += tokenizer.pad_token  # 初看不需要\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sft_template(sample):\n",
    "    user_prompt = sample[\"instruction\"] + \"\\n\" + sample[\"input\"]\n",
    "    assistant_prompt = sample[\"output\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    sample[\"text\"] = text\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276832fee388404ba4ff3720aad9b914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f134978397ab45cdaad96b941260223a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(apply_sft_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-8: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer, mlm=False) \n",
    "# collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "# collate_fn = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "# collate_fn = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9620d86f81c14ce1b225d76161e9e9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fed038578c41b4af4127cde411f30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fccb931186c46978b0e06a762fd5e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45e2b2159e442838aa63bc2983058d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2705ce4222614ec8929f040c190806c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d177786f00a4434cb735b48394e9ade0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8e58af07f94dbea27933c81181e523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b612923989604c68bd0680fc528af917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-9: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3469866514205933,\n",
      " 'eval_runtime': 10.6115,\n",
      " 'eval_samples_per_second': 18.847,\n",
      " 'eval_steps_per_second': 18.847}\n"
     ]
    }
   ],
   "source": [
    "evaluating_result = trainer.evaluate()\n",
    "# testing_result = trainer.evaluate(dataset_test)\n",
    "pp(evaluating_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-10: 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - 使用 Trainer 训练时保存整个训练模型(包含训练状态（模型权重、配置文件、优化器等）)\n",
    "trainer.save_model(output_dir=os.path.join(path_output, \"model_fft_1\"))\n",
    "\n",
    "# 2 - 通常用于非 Trainer 环境下保存模型(只保存模型权重、配置文件和分词器等)\n",
    "base_model.save_pretrained(save_directory=os.path.join(path_output, \"model_fft_2\"), max_shard_size=\"4GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-11: 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = train_dataset[0][\"instruction\"] + \"\\n\" + train_dataset[0][\"input\"]\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法-1\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True  # inference\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "# model_inputs = tokenizer([text1, text2], return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法-2\n",
    "# model_inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=True,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_dict=True\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens = 1024  # 取训练样本答案的最长值\n",
    "# top_p = 0.9\n",
    "# temperature = 0.1  # 0.5，0.35，0.1，0.01\n",
    "# # repetition_penalty = 1.5\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 1024,  # 取训练样本答案的最长值\n",
    "    \"top_p\": 0.5,\n",
    "    \"temperature\": 0.5,  # 0.5，0.35，0.1，0.01\n",
    "    \"do_sample\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:00.721776\n",
      "The total surface area of a cube is 150 cm².\n"
     ]
    }
   ],
   "source": [
    "# inference 模板\n",
    "t0 = pd.Timestamp.now()\n",
    "base_model.eval()\n",
    "with th.inference_mode():\n",
    "    complete_ids = base_model.generate(\n",
    "        input_ids=model_inputs.input_ids,  # 针对 tokenizer.padding_side\n",
    "        attention_mask=model_inputs.attention_mask,  # 针对 tokenizer.padding_side\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    # also OK\n",
    "    # complete_ids = model_sft.generate(\n",
    "    #     **model_inputs,  # 针对 tokenizer.padding_side\n",
    "    #     **gen_kwargs\n",
    "    # )\n",
    "t1 = pd.Timestamp.now()\n",
    "print(t1 - t0)\n",
    "\n",
    "input_ids = model_inputs.input_ids\n",
    "generated_ids = [O[len(I): ] for (I, O) in zip(input_ids, complete_ids)]\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total surface area of the cube: 150 cm^2'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
