{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name unsloth python=3.11\n",
    "# conda env list\n",
    "# activate unsloth\n",
    "# conda install jupyterlab ipykernel\n",
    "# python -m ipykernel install --user -name unsloth --display-name \"Python unsloth\"\n",
    "# pip install --upgrade --force-reinstall  --no-cache-dir unsloth unsloth_zoo\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18829ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import torch as th\n",
    "# import torch_npu as th_npu\n",
    "\n",
    "from pprint import pp\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import (SFTConfig, SFTTrainer)\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import standardize_sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3974d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "torch version = 2.5.1+cu121\n",
      "cuda version = 12.1\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# device = th.device(\"npu\" if th.npu.is_available() else \"cpu\")\n",
    "devive_cnt = th.cuda.device_count()\n",
    "# devive_cnt = th.npu.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(f\"torch version = {th.__version__}\")\n",
    "print(f\"cuda version = {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cace0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_model = \"F:/LLM\"\n",
    "path_output = os.path.join(path_model, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641ef1d",
   "metadata": {},
   "source": [
    "## step-1: 数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"alpaca/train-00000-of-00001-a09b74b3ef9c3b56.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"parquet\",\n",
    "    data_files=os.path.join(path_data, filename),\n",
    "    split=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(2000))  # 预研\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0)\n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdd588",
   "metadata": {},
   "source": [
    "## step-2~4: tokenizer/量化/载入基模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=os.path.join(path_model, checkpoint),\n",
    "    max_seq_length=2048,\n",
    "    dtype=th.bfloat16,\n",
    "    load_in_4bit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"bos_token\": \"<|im_start|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(f\"bos_token = {tokenizer.bos_token}\")\n",
    "pp(f\"eos_token = {tokenizer.eos_token}\")\n",
    "pp(f\"pad_token = {tokenizer.pad_token}\")\n",
    "pp(f\"padding_side = {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f773e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, parm) in enumerate(base_model.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868828be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()\n",
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbcb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce GTX 1080 Ti. Max memory = 11.0 GB\n",
      "0.0 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = th.cuda.get_device_properties(device)\n",
    "start_memory = round(th.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024**3, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB\")\n",
    "print(f\"{start_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420564a",
   "metadata": {},
   "source": [
    "## step-5: 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = SFTConfig(\n",
    "    output_dir=os.path.join(path_output, \"model_unsloth\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=0.00005,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",  # linear, cosine\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da98bc",
   "metadata": {},
   "source": [
    "## step-6: LoRA参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = FastLanguageModel.get_peft_model(\n",
    "    model=base_model,\n",
    "    r=16,\n",
    "    lora_alpha=32\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # self_attn\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # mlp\n",
    "        \"lm_head\"  # lm_head\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lora_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ff9b7",
   "metadata": {},
   "source": [
    "## step-7: 整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8347c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法-1\n",
    "def apply_sft_template(sample):\n",
    "    user_prompt = sample[\"instruction\"] + \"\\n\" + sample[\"input\"]\n",
    "    assistant_prompt = sample[\"output\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "        enable_thinking=False  # for Qwen3\n",
    "    )\n",
    "    sample[\"text\"] = text\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(apply_sft_template)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebe8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法-2\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48337b3",
   "metadata": {},
   "source": [
    "## step-8: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda676d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer, mlm=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4027ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51581469",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8016d",
   "metadata": {},
   "source": [
    "## step-9: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_result = trainer.evaluate()\n",
    "pp(evaluating_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1017e9",
   "metadata": {},
   "source": [
    "## step-10: 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561209e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained_gguf(\"lora_model_bf16\", tokenizer, quantization_method=\"bf16\")  # q4_k_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1fc81",
   "metadata": {},
   "source": [
    "## step-11: 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec6fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"你好，好久不见！\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a936ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"top_p\": 0.5,\n",
    "    \"temperature\": 0.5,\n",
    "    \"do_sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d653f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.inference_mode():\n",
    "    complete_ids = lora_model.generate(\n",
    "        input_ids=model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        use_cache=True,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    \n",
    "input_ids = model_inputs.input_ids\n",
    "generated_ids = [O[len(I): ] for (I, O) in zip(input_ids, complete_ids)]\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
