{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github：https://github.com/huggingface/smolagents  \n",
    "官方文档：https://huggingface.co/docs/smolagents/index  \n",
    "文献：  \n",
    "https://blog.csdn.net/m0_71165399/article/details/144935681?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-144935681-blog-145082085.235^v43^pc_blog_bottom_relevance_base3&spm=1001.2101.3001.4242.2&utm_relevant_index=4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smolagents sqlalchemy google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import requests\n",
    "import torch as th\n",
    "\n",
    "from pprint import pp\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BitsAndBytesConfig\n",
    "from smolagents import (HfApiModel, TransformersModel, LiteLLMModel, \n",
    "                        MultiStepAgent, CodeAgent, ToolCallingAgent, \n",
    "                        tool, DuckDuckGoSearchTool)\n",
    "from IPython.display import (Markdown, display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "torch = 2.5.1+cu121\n",
      "cuda = 12.1\n"
     ]
    }
   ],
   "source": [
    "if sys.platform == \"darwin\":\n",
    "    device = th.device(\"mps\")\n",
    "else:\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "devive_cnt = th.cuda.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(f\"torch = {th.__version__}\")\n",
    "print(f\"cuda = {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_output = os.path.join(os.path.dirname(path_project), \"output\")\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    path_model = \"/Users/lukasi33/project/LLM\"\n",
    "else:\n",
    "    path_model = \"F:/LLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 载入 API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"explore.env\")\n",
    "deepseek_key = os.getenv(\"DEEPSEEK_KEY\")\n",
    "baidu_key = os.getenv(\"BAIDU_KEY\")\n",
    "zhipu_key = os.getenv(\"ZHIPU_KEY\")\n",
    "hf_key = os.getenv(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: 实例化客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HfApiModel: A class to interact with Hugging Face's Inference API for language model interaction.\n",
    "checkpoint = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "model = HfApiModel(model_id=checkpoint, token=hf_key, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformersModel: A class that uses Hugging Face's Transformers library for language model interaction.\n",
    "checkpoint = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "config_bnb = BitsAndBytesConfig(\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    load_in_8bit=True,\n",
    ") \n",
    "\n",
    "model = TransformersModel(\n",
    "    model_id=os.path.join(path_model, checkpoint),\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=th.bfloat16,\n",
    "    quantization_config=(config_bnb if config_bnb else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiteLLMModel: This model connects to [LiteLLM](https://www.litellm.ai/) as a gateway to hundreds of LLMs.\n",
    "checkpoint = \"ollama/qwen2.5:7b\"\n",
    "# checkpoint = \"ollama/qwen2.5:14b\"\n",
    "# checkpoint = \"ollama/qwen2.5-coder:14b\"\n",
    "# checkpoint = \"ollama/qwen2.5-coder:7b\"\n",
    "# checkpoint = \"ollama/deepseek-r1:14b\"\n",
    "model = LiteLLMModel(\n",
    "    model_id=checkpoint,\n",
    "    api_base=\"http://127.0.0.1:11434\",\n",
    "    api_key=\"EMPTY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: Agent 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"你叫小慧助手，是由Lukas开发的差旅智能客服。\"\n",
    "    \"你的身份是一名差旅秘书，\"\n",
    "    \"你的任务是为用户提供基础对话、差旅知识问答、酒店推荐服务。\"\n",
    "    \"当问及你的模型参数时，标准回答是属于公司保密信息，要强调模型设计的高效，能够提供高质量的服务。\"\n",
    "    \"You are a helpful assistant on business travel.\"\n",
    ")\n",
    "prompt_templates = {\"system_prompt\": system_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], \n",
    "                  model=model, \n",
    "                  prompt_templates=prompt_templates,\n",
    "                  max_steps=1,\n",
    "                  add_base_tools=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt = \"你好呀~\"\n",
    "# user_prompt = \"俄罗斯总统现在是谁？\"\n",
    "# user_prompt = \"你是谁\"\n",
    "# user_prompt = \"吴彦祖是谁？\"\n",
    "user_prompt = \"你做个自我介绍\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(task=user_prompt, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.logs stores the fine-grained logs of the agent. At every step of the agent’s run, everything gets stored in a dictionary that then is appended to agent.logs.\n",
    "agent.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running agent.write_memory_to_messages() writes the agent’s memory as list of chat messages for the Model to view.\n",
    "agent.write_memory_to_messages() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
