# -*- coding: utf-8 -*-
'''
https://huggingface.co/course/zh-CN/chapter2/3?fw=pt
'''
import os
import random
import numpy as np
import torch as th
from torch import nn
from torch.utils.data import random_split
from transformers import (pipeline, 
                          AutoTokenizer, BertTokenizer,
                          AutoModel, BertModel, BertConfig,
                          AutoModelForSequenceClassification, 
                          BertForSequenceClassification,
                          AdamW, AutoConfig)
from transformers import (DataCollatorWithPadding, DataCollatorForLanguageModeling,
                          DataCollatorForSeq2Seq, default_data_collator)
from datasets import (load_dataset, load_from_disk)
from torchcrf import CRF
import torch.optim as optim

device = th.device("cuda" if th.cuda.is_available() else "cpu")

# ----------------------------------------------------------------------------------------------------------------
# è·¯å¾„
path_project = "C:/my_project/MyGit/Machine-Learning-Column/hugging_face"
path_data = os.path.join(os.path.dirname(path_project), "data")
path_model = os.path.join(os.path.dirname(path_project), "model")

# ----------------------------------------------------------------------------------------------------------------
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# 1-ä»é»˜è®¤é…ç½®åˆ›å»ºæ¨¡å‹ä¼šä½¿ç”¨éšæœºå€¼å¯¹å…¶è¿›è¡Œåˆå§‹åŒ–
checkpoint = "bert-base-chinese"

# config = AutoConfig.from_pretrained(
#     pretrained_model_name_or_path=os.path.join(path_model, checkpoint),
#     cache_dir=path_model,
#     force_download=False,
#     local_files_only=False
#     )  # 1
# config = BertConfig()  # 2
# pretrained = BertModel(config)

# 2-åŠ è½½å·²ç»è®­ç»ƒè¿‡çš„Transformersæ¨¡å‹
pretrained = BertModel.from_pretrained(
    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),
    cache_dir=path_model,
    force_download=False,
    local_files_only=False
    )
print(pretrained)
print(pretrained.config)
print(pretrained.config.output_attentions)  # False
print(pretrained.config.output_hidden_states)  # False

for param in pretrained.parameters():
    param.requires_grad_(False)
    
'''
æƒé‡å·²ä¸‹è½½å¹¶ç¼“å­˜åœ¨ç¼“å­˜æ–‡ä»¶å¤¹ä¸­ï¼ˆå› æ­¤å°†æ¥å¯¹from_pretrained()æ–¹æ³•çš„è°ƒç”¨å°†ä¸ä¼šé‡æ–°ä¸‹è½½å®ƒä»¬ï¼‰
é»˜è®¤ä¸º ~/.cache/huggingface/transformers . æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® HF_HOME ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜æ–‡ä»¶å¤¹ã€‚
'''

pretrained.save_pretrained(save_directory=os.path.join(path_model, "my_model_dir"))
'''
å¦‚æœä½ çœ‹ä¸€ä¸‹ config.json æ–‡ä»¶ï¼Œæ‚¨å°†è¯†åˆ«æ„å»ºæ¨¡å‹ä½“ç³»ç»“æ„æ‰€éœ€çš„å±æ€§ã€‚
è¯¥æ–‡ä»¶è¿˜åŒ…å«ä¸€äº›å…ƒæ•°æ®ï¼Œä¾‹å¦‚æ£€æŸ¥ç‚¹çš„æ¥æºä»¥åŠä¸Šæ¬¡ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ä½¿ç”¨çš„ğŸ¤— Transformersç‰ˆæœ¬ã€‚

è¿™ä¸ª pytorch_model.bin æ–‡ä»¶å°±æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„state dictionary; å®ƒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰æƒé‡ã€‚
è¿™ä¸¤ä¸ªæ–‡ä»¶é½å¤´å¹¶è¿›ï¼›é…ç½®æ˜¯äº†è§£æ¨¡å‹ä½“ç³»ç»“æ„æ‰€å¿…éœ€çš„ï¼Œè€Œæ¨¡å‹æƒé‡æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚
'''

# ----------------------------------------------------------------------------------------------------------------
# ç®€å•æ¡ˆä¾‹ï¼šä¸å¸¦Model Headçš„æ¨¡å‹è°ƒç”¨
tokenizer = AutoTokenizer.from_pretrained("rbt3")
sen = "å¼±å°çš„æˆ‘ä¹Ÿæœ‰å¤§æ¢¦æƒ³ï¼"
inputs = tokenizer(sen, return_tensors="pt")

model = AutoModel.from_pretrained("rbt3", output_attentions=True)  # å¼ºåˆ¶åŠ ä¸Š output_attentions=True è¾“å‡º attentions
output = model(**inputs)
print(output.last_hidden_state.shape())  # torch.Size([1, 12, 768])

# ç®€å•æ¡ˆä¾‹ï¼šå¸¦Model Headçš„æ¨¡å‹è°ƒç”¨
model = AutoModelForSequenceClassification.from_pretrained("rbt3", num_labels=10)
output = model(**inputs)
print(output.logits.shape())  # torch.Size([1, 10])

# ----------------------------------------------------------------------------------------------------------------
# ä¸­æ–‡åˆ†ç±»
# æ•°æ®é›†ç±»
class Dataset(th.utils.data.Dataset):
    def __init__(self):
        # super(Dataset, self).__init__()
        self.dataset = load_dataset(
            path="csv",
            data_files=os.path.join(path_data, "text_cls.csv"),
            split="all"
            )
    
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        text = self.dataset[i]["text"]
        label = self.dataset[i]["label"]
        return text, label 

# åˆ’åˆ†æ•°æ®é›†
dataset = Dataset()
trainset, validset = random_split(dataset, lengths=[0.9, 0.1])
# trainset, validset = dataset.train_test_split(test_size=0.1, stratify_by_column="label") 

# æ•´ç†å‡½æ•°
def collate_fn(dataset):
    texts = [x[0] for x in dataset]
    labels = [x[1] for x in dataset]
    max_length = max(len(x) for x in texts) + 2
    
    #ç¼–ç 
    inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=texts,
                                          truncation=True,
                                          padding="max_length",
                                          max_length=max_length,
                                          return_tensors="pt",
                                          return_length=True)

    labels = th.LongTensor(labels)  # torch.int64
    # labels = th.Tensor(labels, dtype=th.long)  # torch.int64
    return inputs, labels
# collate_fn = DataCollatorWithPadding(tokenizer)

# æ•°æ®è¿­ä»£å™¨
loader = th.utils.data.DataLoader(dataset=dataset,
                                  batch_size=16,
                                  collate_fn=collate_fn,
                                  shuffle=True,
                                  drop_last=True)

# ä¸‹æ¸¸æ¨¡å‹
config = {"pretrained": pretrained}

class Model(th.nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        self.pretrained = config.get("pretrained")
        self.mlp = th.nn.Linear(768, 2)  # æ­¤å¤„å¯è®¾è®¡ä¸ºå¤šå±‚mlpï¼Œç”¨nn.SequentialåŒ…è£¹

    def forward(self, inputs):
        tokens = inputs["input_ids"]
        segments = inputs["token_type_ids"]
        valid_lens = inputs["attention_mask"]
        
        output_bert = self.pretrained(
            input_ids=tokens,
            token_type_ids=segments,
            attention_mask=valid_lens
            ).last_hidden_state

        out_mlp = self.mlp(output_bert[:, 0, :])
        return out_mlp

# è®­ç»ƒ
model = Model(config).to(device)
opti = optim.AdamW(params=model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=10**-8, weight_decay=0.01)
# opti = optim.SGD(params=model.parameters(), lr=0.01, momentum=0.9)
# objt = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction="mean")
objt = nn.CrossEntropyLoss(reduction="mean")
epochs = 200

for epoch in range(epochs):
    # train
    loss_tmp = 0
    model.train()
    for (i, (inputs, labels)) in enumerate(loader):
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        output_mlp = model(inputs)
        loss = objt(output_mlp, labels)
        loss_tmp += loss.item()
        
        opti.zero_grad(set_to_none=True)
        loss.backward()
        opti.step()
    
    loss_train = loss_tmp / (i+1)
    print(f"epoch {epoch}  loss_train {loss_train:.4f}")

# æ¨ç†
max_length = len(texts)
inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=texts,
                                     truncation=True,
                                     padding="max_length",
                                     max_length=max_length,
                                     return_tensors="pt",
                                     return_length=True)

model.eval()
with th.no_grad():
    out_mlp = model(inputs)
    y_hat = th.softmax(out_mlp, dim=1)
    y_pred = th.argmax(y_hat, dim=1)


# ----------------------------------------------------------------------------------------------------------------
# å®Œå½¢å¡«ç©º
# æ•°æ®é›†ç±»
class Dataset(th.utils.data.Dataset):
    def __init__(self):
        dataset = load_dataset(
            path="csv",
            data_files=os.path.join(path_data, "text_fill.csv"),
            split="all"
            )

        def func(dataset):
            return len(dataset["text"]) > 30
        self.dataset = dataset.filter(func)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        text = self.dataset[i]["text"]
        return text

# æ•´ç†å‡½æ•°
def collate_fn(dataset):
    #ç¼–ç 
    inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=dataset,
                                         truncation=True,
                                         padding="max_length",
                                         max_length=30,
                                         return_tensors="pt",
                                         return_length=True)

    tokens = inputs["input_ids"]
    segments = inputs["token_type_ids"]
    valid_lens = inputs["attention_mask"]

    # æŠŠç¬¬15ä¸ªè¯å›ºå®šæ›¿æ¢ä¸ºmask
    labels = tokens[:, 15].reshape(-1).clone()
    tokens[:, 15] = tokenizer.mask_token_id
    labels = th.LongTensor(labels)
    return tokens, segments, valid_lens, labels

# æ•°æ®è¿­ä»£å™¨
loader = th.utils.data.DataLoader(dataset=Dataset(),
                                  batch_size=16,
                                  collate_fn=collate_fn,
                                  shuffle=True,
                                  drop_last=True)

# ä¸‹æ¸¸æ¨¡å‹
class Model(th.nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        self.pretrained = config.get("pretrained")
        self.mlp = th.nn.Linear(768, tokenizer.vocab_size, bias=False)
        bias = th.nn.Parameter(th.zeros(tokenizer.vocab_size))  # å¯é€‰
        self.mlp.bias = bias  # å¯é€‰

    def forward(self, tokens, segments, valid_lens):
        output_bert = self.pretrained(
            input_ids=tokens,
            token_type_ids=segments,
            attention_mask=valid_lens
            ).last_hidden_state

        out_mlp = self.mlp(output_bert[:, 15, :])
        return out_mlp

# ----------------------------------------------------------------------------------------------------------------
# ä¸­æ–‡å¥å­å…³ç³»æ¨æ–­
# æ•°æ®é›†ç±»
class Dataset(th.utils.data.Dataset):
    def __init__(self):
        dataset = load_dataset(
            path="csv",
            data_files=os.path.join(path_data, "text_inference.csv"),
            split="all"
            )

        def func(data):
            return len(data["text"]) > 40
        self.dataset = dataset.filter(func)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        text = self.dataset[i]["text"]
        sent1 = text[0:20]
        
        # æœ‰ä¸€åŠçš„æ¦‚ç‡æŠŠååŠå¥æ›¿æ¢ä¸ºä¸€å¥æ— å…³çš„è¯
        if random.random() < 0.5:
            j = random.choice(range(self.dataset))
            sent2 = self.dataset[j]["text"][20:40]
            label = 1
        else:
            sent2 = text[20:40]
            label = 0

        return sent1, sent2, label


# æ•´ç†å‡½æ•°
def collate_fn(data_set):
    sents = [x[0:2] for x in data_set]  # é€‚ç”¨äºå®šä¹‰æ•°æ®é›†
    labels = [x[2] for x in data_set]  # é€‚ç”¨äºå®šä¹‰æ•°æ®é›†
    # sents = [(dct.get("sent1"), dct.get("sent2")) for dct in data_set]  # é€‚ç”¨äº Dataset.from_pandas()
    # labels = [dct.get("label") for dct in data_set]  # é€‚ç”¨äº Dataset.from_pandas()
    max_length = max(len(x[0]) + len(x[1]) for x in sents) + 3

    #ç¼–ç 
    inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,
                                         truncation=True,
                                         padding="max_length",
                                         max_length=max_length,
                                         add_special_tokens=True,
                                         return_token_type_ids=True,
                                         return_attention_mask=True,
                                         return_special_tokens_mask=True,
                                         return_tensors="pt",
                                         return_length=True)
    labels = th.LongTensor(labels)
    return inputs, labels


# æ•°æ®è¿­ä»£å™¨
data_set = Dataset()
loader = th.utils.data.DataLoader(dataset=data_set,
                                  batch_size=16,
                                  collate_fn=collate_fn,
                                  shuffle=True,
                                  drop_last=True)

# ä¸‹æ¸¸æ¨¡å‹
class Model(th.nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        self.pretrained = config.get("pretrained")
        self.mlp = th.nn.Linear(768, 2)

    def forward(self, inputs):
        output_bert = self.pretrained(
            input_ids=tokens,
            token_type_ids=segments,
            attention_mask=valid_lens
            ).last_hidden_state

        out_mlp = self.mlp(output_bert[:, 0, :])
        return out_mlp
    

    
