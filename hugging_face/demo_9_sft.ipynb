{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets accelerate peft trl tensorboard bitsandbytes langchain sentencepiece transformers\n",
    "# !pip install transformers==4.37.2 --user\n",
    "# !pip install tiktoken einops transformers_stream_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import (load_dataset, load_from_disk, Dataset)\n",
    "from transformers import (AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "                          TrainingArguments, DataCollatorWithPadding, DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForSeq2Seq, DataCollatorForTokenClassification)\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from peft import (LoraConfig, get_peft_model, PeftModel, TaskType, get_peft_model_state_dict)\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n",
      "2.0.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "devive_cnt = th.cuda.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")\n",
    "print(th.__version__)\n",
    "print(th.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if th.cuda.is_bf16_supported():\n",
    "#     th.set_default_tensor_type(th.cuda.BFloat16Tensor)\n",
    "# else:\n",
    "#     th.set_default_tensor_type(th.cuda.HalfTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = \"C:/my_project/MyGit/Machine-Learning-Column/hugging_face\"\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_model = os.path.join(os.path.dirname(path_project), \"model\")\n",
    "path_output = os.path.join(os.path.dirname(path_project), \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 载入数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"tatsu-lab/alpaca/train-00000-of-00001-a09b74b3ef9c3b56.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"parquet\",\n",
    "    data_files=os.path.join(path_data, filename),\n",
    "    split=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(200))\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0) \n",
    "dataset_train, dataset_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Qwen1.5-4B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|im_end|>\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token  # 半精度训练时需要\n",
    "# tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: 配置量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")  # QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-4: 载入基础/任务大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa6a27a8a014d42b61d0abf941d63ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=th.bfloat16,  # \"auto\", th.bfloat16\n",
    "    quantization_config=(config_bnb if config_bnb else None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  name: model.embed_tokens.weight;  shape: torch.Size([151936, 2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "1  name: model.layers.0.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "2  name: model.layers.0.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "3  name: model.layers.0.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "4  name: model.layers.0.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "5  name: model.layers.0.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "6  name: model.layers.0.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "7  name: model.layers.0.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "8  name: model.layers.0.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "9  name: model.layers.0.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "10  name: model.layers.0.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "11  name: model.layers.0.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "12  name: model.layers.0.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "13  name: model.layers.1.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "14  name: model.layers.1.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "15  name: model.layers.1.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "16  name: model.layers.1.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "17  name: model.layers.1.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "18  name: model.layers.1.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "19  name: model.layers.1.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "20  name: model.layers.1.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "21  name: model.layers.1.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "22  name: model.layers.1.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "23  name: model.layers.1.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "24  name: model.layers.1.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "25  name: model.layers.2.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "26  name: model.layers.2.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "27  name: model.layers.2.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "28  name: model.layers.2.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "29  name: model.layers.2.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "30  name: model.layers.2.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "31  name: model.layers.2.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "32  name: model.layers.2.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "33  name: model.layers.2.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "34  name: model.layers.2.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "35  name: model.layers.2.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "36  name: model.layers.2.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "37  name: model.layers.3.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "38  name: model.layers.3.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "39  name: model.layers.3.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "40  name: model.layers.3.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "41  name: model.layers.3.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "42  name: model.layers.3.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "43  name: model.layers.3.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "44  name: model.layers.3.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "45  name: model.layers.3.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "46  name: model.layers.3.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "47  name: model.layers.3.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "48  name: model.layers.3.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "49  name: model.layers.4.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "50  name: model.layers.4.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "51  name: model.layers.4.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "52  name: model.layers.4.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "53  name: model.layers.4.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "54  name: model.layers.4.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "55  name: model.layers.4.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "56  name: model.layers.4.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "57  name: model.layers.4.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "58  name: model.layers.4.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "59  name: model.layers.4.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "60  name: model.layers.4.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "61  name: model.layers.5.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "62  name: model.layers.5.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "63  name: model.layers.5.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "64  name: model.layers.5.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "65  name: model.layers.5.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "66  name: model.layers.5.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "67  name: model.layers.5.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "68  name: model.layers.5.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "69  name: model.layers.5.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "70  name: model.layers.5.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "71  name: model.layers.5.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "72  name: model.layers.5.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "73  name: model.layers.6.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "74  name: model.layers.6.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "75  name: model.layers.6.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "76  name: model.layers.6.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "77  name: model.layers.6.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "78  name: model.layers.6.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "79  name: model.layers.6.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "80  name: model.layers.6.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "81  name: model.layers.6.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "82  name: model.layers.6.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "83  name: model.layers.6.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "84  name: model.layers.6.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "85  name: model.layers.7.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "86  name: model.layers.7.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "87  name: model.layers.7.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "88  name: model.layers.7.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "89  name: model.layers.7.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "90  name: model.layers.7.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "91  name: model.layers.7.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "92  name: model.layers.7.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "93  name: model.layers.7.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "94  name: model.layers.7.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "95  name: model.layers.7.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "96  name: model.layers.7.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "97  name: model.layers.8.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "98  name: model.layers.8.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "99  name: model.layers.8.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "100  name: model.layers.8.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "101  name: model.layers.8.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "102  name: model.layers.8.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "103  name: model.layers.8.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "104  name: model.layers.8.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "105  name: model.layers.8.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "106  name: model.layers.8.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "107  name: model.layers.8.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "108  name: model.layers.8.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "109  name: model.layers.9.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "110  name: model.layers.9.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "111  name: model.layers.9.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "112  name: model.layers.9.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "113  name: model.layers.9.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "114  name: model.layers.9.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "115  name: model.layers.9.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "116  name: model.layers.9.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "117  name: model.layers.9.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "118  name: model.layers.9.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "119  name: model.layers.9.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "120  name: model.layers.9.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "121  name: model.layers.10.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "122  name: model.layers.10.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "123  name: model.layers.10.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "124  name: model.layers.10.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "125  name: model.layers.10.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "126  name: model.layers.10.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "127  name: model.layers.10.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "128  name: model.layers.10.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "129  name: model.layers.10.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "130  name: model.layers.10.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "131  name: model.layers.10.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "132  name: model.layers.10.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "133  name: model.layers.11.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "134  name: model.layers.11.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "135  name: model.layers.11.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "136  name: model.layers.11.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "137  name: model.layers.11.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "138  name: model.layers.11.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "139  name: model.layers.11.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "140  name: model.layers.11.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "141  name: model.layers.11.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "142  name: model.layers.11.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "143  name: model.layers.11.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "144  name: model.layers.11.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "145  name: model.layers.12.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "146  name: model.layers.12.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "147  name: model.layers.12.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "148  name: model.layers.12.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "149  name: model.layers.12.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "150  name: model.layers.12.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "151  name: model.layers.12.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "152  name: model.layers.12.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "153  name: model.layers.12.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "154  name: model.layers.12.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "155  name: model.layers.12.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "156  name: model.layers.12.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "157  name: model.layers.13.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "158  name: model.layers.13.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "159  name: model.layers.13.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "160  name: model.layers.13.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "161  name: model.layers.13.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "162  name: model.layers.13.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "163  name: model.layers.13.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "164  name: model.layers.13.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "165  name: model.layers.13.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "166  name: model.layers.13.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "167  name: model.layers.13.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "168  name: model.layers.13.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "169  name: model.layers.14.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "170  name: model.layers.14.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "171  name: model.layers.14.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "172  name: model.layers.14.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "173  name: model.layers.14.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "174  name: model.layers.14.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "175  name: model.layers.14.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "176  name: model.layers.14.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "177  name: model.layers.14.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "178  name: model.layers.14.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "179  name: model.layers.14.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "180  name: model.layers.14.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "181  name: model.layers.15.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "182  name: model.layers.15.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "183  name: model.layers.15.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "184  name: model.layers.15.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "185  name: model.layers.15.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "186  name: model.layers.15.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "187  name: model.layers.15.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "188  name: model.layers.15.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "189  name: model.layers.15.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "190  name: model.layers.15.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "191  name: model.layers.15.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "192  name: model.layers.15.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "193  name: model.layers.16.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "194  name: model.layers.16.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "195  name: model.layers.16.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "196  name: model.layers.16.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "197  name: model.layers.16.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "198  name: model.layers.16.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "199  name: model.layers.16.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "200  name: model.layers.16.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "201  name: model.layers.16.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "202  name: model.layers.16.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "203  name: model.layers.16.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "204  name: model.layers.16.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "205  name: model.layers.17.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "206  name: model.layers.17.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "207  name: model.layers.17.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "208  name: model.layers.17.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "209  name: model.layers.17.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "210  name: model.layers.17.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "211  name: model.layers.17.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "212  name: model.layers.17.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "213  name: model.layers.17.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "214  name: model.layers.17.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "215  name: model.layers.17.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "216  name: model.layers.17.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "217  name: model.layers.18.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "218  name: model.layers.18.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "219  name: model.layers.18.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "220  name: model.layers.18.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "221  name: model.layers.18.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "222  name: model.layers.18.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "223  name: model.layers.18.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "224  name: model.layers.18.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "225  name: model.layers.18.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "226  name: model.layers.18.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "227  name: model.layers.18.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "228  name: model.layers.18.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "229  name: model.layers.19.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "230  name: model.layers.19.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "231  name: model.layers.19.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "232  name: model.layers.19.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "233  name: model.layers.19.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "234  name: model.layers.19.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "235  name: model.layers.19.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "236  name: model.layers.19.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "237  name: model.layers.19.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "238  name: model.layers.19.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "239  name: model.layers.19.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "240  name: model.layers.19.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "241  name: model.layers.20.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "242  name: model.layers.20.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "243  name: model.layers.20.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "244  name: model.layers.20.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "245  name: model.layers.20.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "246  name: model.layers.20.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "247  name: model.layers.20.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "248  name: model.layers.20.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "249  name: model.layers.20.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "250  name: model.layers.20.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "251  name: model.layers.20.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "252  name: model.layers.20.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "253  name: model.layers.21.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "254  name: model.layers.21.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "255  name: model.layers.21.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "256  name: model.layers.21.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "257  name: model.layers.21.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "258  name: model.layers.21.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "259  name: model.layers.21.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "260  name: model.layers.21.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "261  name: model.layers.21.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "262  name: model.layers.21.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "263  name: model.layers.21.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "264  name: model.layers.21.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "265  name: model.layers.22.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "266  name: model.layers.22.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "267  name: model.layers.22.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "268  name: model.layers.22.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "269  name: model.layers.22.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "270  name: model.layers.22.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "271  name: model.layers.22.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "272  name: model.layers.22.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "273  name: model.layers.22.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "274  name: model.layers.22.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "275  name: model.layers.22.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "276  name: model.layers.22.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "277  name: model.layers.23.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "278  name: model.layers.23.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "279  name: model.layers.23.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "280  name: model.layers.23.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "281  name: model.layers.23.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "282  name: model.layers.23.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "283  name: model.layers.23.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "284  name: model.layers.23.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "285  name: model.layers.23.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "286  name: model.layers.23.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "287  name: model.layers.23.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "288  name: model.layers.23.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "289  name: model.layers.24.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "290  name: model.layers.24.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "291  name: model.layers.24.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "292  name: model.layers.24.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "293  name: model.layers.24.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "294  name: model.layers.24.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "295  name: model.layers.24.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "296  name: model.layers.24.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "297  name: model.layers.24.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "298  name: model.layers.24.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "299  name: model.layers.24.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "300  name: model.layers.24.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "301  name: model.layers.25.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "302  name: model.layers.25.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "303  name: model.layers.25.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "304  name: model.layers.25.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "305  name: model.layers.25.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "306  name: model.layers.25.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "307  name: model.layers.25.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "308  name: model.layers.25.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "309  name: model.layers.25.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "310  name: model.layers.25.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "311  name: model.layers.25.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "312  name: model.layers.25.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "313  name: model.layers.26.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "314  name: model.layers.26.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "315  name: model.layers.26.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "316  name: model.layers.26.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "317  name: model.layers.26.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "318  name: model.layers.26.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "319  name: model.layers.26.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "320  name: model.layers.26.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "321  name: model.layers.26.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "322  name: model.layers.26.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "323  name: model.layers.26.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "324  name: model.layers.26.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "325  name: model.layers.27.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "326  name: model.layers.27.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "327  name: model.layers.27.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "328  name: model.layers.27.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "329  name: model.layers.27.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "330  name: model.layers.27.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "331  name: model.layers.27.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "332  name: model.layers.27.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "333  name: model.layers.27.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "334  name: model.layers.27.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "335  name: model.layers.27.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "336  name: model.layers.27.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "337  name: model.layers.28.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "338  name: model.layers.28.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "339  name: model.layers.28.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "340  name: model.layers.28.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "341  name: model.layers.28.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "342  name: model.layers.28.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "343  name: model.layers.28.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "344  name: model.layers.28.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "345  name: model.layers.28.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "346  name: model.layers.28.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "347  name: model.layers.28.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "348  name: model.layers.28.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "349  name: model.layers.29.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "350  name: model.layers.29.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "351  name: model.layers.29.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "352  name: model.layers.29.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "353  name: model.layers.29.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "354  name: model.layers.29.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "355  name: model.layers.29.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "356  name: model.layers.29.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "357  name: model.layers.29.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "358  name: model.layers.29.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "359  name: model.layers.29.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "360  name: model.layers.29.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "361  name: model.layers.30.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "362  name: model.layers.30.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "363  name: model.layers.30.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "364  name: model.layers.30.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "365  name: model.layers.30.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "366  name: model.layers.30.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "367  name: model.layers.30.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "368  name: model.layers.30.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "369  name: model.layers.30.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "370  name: model.layers.30.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "371  name: model.layers.30.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "372  name: model.layers.30.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "373  name: model.layers.31.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "374  name: model.layers.31.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "375  name: model.layers.31.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "376  name: model.layers.31.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "377  name: model.layers.31.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "378  name: model.layers.31.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "379  name: model.layers.31.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "380  name: model.layers.31.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "381  name: model.layers.31.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "382  name: model.layers.31.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "383  name: model.layers.31.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "384  name: model.layers.31.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "385  name: model.layers.32.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "386  name: model.layers.32.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "387  name: model.layers.32.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "388  name: model.layers.32.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "389  name: model.layers.32.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "390  name: model.layers.32.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "391  name: model.layers.32.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "392  name: model.layers.32.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "393  name: model.layers.32.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "394  name: model.layers.32.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "395  name: model.layers.32.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "396  name: model.layers.32.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "397  name: model.layers.33.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "398  name: model.layers.33.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "399  name: model.layers.33.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "400  name: model.layers.33.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "401  name: model.layers.33.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "402  name: model.layers.33.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "403  name: model.layers.33.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "404  name: model.layers.33.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "405  name: model.layers.33.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "406  name: model.layers.33.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "407  name: model.layers.33.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "408  name: model.layers.33.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "409  name: model.layers.34.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "410  name: model.layers.34.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "411  name: model.layers.34.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "412  name: model.layers.34.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "413  name: model.layers.34.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "414  name: model.layers.34.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "415  name: model.layers.34.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "416  name: model.layers.34.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "417  name: model.layers.34.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "418  name: model.layers.34.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "419  name: model.layers.34.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "420  name: model.layers.34.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "421  name: model.layers.35.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "422  name: model.layers.35.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "423  name: model.layers.35.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "424  name: model.layers.35.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "425  name: model.layers.35.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "426  name: model.layers.35.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "427  name: model.layers.35.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "428  name: model.layers.35.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "429  name: model.layers.35.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "430  name: model.layers.35.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "431  name: model.layers.35.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "432  name: model.layers.35.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "433  name: model.layers.36.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "434  name: model.layers.36.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "435  name: model.layers.36.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "436  name: model.layers.36.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "437  name: model.layers.36.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "438  name: model.layers.36.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "439  name: model.layers.36.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "440  name: model.layers.36.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "441  name: model.layers.36.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "442  name: model.layers.36.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "443  name: model.layers.36.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "444  name: model.layers.36.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "445  name: model.layers.37.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "446  name: model.layers.37.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "447  name: model.layers.37.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "448  name: model.layers.37.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "449  name: model.layers.37.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "450  name: model.layers.37.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "451  name: model.layers.37.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "452  name: model.layers.37.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "453  name: model.layers.37.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "454  name: model.layers.37.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "455  name: model.layers.37.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "456  name: model.layers.37.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "457  name: model.layers.38.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "458  name: model.layers.38.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "459  name: model.layers.38.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "460  name: model.layers.38.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "461  name: model.layers.38.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "462  name: model.layers.38.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "463  name: model.layers.38.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "464  name: model.layers.38.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "465  name: model.layers.38.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "466  name: model.layers.38.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "467  name: model.layers.38.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "468  name: model.layers.38.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "469  name: model.layers.39.self_attn.q_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "470  name: model.layers.39.self_attn.q_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "471  name: model.layers.39.self_attn.k_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "472  name: model.layers.39.self_attn.k_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "473  name: model.layers.39.self_attn.v_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "474  name: model.layers.39.self_attn.v_proj.bias;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "475  name: model.layers.39.self_attn.o_proj.weight;  shape: torch.Size([3276800, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "476  name: model.layers.39.mlp.gate_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "477  name: model.layers.39.mlp.up_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "478  name: model.layers.39.mlp.down_proj.weight;  shape: torch.Size([8847360, 1]);  dtype: torch.uint8;  device: cuda:0\n",
      "479  name: model.layers.39.input_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "480  name: model.layers.39.post_attention_layernorm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "481  name: model.norm.weight;  shape: torch.Size([2560]);  dtype: torch.bfloat16;  device: cuda:0\n",
      "482  name: lm_head.weight;  shape: torch.Size([151936, 2560]);  dtype: torch.bfloat16;  device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "for i, (name, parm) in enumerate(model_base.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：glm3 没有了 lm_head，有一个 output_layer，这个时候可能会分配到两个 device，导致计算 loss 的时候报错\n",
    "# if th.cuda.device_count() > 1:\n",
    "# \tmodel_base.hf_device_map[\"transformer.output_layer\"] = model_base.hf_device_map[\"transformer.embedding\"]  # 1 <- 0\n",
    "# \tdct_device_map = model_base.hf_device_map\n",
    " \n",
    "# \tmodel_base.cpu()\n",
    "# \tdel model_base\n",
    "# \tth.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load\n",
    "# model_base = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=path_model,\n",
    "#     cache_dir=path_model,\n",
    "#     force_download=False,\n",
    "#     local_files_only=True,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=dct_device_map,\n",
    "#     torch_dtype=th.float16,\n",
    "#     # quantization_config=config_bnb\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "# if TrainingArguments(gradient_checkpointing=True)\n",
    "model_base.gradient_checkpointing_enable()\n",
    "# note: Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping the model weights fixed. \n",
    "# if TrainingArguments(gradient_checkpointing=True)\n",
    "model_base.enable_input_require_grads()\n",
    "model_base.config.use_cache = False\n",
    "\n",
    "if th.cuda.device_count() > 1:\n",
    "    model_base.is_parallelizable = True\n",
    "    model_base.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(model_base.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已分配的GPU内存：3.61G, 已缓存的GPU内存：3.85G\n"
     ]
    }
   ],
   "source": [
    "allocated_memory = th.cuda.memory_allocated()\n",
    "cached_memory = th.cuda.memory_cached()\n",
    "print(f\"已分配的GPU内存：{allocated_memory / 1024**3:.2f}G, 已缓存的GPU内存：{cached_memory / 1024**3:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embedding_size\n",
    "tokenizer_size = len(tokenizer)\n",
    "embedding_size = model_base.get_input_embeddings().weight.shape[0]\n",
    "if tokenizer_size > embedding_size:\n",
    "    model_base.resize_token_embeddings(tokenizer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-5: 配置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"rank\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"use_rslora\": True,\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_steps\": 1,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_seq_lenght\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-6: 配置LoRA模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA: Low-Rank Adaptation of Large Language Models\n",
    "# config_lora = LoraConfig(target_modules=[\"0\"])\n",
    "# config_lora = LoraConfig(target_modules=[\"query_key_value\", \"dense_4h_to_h\"])\n",
    "# config_lora = LoraConfig(target_modules=[\".*\\.1.*query_key_value\"])\n",
    "# config_lora = LoraConfig(target_modules=[\"query_key_value\"], modules_to_save=[\"word_embeddings\"])\n",
    "config_lora = LoraConfig(\n",
    "    r=config_model.get(\"rank\"),\n",
    "    lora_alpha=config_model.get(\"lora_alpha\"),\n",
    "    lora_dropout=config_model.get(\"lora_dropout\"),\n",
    "    use_rslora=config_model.get(\"use_rslora\"),\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    # \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"lm_head\"\n",
    "                    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora = get_peft_model(model=model_base, peft_config=config_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6553600 || all params: 2370951680 || trainable%: 0.2764\n"
     ]
    }
   ],
   "source": [
    "# print_trainable_parameters - 1\n",
    "# print(model_lora.print_trainable_parameters())\n",
    "\n",
    "# print_trainable_parameters - 2\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for param in model_lora.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "    all_params += param.numel()\n",
    "\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight': tensor([[-0.0074,  0.0054,  0.0045,  ...,  0.0008, -0.0104, -0.0091],\n",
       "         [ 0.0059,  0.0129,  0.0136,  ..., -0.0137,  0.0157,  0.0183],\n",
       "         [ 0.0138, -0.0016,  0.0058,  ..., -0.0149, -0.0082, -0.0069],\n",
       "         ...,\n",
       "         [-0.0046, -0.0159,  0.0037,  ..., -0.0007,  0.0109,  0.0016],\n",
       "         [ 0.0070, -0.0074,  0.0131,  ...,  0.0006, -0.0087,  0.0071],\n",
       "         [-0.0118, -0.0109,  0.0006,  ...,  0.0073,  0.0127, -0.0052]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight': tensor([[ 1.8852e-02, -1.1636e-02,  1.1881e-03,  ..., -1.5452e-02,\n",
       "          -1.7447e-02, -6.3963e-03],\n",
       "         [-1.2236e-02,  1.1357e-02,  6.0192e-04,  ...,  1.7124e-02,\n",
       "          -1.9227e-02, -8.0728e-03],\n",
       "         [ 1.2706e-02,  1.9290e-02,  3.5537e-03,  ..., -2.8927e-03,\n",
       "           6.3946e-03,  6.4813e-03],\n",
       "         ...,\n",
       "         [-6.4032e-03,  1.7366e-02,  1.1291e-03,  ...,  8.5208e-03,\n",
       "          -8.3226e-03,  1.7876e-02],\n",
       "         [-1.6413e-02,  1.8561e-02,  1.9118e-02,  ..., -5.6038e-03,\n",
       "          -1.0892e-02, -1.7729e-02],\n",
       "         [-1.2010e-02,  1.5397e-02,  1.8290e-02,  ...,  1.0271e-05,\n",
       "          -1.8497e-02,  9.2884e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight': tensor([[-0.0013, -0.0124,  0.0103,  ..., -0.0179,  0.0166,  0.0148],\n",
       "         [-0.0033,  0.0178,  0.0055,  ..., -0.0188,  0.0113,  0.0139],\n",
       "         [ 0.0098,  0.0016, -0.0055,  ...,  0.0079, -0.0146,  0.0111],\n",
       "         ...,\n",
       "         [-0.0062, -0.0032, -0.0069,  ..., -0.0108,  0.0061,  0.0076],\n",
       "         [-0.0081, -0.0048,  0.0153,  ..., -0.0110, -0.0123,  0.0117],\n",
       "         [-0.0020,  0.0133, -0.0095,  ..., -0.0102, -0.0178,  0.0078]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight': tensor([[-0.0113,  0.0089, -0.0119,  ..., -0.0116,  0.0036, -0.0110],\n",
       "         [-0.0048,  0.0056, -0.0013,  ...,  0.0038, -0.0085,  0.0197],\n",
       "         [ 0.0072,  0.0028, -0.0070,  ...,  0.0132,  0.0179, -0.0035],\n",
       "         ...,\n",
       "         [ 0.0090,  0.0143, -0.0146,  ..., -0.0194,  0.0087, -0.0050],\n",
       "         [-0.0128,  0.0020, -0.0180,  ..., -0.0133, -0.0062,  0.0087],\n",
       "         [ 0.0087,  0.0047, -0.0021,  ...,  0.0032,  0.0137, -0.0106]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight': tensor([[-0.0003, -0.0112, -0.0145,  ...,  0.0182, -0.0026, -0.0106],\n",
       "         [-0.0165,  0.0147,  0.0175,  ..., -0.0026, -0.0099, -0.0091],\n",
       "         [ 0.0002, -0.0083, -0.0124,  ..., -0.0089,  0.0120, -0.0103],\n",
       "         ...,\n",
       "         [-0.0015, -0.0176, -0.0177,  ...,  0.0005,  0.0006,  0.0106],\n",
       "         [-0.0133,  0.0192, -0.0026,  ..., -0.0160,  0.0176, -0.0174],\n",
       "         [ 0.0105, -0.0166,  0.0177,  ...,  0.0008,  0.0104,  0.0184]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight': tensor([[ 0.0119,  0.0020, -0.0038,  ...,  0.0063,  0.0147, -0.0093],\n",
       "         [-0.0122,  0.0141,  0.0134,  ...,  0.0195,  0.0132, -0.0118],\n",
       "         [ 0.0192, -0.0152, -0.0117,  ...,  0.0149,  0.0004, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0078, -0.0021,  0.0126,  ..., -0.0119, -0.0018,  0.0090],\n",
       "         [-0.0086, -0.0091, -0.0118,  ..., -0.0074,  0.0141,  0.0088],\n",
       "         [-0.0024, -0.0018, -0.0053,  ..., -0.0043, -0.0073,  0.0094]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight': tensor([[ 0.0188, -0.0021,  0.0132,  ..., -0.0088, -0.0105,  0.0109],\n",
       "         [ 0.0027,  0.0005,  0.0040,  ...,  0.0011, -0.0081,  0.0032],\n",
       "         [-0.0076,  0.0088,  0.0102,  ...,  0.0019,  0.0109,  0.0086],\n",
       "         ...,\n",
       "         [-0.0016, -0.0003, -0.0098,  ...,  0.0082,  0.0046,  0.0194],\n",
       "         [ 0.0046, -0.0031,  0.0025,  ...,  0.0113, -0.0040,  0.0067],\n",
       "         [-0.0096,  0.0033, -0.0061,  ...,  0.0013, -0.0039,  0.0066]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight': tensor([[ 0.0175, -0.0178,  0.0145,  ..., -0.0050, -0.0140,  0.0021],\n",
       "         [-0.0006,  0.0172,  0.0180,  ..., -0.0097, -0.0116,  0.0151],\n",
       "         [ 0.0193,  0.0064, -0.0043,  ...,  0.0094, -0.0102,  0.0098],\n",
       "         ...,\n",
       "         [-0.0173, -0.0034, -0.0005,  ...,  0.0118,  0.0058,  0.0135],\n",
       "         [-0.0040,  0.0023, -0.0052,  ...,  0.0139, -0.0193, -0.0048],\n",
       "         [ 0.0144,  0.0098,  0.0120,  ...,  0.0173, -0.0041, -0.0029]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight': tensor([[ 0.0119,  0.0068,  0.0089,  ..., -0.0114, -0.0132, -0.0146],\n",
       "         [ 0.0004, -0.0123,  0.0095,  ..., -0.0050, -0.0002, -0.0020],\n",
       "         [-0.0114,  0.0100,  0.0047,  ...,  0.0049,  0.0029,  0.0158],\n",
       "         ...,\n",
       "         [-0.0134,  0.0128,  0.0026,  ...,  0.0016,  0.0023,  0.0016],\n",
       "         [ 0.0158, -0.0008,  0.0093,  ..., -0.0167,  0.0071,  0.0047],\n",
       "         [ 0.0103,  0.0104, -0.0046,  ...,  0.0101, -0.0054,  0.0125]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight': tensor([[ 0.0194,  0.0018,  0.0044,  ...,  0.0030,  0.0157, -0.0157],\n",
       "         [-0.0158, -0.0129,  0.0063,  ..., -0.0029, -0.0053,  0.0034],\n",
       "         [ 0.0086, -0.0172, -0.0145,  ..., -0.0194,  0.0030, -0.0087],\n",
       "         ...,\n",
       "         [ 0.0175,  0.0093,  0.0169,  ..., -0.0127, -0.0017,  0.0173],\n",
       "         [ 0.0045, -0.0131, -0.0071,  ..., -0.0029, -0.0149, -0.0112],\n",
       "         [-0.0115, -0.0053, -0.0020,  ...,  0.0049,  0.0089, -0.0113]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight': tensor([[ 0.0175, -0.0171,  0.0043,  ...,  0.0162, -0.0012,  0.0147],\n",
       "         [ 0.0106,  0.0163, -0.0191,  ...,  0.0047,  0.0077,  0.0040],\n",
       "         [-0.0057, -0.0172,  0.0049,  ..., -0.0031,  0.0135, -0.0089],\n",
       "         ...,\n",
       "         [-0.0120,  0.0175,  0.0043,  ..., -0.0033, -0.0170,  0.0063],\n",
       "         [-0.0124,  0.0041, -0.0052,  ..., -0.0169,  0.0107,  0.0054],\n",
       "         [-0.0165,  0.0078, -0.0129,  ..., -0.0047, -0.0030,  0.0038]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight': tensor([[ 0.0149, -0.0047, -0.0056,  ...,  0.0050, -0.0157, -0.0078],\n",
       "         [-0.0189, -0.0049,  0.0126,  ...,  0.0080, -0.0132, -0.0190],\n",
       "         [-0.0112, -0.0073, -0.0166,  ..., -0.0177,  0.0023, -0.0053],\n",
       "         ...,\n",
       "         [-0.0017,  0.0016,  0.0028,  ..., -0.0072,  0.0131,  0.0182],\n",
       "         [ 0.0190, -0.0077, -0.0163,  ...,  0.0056, -0.0180,  0.0061],\n",
       "         [ 0.0040,  0.0164,  0.0129,  ...,  0.0020,  0.0136,  0.0177]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight': tensor([[ 0.0010, -0.0035,  0.0170,  ..., -0.0145,  0.0166,  0.0188],\n",
       "         [-0.0183, -0.0057,  0.0167,  ..., -0.0176, -0.0062,  0.0009],\n",
       "         [ 0.0077,  0.0131, -0.0039,  ...,  0.0089,  0.0198,  0.0060],\n",
       "         ...,\n",
       "         [-0.0011,  0.0171,  0.0003,  ...,  0.0186, -0.0094, -0.0078],\n",
       "         [ 0.0186,  0.0028,  0.0132,  ...,  0.0074,  0.0034, -0.0171],\n",
       "         [ 0.0012, -0.0170, -0.0096,  ...,  0.0004, -0.0112,  0.0092]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight': tensor([[ 0.0160,  0.0173, -0.0074,  ..., -0.0029,  0.0034,  0.0038],\n",
       "         [ 0.0149,  0.0043, -0.0142,  ...,  0.0107, -0.0145, -0.0193],\n",
       "         [-0.0162, -0.0122,  0.0014,  ...,  0.0131, -0.0073,  0.0002],\n",
       "         ...,\n",
       "         [ 0.0009, -0.0142,  0.0006,  ..., -0.0143,  0.0125,  0.0193],\n",
       "         [-0.0011,  0.0047, -0.0069,  ..., -0.0096, -0.0026, -0.0195],\n",
       "         [ 0.0045,  0.0027,  0.0045,  ...,  0.0031, -0.0033, -0.0099]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight': tensor([[ 0.0034,  0.0102,  0.0024,  ...,  0.0054, -0.0014,  0.0042],\n",
       "         [-0.0021,  0.0181,  0.0077,  ..., -0.0152, -0.0167, -0.0190],\n",
       "         [-0.0045, -0.0125, -0.0070,  ..., -0.0105,  0.0068, -0.0011],\n",
       "         ...,\n",
       "         [-0.0008, -0.0068,  0.0076,  ..., -0.0046,  0.0040,  0.0156],\n",
       "         [ 0.0179,  0.0016, -0.0062,  ..., -0.0100, -0.0099,  0.0120],\n",
       "         [-0.0015, -0.0161, -0.0177,  ...,  0.0009,  0.0076,  0.0063]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight': tensor([[ 0.0010, -0.0028,  0.0186,  ..., -0.0104, -0.0096,  0.0026],\n",
       "         [ 0.0111,  0.0118,  0.0100,  ...,  0.0092,  0.0181,  0.0064],\n",
       "         [ 0.0148,  0.0041,  0.0053,  ..., -0.0130, -0.0088, -0.0103],\n",
       "         ...,\n",
       "         [-0.0012, -0.0040, -0.0174,  ..., -0.0065, -0.0096, -0.0187],\n",
       "         [ 0.0048, -0.0141,  0.0124,  ...,  0.0101, -0.0012,  0.0061],\n",
       "         [ 0.0197, -0.0135, -0.0138,  ...,  0.0066,  0.0187, -0.0135]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight': tensor([[ 0.0070, -0.0195, -0.0062,  ..., -0.0117, -0.0107,  0.0084],\n",
       "         [-0.0160,  0.0016,  0.0107,  ...,  0.0040,  0.0117,  0.0038],\n",
       "         [ 0.0003, -0.0142, -0.0031,  ..., -0.0115, -0.0063,  0.0060],\n",
       "         ...,\n",
       "         [-0.0150,  0.0137, -0.0194,  ...,  0.0156,  0.0027, -0.0102],\n",
       "         [-0.0128,  0.0068,  0.0014,  ..., -0.0148,  0.0107,  0.0122],\n",
       "         [-0.0026, -0.0029,  0.0134,  ...,  0.0070,  0.0185,  0.0092]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight': tensor([[-0.0164,  0.0159, -0.0109,  ...,  0.0123,  0.0184,  0.0155],\n",
       "         [-0.0080, -0.0035, -0.0131,  ...,  0.0026,  0.0132, -0.0093],\n",
       "         [ 0.0179,  0.0118,  0.0081,  ...,  0.0191, -0.0174, -0.0020],\n",
       "         ...,\n",
       "         [-0.0120,  0.0104,  0.0119,  ..., -0.0145,  0.0173,  0.0026],\n",
       "         [ 0.0156, -0.0059, -0.0103,  ..., -0.0119,  0.0031,  0.0051],\n",
       "         [ 0.0066,  0.0115, -0.0046,  ...,  0.0005, -0.0150,  0.0086]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight': tensor([[-0.0191, -0.0164, -0.0128,  ...,  0.0003,  0.0052, -0.0030],\n",
       "         [-0.0067,  0.0114,  0.0096,  ..., -0.0110, -0.0190, -0.0151],\n",
       "         [ 0.0002,  0.0095, -0.0183,  ..., -0.0185, -0.0133, -0.0187],\n",
       "         ...,\n",
       "         [-0.0117, -0.0151, -0.0044,  ..., -0.0030,  0.0115,  0.0103],\n",
       "         [-0.0173,  0.0009,  0.0033,  ..., -0.0183, -0.0098,  0.0111],\n",
       "         [-0.0168, -0.0150, -0.0010,  ..., -0.0183,  0.0117,  0.0129]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight': tensor([[-1.1051e-02,  1.7440e-02, -4.9680e-05,  ..., -1.1332e-02,\n",
       "           1.4421e-02, -6.4117e-03],\n",
       "         [-1.6386e-02,  1.9146e-02, -1.5260e-03,  ..., -5.6374e-03,\n",
       "           1.8958e-02,  4.8239e-03],\n",
       "         [-4.8699e-03, -5.5192e-03,  1.3491e-02,  ..., -3.7229e-03,\n",
       "          -1.0165e-02,  1.3759e-02],\n",
       "         ...,\n",
       "         [ 1.2764e-02, -1.6584e-02, -1.8769e-02,  ...,  1.9256e-02,\n",
       "           1.7753e-02, -1.7180e-02],\n",
       "         [ 1.0232e-02, -1.1923e-02, -1.8421e-02,  ..., -1.3818e-02,\n",
       "           1.6720e-02,  1.9533e-02],\n",
       "         [-1.1275e-02,  1.9344e-02,  1.7965e-02,  ...,  1.2644e-02,\n",
       "           1.3677e-02,  1.3193e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight': tensor([[ 0.0133,  0.0191,  0.0103,  ..., -0.0022,  0.0171, -0.0179],\n",
       "         [-0.0060, -0.0048, -0.0055,  ...,  0.0076, -0.0169, -0.0141],\n",
       "         [ 0.0022,  0.0193,  0.0088,  ..., -0.0168,  0.0083, -0.0037],\n",
       "         ...,\n",
       "         [ 0.0031,  0.0056,  0.0182,  ..., -0.0048,  0.0122, -0.0079],\n",
       "         [-0.0088,  0.0070,  0.0136,  ..., -0.0128,  0.0154,  0.0095],\n",
       "         [ 0.0085, -0.0152,  0.0133,  ..., -0.0071,  0.0194,  0.0153]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight': tensor([[-0.0172, -0.0018, -0.0101,  ...,  0.0091, -0.0057, -0.0027],\n",
       "         [ 0.0183, -0.0145,  0.0016,  ..., -0.0001, -0.0150, -0.0115],\n",
       "         [-0.0126,  0.0184,  0.0039,  ...,  0.0188,  0.0051,  0.0046],\n",
       "         ...,\n",
       "         [ 0.0153,  0.0166,  0.0003,  ..., -0.0035,  0.0153,  0.0095],\n",
       "         [ 0.0113, -0.0024,  0.0107,  ..., -0.0008,  0.0018,  0.0047],\n",
       "         [ 0.0140, -0.0041,  0.0165,  ...,  0.0066, -0.0182, -0.0086]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight': tensor([[ 0.0184, -0.0093, -0.0084,  ...,  0.0178,  0.0151,  0.0080],\n",
       "         [-0.0061, -0.0184, -0.0101,  ..., -0.0073, -0.0003,  0.0060],\n",
       "         [ 0.0194, -0.0183, -0.0002,  ..., -0.0013, -0.0082, -0.0185],\n",
       "         ...,\n",
       "         [-0.0172, -0.0163, -0.0189,  ...,  0.0081, -0.0136,  0.0110],\n",
       "         [ 0.0177, -0.0188, -0.0104,  ..., -0.0184,  0.0172, -0.0064],\n",
       "         [-0.0069, -0.0126,  0.0086,  ...,  0.0031,  0.0185,  0.0098]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight': tensor([[-0.0074, -0.0168,  0.0127,  ...,  0.0043,  0.0185, -0.0066],\n",
       "         [ 0.0179, -0.0087, -0.0154,  ...,  0.0033,  0.0180, -0.0073],\n",
       "         [-0.0099,  0.0188,  0.0157,  ...,  0.0082, -0.0121, -0.0090],\n",
       "         ...,\n",
       "         [ 0.0149, -0.0110,  0.0112,  ...,  0.0189,  0.0131,  0.0010],\n",
       "         [ 0.0050, -0.0025,  0.0156,  ...,  0.0116, -0.0035,  0.0081],\n",
       "         [-0.0099,  0.0033,  0.0066,  ...,  0.0163, -0.0106, -0.0144]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight': tensor([[ 0.0008,  0.0138,  0.0020,  ...,  0.0150,  0.0190,  0.0127],\n",
       "         [-0.0091, -0.0096, -0.0189,  ...,  0.0038,  0.0096,  0.0135],\n",
       "         [ 0.0080, -0.0012,  0.0085,  ...,  0.0044, -0.0003,  0.0167],\n",
       "         ...,\n",
       "         [ 0.0193,  0.0064, -0.0013,  ...,  0.0056,  0.0128, -0.0004],\n",
       "         [-0.0160,  0.0004,  0.0156,  ..., -0.0154,  0.0134, -0.0132],\n",
       "         [-0.0051, -0.0132,  0.0181,  ..., -0.0183, -0.0085, -0.0004]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight': tensor([[ 0.0137,  0.0015,  0.0082,  ...,  0.0177, -0.0194, -0.0146],\n",
       "         [-0.0183,  0.0064,  0.0107,  ..., -0.0163, -0.0191, -0.0059],\n",
       "         [-0.0006,  0.0193, -0.0005,  ..., -0.0110, -0.0018,  0.0007],\n",
       "         ...,\n",
       "         [ 0.0126, -0.0143,  0.0003,  ...,  0.0013,  0.0016,  0.0097],\n",
       "         [-0.0007,  0.0170,  0.0089,  ..., -0.0081,  0.0081,  0.0165],\n",
       "         [-0.0063,  0.0047, -0.0191,  ..., -0.0088, -0.0181, -0.0113]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight': tensor([[ 0.0017,  0.0037, -0.0178,  ...,  0.0177,  0.0072,  0.0179],\n",
       "         [-0.0054, -0.0015, -0.0023,  ..., -0.0087,  0.0053, -0.0196],\n",
       "         [-0.0176, -0.0018, -0.0037,  ...,  0.0157,  0.0147, -0.0095],\n",
       "         ...,\n",
       "         [ 0.0155,  0.0116, -0.0093,  ...,  0.0090,  0.0085,  0.0096],\n",
       "         [ 0.0063, -0.0106,  0.0182,  ..., -0.0159, -0.0114,  0.0088],\n",
       "         [-0.0090,  0.0163, -0.0035,  ...,  0.0185, -0.0032,  0.0159]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight': tensor([[ 0.0022, -0.0097, -0.0017,  ...,  0.0094, -0.0084,  0.0187],\n",
       "         [ 0.0145, -0.0105, -0.0141,  ..., -0.0023, -0.0067, -0.0155],\n",
       "         [-0.0144, -0.0183, -0.0088,  ...,  0.0154, -0.0053, -0.0078],\n",
       "         ...,\n",
       "         [ 0.0066, -0.0141, -0.0041,  ...,  0.0142, -0.0055,  0.0145],\n",
       "         [-0.0071, -0.0033, -0.0072,  ..., -0.0056,  0.0020,  0.0049],\n",
       "         [-0.0153, -0.0126,  0.0168,  ..., -0.0012, -0.0027,  0.0017]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight': tensor([[-0.0113, -0.0144, -0.0140,  ...,  0.0040,  0.0145,  0.0176],\n",
       "         [ 0.0078, -0.0017,  0.0165,  ...,  0.0096, -0.0111,  0.0017],\n",
       "         [-0.0170, -0.0153, -0.0125,  ..., -0.0076,  0.0189, -0.0177],\n",
       "         ...,\n",
       "         [ 0.0091,  0.0054,  0.0101,  ...,  0.0157, -0.0092, -0.0077],\n",
       "         [-0.0122,  0.0157,  0.0104,  ...,  0.0102,  0.0097, -0.0031],\n",
       "         [ 0.0185,  0.0078, -0.0095,  ..., -0.0072,  0.0157, -0.0191]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight': tensor([[ 0.0089,  0.0019, -0.0153,  ...,  0.0155, -0.0102,  0.0046],\n",
       "         [-0.0030,  0.0075,  0.0185,  ..., -0.0025,  0.0028,  0.0011],\n",
       "         [ 0.0047,  0.0167, -0.0181,  ...,  0.0172,  0.0024,  0.0070],\n",
       "         ...,\n",
       "         [ 0.0026,  0.0196,  0.0192,  ...,  0.0035,  0.0145, -0.0118],\n",
       "         [ 0.0008, -0.0153,  0.0051,  ...,  0.0042, -0.0181,  0.0184],\n",
       "         [-0.0122, -0.0022, -0.0012,  ...,  0.0133, -0.0052,  0.0057]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight': tensor([[-0.0168,  0.0062, -0.0087,  ...,  0.0066, -0.0140, -0.0052],\n",
       "         [-0.0017,  0.0147,  0.0069,  ..., -0.0035, -0.0099, -0.0090],\n",
       "         [-0.0160, -0.0022, -0.0080,  ...,  0.0016, -0.0010, -0.0146],\n",
       "         ...,\n",
       "         [-0.0091, -0.0170, -0.0129,  ...,  0.0021, -0.0084, -0.0073],\n",
       "         [ 0.0040,  0.0010, -0.0148,  ..., -0.0071,  0.0111, -0.0158],\n",
       "         [-0.0115,  0.0091, -0.0107,  ..., -0.0051, -0.0187,  0.0102]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight': tensor([[ 3.8309e-03, -4.5058e-04, -1.5130e-02,  ..., -1.8395e-02,\n",
       "           2.1969e-03, -1.8459e-02],\n",
       "         [ 1.2273e-02, -7.7620e-03, -1.7789e-02,  ..., -3.1437e-04,\n",
       "           1.0458e-02,  7.0465e-03],\n",
       "         [ 3.2642e-04,  1.4367e-02, -1.2388e-03,  ...,  4.2810e-03,\n",
       "          -1.0628e-02, -8.3960e-03],\n",
       "         ...,\n",
       "         [ 1.0798e-02, -1.2188e-02,  2.6822e-05,  ...,  1.5104e-02,\n",
       "           1.4509e-02,  8.5924e-03],\n",
       "         [-9.4212e-03, -1.2201e-02,  9.7151e-03,  ...,  9.5724e-04,\n",
       "          -1.3088e-02,  1.9605e-02],\n",
       "         [-1.9203e-02,  9.9058e-03, -1.2975e-02,  ..., -1.5939e-02,\n",
       "           6.5289e-03,  1.4790e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight': tensor([[ 0.0069,  0.0148,  0.0088,  ..., -0.0155, -0.0033, -0.0189],\n",
       "         [-0.0056, -0.0156,  0.0087,  ...,  0.0047, -0.0057, -0.0098],\n",
       "         [ 0.0099, -0.0062,  0.0033,  ...,  0.0147,  0.0100, -0.0196],\n",
       "         ...,\n",
       "         [ 0.0147, -0.0058,  0.0072,  ..., -0.0188, -0.0009,  0.0107],\n",
       "         [-0.0024, -0.0021,  0.0121,  ...,  0.0154,  0.0181, -0.0036],\n",
       "         [ 0.0034, -0.0016, -0.0105,  ...,  0.0152, -0.0042, -0.0145]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight': tensor([[-0.0028, -0.0095, -0.0154,  ..., -0.0002, -0.0112,  0.0016],\n",
       "         [ 0.0075,  0.0095, -0.0092,  ...,  0.0160, -0.0067,  0.0131],\n",
       "         [-0.0192, -0.0084,  0.0146,  ...,  0.0097,  0.0014,  0.0041],\n",
       "         ...,\n",
       "         [ 0.0114, -0.0108,  0.0121,  ..., -0.0194, -0.0157, -0.0174],\n",
       "         [ 0.0089, -0.0064, -0.0187,  ...,  0.0078, -0.0175, -0.0049],\n",
       "         [ 0.0061, -0.0181,  0.0082,  ...,  0.0134, -0.0144,  0.0150]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight': tensor([[-0.0186,  0.0100,  0.0145,  ..., -0.0195,  0.0012,  0.0030],\n",
       "         [ 0.0179, -0.0003, -0.0154,  ...,  0.0060,  0.0172,  0.0001],\n",
       "         [-0.0168, -0.0087,  0.0036,  ...,  0.0111, -0.0090, -0.0075],\n",
       "         ...,\n",
       "         [-0.0031,  0.0193, -0.0152,  ...,  0.0094, -0.0074,  0.0194],\n",
       "         [ 0.0173,  0.0029, -0.0164,  ...,  0.0012,  0.0113, -0.0077],\n",
       "         [ 0.0062,  0.0177, -0.0128,  ...,  0.0144, -0.0005,  0.0112]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight': tensor([[-0.0142,  0.0126,  0.0006,  ..., -0.0135, -0.0023, -0.0036],\n",
       "         [ 0.0150,  0.0120, -0.0192,  ..., -0.0041,  0.0171,  0.0166],\n",
       "         [-0.0072, -0.0190,  0.0131,  ...,  0.0168,  0.0011, -0.0159],\n",
       "         ...,\n",
       "         [ 0.0129, -0.0078, -0.0101,  ...,  0.0077, -0.0130,  0.0172],\n",
       "         [-0.0021,  0.0139,  0.0147,  ...,  0.0191,  0.0091,  0.0141],\n",
       "         [-0.0083,  0.0169, -0.0088,  ..., -0.0143,  0.0023, -0.0031]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight': tensor([[-1.8327e-02, -2.9730e-03,  1.0853e-02,  ..., -3.4130e-03,\n",
       "           1.3238e-02, -1.7124e-02],\n",
       "         [ 3.8201e-03, -1.7870e-02,  6.0699e-04,  ..., -8.5707e-03,\n",
       "           1.6854e-02,  1.1269e-02],\n",
       "         [ 1.7059e-02, -7.0137e-03, -5.5393e-03,  ..., -1.4800e-03,\n",
       "          -1.2926e-02, -1.4125e-02],\n",
       "         ...,\n",
       "         [ 1.9518e-02,  1.6196e-02, -1.5722e-02,  ...,  2.7639e-03,\n",
       "          -8.9960e-03, -9.5290e-03],\n",
       "         [-1.8333e-02,  1.2018e-03, -1.0708e-02,  ..., -4.9792e-03,\n",
       "          -7.0404e-03, -8.4864e-05],\n",
       "         [-8.8363e-03, -1.6128e-02,  4.1829e-03,  ..., -1.7485e-03,\n",
       "          -9.9376e-03, -1.5446e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight': tensor([[ 0.0066,  0.0132, -0.0041,  ..., -0.0169, -0.0103,  0.0061],\n",
       "         [-0.0088, -0.0071, -0.0150,  ...,  0.0095, -0.0004,  0.0136],\n",
       "         [-0.0076,  0.0131,  0.0074,  ..., -0.0167,  0.0089, -0.0182],\n",
       "         ...,\n",
       "         [-0.0165, -0.0197,  0.0158,  ..., -0.0184,  0.0099,  0.0195],\n",
       "         [-0.0018, -0.0088, -0.0112,  ...,  0.0077, -0.0081,  0.0055],\n",
       "         [ 0.0138, -0.0026,  0.0036,  ...,  0.0077, -0.0077, -0.0183]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight': tensor([[-0.0195,  0.0116,  0.0110,  ..., -0.0173,  0.0144,  0.0004],\n",
       "         [ 0.0172, -0.0182, -0.0019,  ..., -0.0139,  0.0189,  0.0144],\n",
       "         [-0.0074, -0.0073,  0.0019,  ..., -0.0187,  0.0031, -0.0119],\n",
       "         ...,\n",
       "         [-0.0137, -0.0083,  0.0162,  ..., -0.0002,  0.0170, -0.0188],\n",
       "         [-0.0181, -0.0136, -0.0044,  ...,  0.0139, -0.0114, -0.0089],\n",
       "         [ 0.0177,  0.0103, -0.0062,  ..., -0.0014, -0.0071,  0.0149]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight': tensor([[-0.0132,  0.0006,  0.0043,  ...,  0.0108,  0.0137, -0.0079],\n",
       "         [-0.0123, -0.0172, -0.0105,  ...,  0.0141,  0.0059,  0.0005],\n",
       "         [ 0.0093, -0.0183,  0.0075,  ..., -0.0054,  0.0104,  0.0062],\n",
       "         ...,\n",
       "         [ 0.0061,  0.0086, -0.0120,  ..., -0.0144, -0.0037,  0.0169],\n",
       "         [-0.0064, -0.0138, -0.0186,  ..., -0.0069, -0.0174, -0.0051],\n",
       "         [-0.0113, -0.0039, -0.0077,  ..., -0.0035,  0.0154, -0.0096]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight': tensor([[-0.0150, -0.0153, -0.0175,  ..., -0.0157, -0.0133, -0.0145],\n",
       "         [-0.0115,  0.0143,  0.0107,  ..., -0.0049,  0.0120, -0.0170],\n",
       "         [ 0.0055,  0.0011,  0.0153,  ..., -0.0168,  0.0157, -0.0062],\n",
       "         ...,\n",
       "         [-0.0175, -0.0087,  0.0112,  ..., -0.0015, -0.0021, -0.0050],\n",
       "         [ 0.0146,  0.0121,  0.0094,  ...,  0.0174, -0.0145,  0.0156],\n",
       "         [-0.0123,  0.0082, -0.0166,  ...,  0.0040, -0.0118,  0.0084]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight': tensor([[ 1.3851e-02,  1.6007e-02,  6.3622e-03,  ...,  1.7382e-02,\n",
       "           1.4809e-03, -8.1012e-03],\n",
       "         [ 2.2687e-03,  4.7580e-03, -8.4183e-03,  ...,  2.6308e-03,\n",
       "          -9.0055e-04,  1.1513e-02],\n",
       "         [ 1.9439e-02, -1.5122e-02, -1.7285e-02,  ..., -1.3178e-02,\n",
       "           1.5258e-02, -1.5015e-02],\n",
       "         ...,\n",
       "         [-1.7659e-02, -7.4830e-03,  1.8310e-02,  ..., -7.9760e-03,\n",
       "          -9.1573e-05, -3.2533e-03],\n",
       "         [ 9.3817e-03,  1.1542e-02, -1.9557e-02,  ..., -1.5559e-02,\n",
       "           1.4172e-02, -1.5127e-02],\n",
       "         [-9.8645e-03, -8.6500e-04, -1.8181e-02,  ..., -1.2494e-02,\n",
       "           1.5743e-02, -1.5985e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight': tensor([[ 0.0003,  0.0188,  0.0047,  ..., -0.0139,  0.0194, -0.0133],\n",
       "         [-0.0127, -0.0139,  0.0139,  ...,  0.0159, -0.0025, -0.0036],\n",
       "         [-0.0054,  0.0086, -0.0044,  ...,  0.0123, -0.0145, -0.0171],\n",
       "         ...,\n",
       "         [ 0.0194, -0.0096,  0.0117,  ..., -0.0080, -0.0081,  0.0067],\n",
       "         [-0.0029,  0.0035, -0.0165,  ..., -0.0077,  0.0025,  0.0018],\n",
       "         [ 0.0049,  0.0108,  0.0046,  ...,  0.0158, -0.0078,  0.0051]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight': tensor([[ 0.0088,  0.0182, -0.0027,  ...,  0.0030, -0.0120, -0.0015],\n",
       "         [-0.0057, -0.0032,  0.0131,  ...,  0.0162, -0.0151,  0.0119],\n",
       "         [-0.0186,  0.0196,  0.0133,  ...,  0.0122, -0.0172, -0.0103],\n",
       "         ...,\n",
       "         [ 0.0164, -0.0142, -0.0129,  ...,  0.0081, -0.0084, -0.0191],\n",
       "         [ 0.0078,  0.0143,  0.0140,  ...,  0.0055, -0.0195, -0.0079],\n",
       "         [-0.0193, -0.0070, -0.0103,  ...,  0.0144,  0.0095, -0.0089]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight': tensor([[-0.0172,  0.0058,  0.0043,  ...,  0.0131,  0.0082, -0.0044],\n",
       "         [ 0.0043, -0.0009, -0.0072,  ..., -0.0058,  0.0097, -0.0151],\n",
       "         [ 0.0152, -0.0173, -0.0077,  ..., -0.0147,  0.0166,  0.0196],\n",
       "         ...,\n",
       "         [-0.0074, -0.0074, -0.0161,  ..., -0.0096, -0.0050,  0.0092],\n",
       "         [-0.0080, -0.0076, -0.0010,  ..., -0.0154, -0.0036,  0.0094],\n",
       "         [ 0.0158,  0.0059,  0.0087,  ..., -0.0147,  0.0106, -0.0013]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight': tensor([[-0.0022,  0.0132,  0.0184,  ...,  0.0043,  0.0083,  0.0041],\n",
       "         [-0.0172, -0.0099,  0.0182,  ...,  0.0147, -0.0063,  0.0039],\n",
       "         [ 0.0004,  0.0081, -0.0057,  ...,  0.0032,  0.0176, -0.0049],\n",
       "         ...,\n",
       "         [-0.0068,  0.0077,  0.0012,  ...,  0.0028, -0.0068, -0.0124],\n",
       "         [ 0.0122, -0.0143, -0.0140,  ...,  0.0047,  0.0162, -0.0044],\n",
       "         [ 0.0181,  0.0148, -0.0053,  ..., -0.0084,  0.0097,  0.0144]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight': tensor([[ 0.0107, -0.0105,  0.0057,  ..., -0.0111,  0.0040,  0.0102],\n",
       "         [ 0.0101,  0.0013, -0.0145,  ...,  0.0041, -0.0095, -0.0185],\n",
       "         [ 0.0065,  0.0081,  0.0042,  ..., -0.0094, -0.0009,  0.0124],\n",
       "         ...,\n",
       "         [-0.0009, -0.0179, -0.0151,  ..., -0.0126,  0.0114, -0.0196],\n",
       "         [ 0.0073,  0.0002,  0.0053,  ...,  0.0098, -0.0128,  0.0100],\n",
       "         [-0.0002, -0.0098, -0.0145,  ..., -0.0085, -0.0132,  0.0088]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight': tensor([[-0.0127, -0.0050,  0.0154,  ..., -0.0025, -0.0091, -0.0094],\n",
       "         [-0.0170, -0.0065, -0.0102,  ..., -0.0033, -0.0178,  0.0177],\n",
       "         [-0.0114,  0.0146,  0.0184,  ..., -0.0081,  0.0049,  0.0011],\n",
       "         ...,\n",
       "         [ 0.0195, -0.0031,  0.0125,  ..., -0.0167,  0.0130,  0.0070],\n",
       "         [ 0.0044,  0.0004, -0.0143,  ...,  0.0148, -0.0022,  0.0147],\n",
       "         [ 0.0010,  0.0146, -0.0152,  ..., -0.0026,  0.0163,  0.0092]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight': tensor([[ 0.0148, -0.0130,  0.0106,  ...,  0.0028, -0.0103, -0.0154],\n",
       "         [ 0.0084,  0.0180, -0.0181,  ..., -0.0085, -0.0137,  0.0127],\n",
       "         [-0.0023, -0.0129, -0.0008,  ...,  0.0117,  0.0105, -0.0159],\n",
       "         ...,\n",
       "         [-0.0186,  0.0147, -0.0093,  ..., -0.0060, -0.0077, -0.0005],\n",
       "         [ 0.0139,  0.0191, -0.0141,  ..., -0.0115, -0.0155,  0.0014],\n",
       "         [ 0.0184, -0.0120,  0.0162,  ..., -0.0001,  0.0150, -0.0050]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight': tensor([[-0.0033, -0.0179, -0.0084,  ..., -0.0079, -0.0171,  0.0132],\n",
       "         [-0.0049,  0.0191,  0.0018,  ...,  0.0066, -0.0060, -0.0147],\n",
       "         [-0.0144,  0.0197, -0.0060,  ...,  0.0042,  0.0026,  0.0083],\n",
       "         ...,\n",
       "         [ 0.0186, -0.0055,  0.0008,  ...,  0.0179, -0.0185, -0.0165],\n",
       "         [ 0.0195,  0.0093, -0.0022,  ...,  0.0061,  0.0155,  0.0018],\n",
       "         [-0.0115,  0.0179,  0.0170,  ..., -0.0164,  0.0050, -0.0076]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight': tensor([[-0.0072,  0.0089, -0.0131,  ..., -0.0168, -0.0195,  0.0052],\n",
       "         [-0.0065,  0.0006, -0.0110,  ..., -0.0162, -0.0008,  0.0107],\n",
       "         [ 0.0061,  0.0124, -0.0053,  ...,  0.0091,  0.0065, -0.0120],\n",
       "         ...,\n",
       "         [ 0.0059,  0.0037, -0.0053,  ..., -0.0135, -0.0093,  0.0104],\n",
       "         [-0.0026,  0.0148, -0.0190,  ...,  0.0091,  0.0141,  0.0071],\n",
       "         [ 0.0084,  0.0101,  0.0115,  ...,  0.0116, -0.0063,  0.0034]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight': tensor([[-0.0093, -0.0118, -0.0151,  ..., -0.0115,  0.0054,  0.0158],\n",
       "         [-0.0036,  0.0024,  0.0177,  ...,  0.0035,  0.0135,  0.0026],\n",
       "         [-0.0057,  0.0166, -0.0154,  ...,  0.0072,  0.0047, -0.0179],\n",
       "         ...,\n",
       "         [ 0.0052,  0.0030,  0.0016,  ...,  0.0142, -0.0194, -0.0122],\n",
       "         [-0.0016, -0.0165, -0.0017,  ...,  0.0122,  0.0061, -0.0123],\n",
       "         [ 0.0082, -0.0133, -0.0025,  ..., -0.0125,  0.0040, -0.0190]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight': tensor([[-0.0080,  0.0101, -0.0063,  ...,  0.0012,  0.0136,  0.0073],\n",
       "         [ 0.0038,  0.0070, -0.0183,  ..., -0.0024, -0.0111, -0.0154],\n",
       "         [ 0.0117, -0.0109, -0.0127,  ..., -0.0089,  0.0032, -0.0074],\n",
       "         ...,\n",
       "         [-0.0080, -0.0176,  0.0150,  ..., -0.0033,  0.0053,  0.0071],\n",
       "         [ 0.0006, -0.0141,  0.0127,  ...,  0.0010,  0.0070,  0.0132],\n",
       "         [-0.0096,  0.0086,  0.0168,  ...,  0.0047, -0.0041, -0.0140]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight': tensor([[ 0.0139, -0.0181, -0.0122,  ...,  0.0017, -0.0098, -0.0101],\n",
       "         [-0.0124, -0.0122,  0.0157,  ...,  0.0136, -0.0100, -0.0135],\n",
       "         [-0.0042, -0.0090, -0.0084,  ..., -0.0165, -0.0019,  0.0133],\n",
       "         ...,\n",
       "         [ 0.0011,  0.0103,  0.0181,  ...,  0.0052,  0.0180, -0.0192],\n",
       "         [-0.0057, -0.0151, -0.0186,  ..., -0.0156,  0.0119,  0.0147],\n",
       "         [-0.0006,  0.0178,  0.0153,  ..., -0.0095,  0.0040, -0.0134]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight': tensor([[-0.0151, -0.0008, -0.0186,  ...,  0.0078, -0.0196, -0.0095],\n",
       "         [-0.0034, -0.0033, -0.0051,  ..., -0.0164, -0.0142, -0.0128],\n",
       "         [ 0.0123,  0.0153,  0.0025,  ..., -0.0088, -0.0069, -0.0195],\n",
       "         ...,\n",
       "         [-0.0062,  0.0192,  0.0163,  ...,  0.0193, -0.0155,  0.0012],\n",
       "         [-0.0056,  0.0003, -0.0080,  ..., -0.0010,  0.0118, -0.0048],\n",
       "         [-0.0170,  0.0176,  0.0022,  ..., -0.0162, -0.0108, -0.0067]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight': tensor([[ 1.2306e-02,  1.9504e-02,  1.9760e-02,  ..., -1.1653e-02,\n",
       "          -1.4953e-02,  1.7333e-02],\n",
       "         [-1.0241e-02, -8.9344e-03,  1.2450e-02,  ...,  9.7634e-05,\n",
       "           4.7859e-03, -9.7217e-03],\n",
       "         [-9.5137e-03,  9.2611e-03, -1.0174e-02,  ..., -1.0661e-02,\n",
       "           4.8423e-03, -7.7107e-03],\n",
       "         ...,\n",
       "         [-1.7897e-02, -1.3796e-03, -1.5987e-02,  ...,  8.4498e-03,\n",
       "          -1.3595e-02,  1.4479e-02],\n",
       "         [-8.2201e-03,  2.3946e-03, -2.2938e-03,  ..., -1.4549e-02,\n",
       "          -1.8215e-02,  1.6815e-02],\n",
       "         [-6.6989e-03, -1.3822e-02, -2.7992e-03,  ...,  1.0482e-02,\n",
       "           3.4993e-03,  6.5279e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight': tensor([[-0.0047,  0.0039,  0.0097,  ..., -0.0006, -0.0036, -0.0112],\n",
       "         [-0.0062,  0.0109,  0.0056,  ..., -0.0139, -0.0148,  0.0127],\n",
       "         [ 0.0079, -0.0027, -0.0195,  ...,  0.0117, -0.0170,  0.0076],\n",
       "         ...,\n",
       "         [-0.0184, -0.0027,  0.0129,  ..., -0.0187,  0.0039, -0.0072],\n",
       "         [-0.0099,  0.0196,  0.0026,  ...,  0.0088,  0.0177,  0.0151],\n",
       "         [-0.0177, -0.0169, -0.0011,  ..., -0.0157, -0.0178,  0.0153]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight': tensor([[ 0.0049,  0.0031, -0.0094,  ..., -0.0123,  0.0140,  0.0114],\n",
       "         [ 0.0053, -0.0160, -0.0042,  ..., -0.0112, -0.0067, -0.0017],\n",
       "         [ 0.0184, -0.0002,  0.0043,  ..., -0.0015, -0.0039, -0.0018],\n",
       "         ...,\n",
       "         [ 0.0121,  0.0152,  0.0088,  ..., -0.0169, -0.0095, -0.0157],\n",
       "         [ 0.0145, -0.0019, -0.0168,  ...,  0.0163,  0.0192, -0.0142],\n",
       "         [ 0.0133,  0.0123, -0.0168,  ..., -0.0115,  0.0036,  0.0147]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight': tensor([[ 0.0121, -0.0103,  0.0107,  ...,  0.0095, -0.0032,  0.0179],\n",
       "         [ 0.0001, -0.0027,  0.0027,  ...,  0.0142, -0.0045, -0.0007],\n",
       "         [-0.0013, -0.0081, -0.0024,  ...,  0.0099, -0.0094, -0.0197],\n",
       "         ...,\n",
       "         [ 0.0180,  0.0011,  0.0024,  ...,  0.0177,  0.0112, -0.0088],\n",
       "         [ 0.0130, -0.0133,  0.0048,  ..., -0.0046,  0.0088, -0.0143],\n",
       "         [-0.0103, -0.0031, -0.0120,  ..., -0.0080,  0.0094, -0.0107]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight': tensor([[-0.0040, -0.0139,  0.0046,  ..., -0.0066, -0.0028, -0.0142],\n",
       "         [-0.0064,  0.0006, -0.0089,  ..., -0.0017,  0.0158, -0.0012],\n",
       "         [ 0.0065, -0.0020, -0.0049,  ..., -0.0166,  0.0040,  0.0034],\n",
       "         ...,\n",
       "         [ 0.0069, -0.0078,  0.0046,  ...,  0.0126,  0.0137, -0.0140],\n",
       "         [ 0.0197, -0.0149, -0.0121,  ...,  0.0134, -0.0052, -0.0106],\n",
       "         [ 0.0117, -0.0015,  0.0107,  ...,  0.0073, -0.0166, -0.0163]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight': tensor([[-4.3543e-05, -1.5257e-02,  1.1771e-02,  ...,  4.9379e-06,\n",
       "          -1.5470e-02,  1.4334e-02],\n",
       "         [ 1.0702e-02,  8.7598e-03,  1.3890e-02,  ..., -5.4069e-04,\n",
       "          -1.6963e-02,  1.8900e-02],\n",
       "         [-4.4664e-03, -6.3370e-03, -1.0209e-02,  ..., -1.5473e-02,\n",
       "           1.7458e-02, -9.2846e-03],\n",
       "         ...,\n",
       "         [-1.0452e-02, -1.0947e-02,  1.4119e-02,  ..., -1.0760e-02,\n",
       "           2.1461e-03,  5.5331e-03],\n",
       "         [ 1.8534e-02, -7.1273e-03,  6.8597e-03,  ..., -2.1321e-03,\n",
       "          -6.5561e-03,  1.8385e-02],\n",
       "         [-1.0968e-02,  4.3973e-03, -5.9736e-03,  ..., -3.2718e-03,\n",
       "          -1.2752e-02,  1.7976e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight': tensor([[ 0.0008,  0.0064, -0.0101,  ...,  0.0194,  0.0035,  0.0193],\n",
       "         [-0.0012,  0.0176, -0.0085,  ..., -0.0155,  0.0191,  0.0136],\n",
       "         [ 0.0033, -0.0087, -0.0006,  ...,  0.0154, -0.0127, -0.0005],\n",
       "         ...,\n",
       "         [-0.0195,  0.0157, -0.0044,  ..., -0.0138,  0.0122,  0.0107],\n",
       "         [ 0.0139, -0.0194, -0.0145,  ..., -0.0018,  0.0019,  0.0067],\n",
       "         [ 0.0139,  0.0097,  0.0045,  ..., -0.0105,  0.0126, -0.0096]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight': tensor([[ 0.0119, -0.0122, -0.0021,  ..., -0.0029, -0.0193, -0.0142],\n",
       "         [ 0.0078, -0.0158,  0.0051,  ..., -0.0079,  0.0140, -0.0090],\n",
       "         [-0.0189, -0.0074,  0.0103,  ..., -0.0101, -0.0096,  0.0117],\n",
       "         ...,\n",
       "         [-0.0007, -0.0029,  0.0194,  ..., -0.0158,  0.0074, -0.0007],\n",
       "         [ 0.0185, -0.0079,  0.0048,  ...,  0.0194, -0.0023, -0.0106],\n",
       "         [-0.0117,  0.0069, -0.0164,  ...,  0.0195,  0.0012, -0.0124]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight': tensor([[ 0.0088, -0.0185, -0.0009,  ..., -0.0179, -0.0110, -0.0020],\n",
       "         [ 0.0025, -0.0081, -0.0165,  ...,  0.0171, -0.0105,  0.0100],\n",
       "         [-0.0094,  0.0006,  0.0100,  ...,  0.0007,  0.0125, -0.0150],\n",
       "         ...,\n",
       "         [ 0.0003,  0.0040, -0.0078,  ...,  0.0008,  0.0159,  0.0062],\n",
       "         [ 0.0114, -0.0034,  0.0153,  ...,  0.0197, -0.0073, -0.0005],\n",
       "         [-0.0165, -0.0115,  0.0120,  ..., -0.0092,  0.0178,  0.0144]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight': tensor([[-0.0055,  0.0107, -0.0114,  ...,  0.0100, -0.0013,  0.0101],\n",
       "         [-0.0103, -0.0096,  0.0159,  ..., -0.0147, -0.0020, -0.0141],\n",
       "         [-0.0165,  0.0090, -0.0181,  ..., -0.0114, -0.0107,  0.0192],\n",
       "         ...,\n",
       "         [ 0.0035,  0.0053,  0.0100,  ...,  0.0090,  0.0040,  0.0008],\n",
       "         [ 0.0110, -0.0075, -0.0020,  ..., -0.0119, -0.0160,  0.0083],\n",
       "         [ 0.0017,  0.0157, -0.0138,  ..., -0.0195,  0.0008,  0.0065]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight': tensor([[ 1.5695e-02, -1.8370e-02,  4.1975e-04,  ...,  1.1347e-02,\n",
       "          -5.4455e-04, -1.5958e-02],\n",
       "         [-6.3600e-03,  3.9873e-03,  8.1636e-05,  ..., -1.3700e-02,\n",
       "          -1.1048e-02,  1.9506e-02],\n",
       "         [ 1.2629e-02, -5.3403e-03,  7.5405e-03,  ..., -6.8297e-03,\n",
       "           1.2279e-02,  1.7733e-02],\n",
       "         ...,\n",
       "         [-4.1552e-03,  1.8037e-03,  1.5721e-03,  ...,  5.1503e-03,\n",
       "          -1.8922e-02,  1.6160e-02],\n",
       "         [ 6.4366e-03,  7.0565e-03, -1.3789e-02,  ..., -1.2507e-02,\n",
       "          -1.4493e-02, -1.3931e-02],\n",
       "         [ 1.1598e-02,  1.0065e-02, -1.7641e-02,  ..., -1.8753e-03,\n",
       "           1.0921e-02, -5.8985e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight': tensor([[ 0.0139, -0.0156, -0.0066,  ..., -0.0158,  0.0062, -0.0130],\n",
       "         [-0.0048,  0.0096,  0.0002,  ..., -0.0180, -0.0055, -0.0025],\n",
       "         [-0.0180, -0.0013, -0.0081,  ...,  0.0195,  0.0018, -0.0048],\n",
       "         ...,\n",
       "         [-0.0164,  0.0097,  0.0180,  ...,  0.0006,  0.0027, -0.0072],\n",
       "         [-0.0172, -0.0090,  0.0125,  ...,  0.0173, -0.0106, -0.0040],\n",
       "         [-0.0051, -0.0140, -0.0017,  ..., -0.0163, -0.0015,  0.0035]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight': tensor([[-0.0175, -0.0168,  0.0177,  ...,  0.0170, -0.0060,  0.0154],\n",
       "         [ 0.0014, -0.0053,  0.0140,  ..., -0.0117, -0.0171,  0.0113],\n",
       "         [ 0.0158, -0.0094,  0.0042,  ...,  0.0194, -0.0090,  0.0007],\n",
       "         ...,\n",
       "         [-0.0185, -0.0174, -0.0152,  ...,  0.0116, -0.0100,  0.0179],\n",
       "         [ 0.0120, -0.0073,  0.0123,  ...,  0.0055,  0.0037, -0.0142],\n",
       "         [-0.0067, -0.0176, -0.0146,  ...,  0.0133, -0.0035, -0.0019]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight': tensor([[-1.9325e-02,  5.6365e-03, -3.3444e-05,  ...,  1.3273e-02,\n",
       "           8.2092e-03, -1.3188e-02],\n",
       "         [-2.9626e-03, -1.0263e-02,  1.4775e-02,  ..., -1.7392e-02,\n",
       "          -7.1735e-03,  8.4075e-03],\n",
       "         [-1.2189e-02,  6.1884e-03,  9.4021e-03,  ..., -1.9333e-02,\n",
       "           1.8900e-02,  2.7382e-04],\n",
       "         ...,\n",
       "         [ 1.1040e-02,  9.3821e-03,  7.0007e-03,  ..., -9.8029e-04,\n",
       "           2.6432e-03,  4.0806e-03],\n",
       "         [ 1.8551e-02,  1.7207e-02,  4.8284e-03,  ...,  9.4531e-03,\n",
       "           1.6511e-02,  1.3854e-03],\n",
       "         [-1.3240e-03,  1.6121e-02,  1.4971e-03,  ...,  1.2386e-02,\n",
       "           6.0084e-03, -1.6070e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight': tensor([[-0.0004, -0.0063, -0.0124,  ..., -0.0074,  0.0138,  0.0042],\n",
       "         [ 0.0153,  0.0176,  0.0165,  ...,  0.0147,  0.0149,  0.0091],\n",
       "         [ 0.0042, -0.0135, -0.0042,  ...,  0.0028, -0.0034, -0.0157],\n",
       "         ...,\n",
       "         [-0.0030, -0.0009, -0.0114,  ..., -0.0048, -0.0165,  0.0194],\n",
       "         [ 0.0080, -0.0113,  0.0129,  ..., -0.0021, -0.0010, -0.0099],\n",
       "         [ 0.0064,  0.0056,  0.0107,  ...,  0.0004, -0.0183,  0.0194]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight': tensor([[ 0.0015, -0.0183, -0.0153,  ..., -0.0016, -0.0142,  0.0110],\n",
       "         [ 0.0088,  0.0073,  0.0183,  ..., -0.0049,  0.0114,  0.0170],\n",
       "         [-0.0097,  0.0081,  0.0067,  ..., -0.0195,  0.0017, -0.0191],\n",
       "         ...,\n",
       "         [ 0.0061, -0.0063, -0.0039,  ...,  0.0049,  0.0111, -0.0089],\n",
       "         [ 0.0149, -0.0009,  0.0158,  ...,  0.0079,  0.0165, -0.0101],\n",
       "         [ 0.0098,  0.0189, -0.0142,  ..., -0.0088, -0.0063, -0.0131]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight': tensor([[-0.0023, -0.0177, -0.0166,  ..., -0.0063,  0.0062, -0.0103],\n",
       "         [ 0.0124,  0.0100, -0.0149,  ..., -0.0038,  0.0071, -0.0105],\n",
       "         [ 0.0115,  0.0186, -0.0129,  ..., -0.0179, -0.0098, -0.0055],\n",
       "         ...,\n",
       "         [-0.0143,  0.0175,  0.0150,  ..., -0.0179, -0.0015,  0.0111],\n",
       "         [ 0.0007,  0.0034, -0.0046,  ..., -0.0011, -0.0157,  0.0112],\n",
       "         [ 0.0114, -0.0131, -0.0022,  ...,  0.0139,  0.0139, -0.0063]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight': tensor([[-0.0170, -0.0030, -0.0035,  ..., -0.0093,  0.0016,  0.0128],\n",
       "         [ 0.0165,  0.0116, -0.0062,  ...,  0.0148, -0.0123, -0.0062],\n",
       "         [-0.0161,  0.0087,  0.0077,  ...,  0.0107, -0.0195, -0.0154],\n",
       "         ...,\n",
       "         [ 0.0035, -0.0086, -0.0137,  ..., -0.0128, -0.0068,  0.0086],\n",
       "         [ 0.0104, -0.0038, -0.0077,  ..., -0.0027, -0.0040,  0.0056],\n",
       "         [-0.0112,  0.0111,  0.0047,  ..., -0.0040,  0.0043,  0.0190]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight': tensor([[-0.0126, -0.0175,  0.0070,  ...,  0.0055, -0.0082, -0.0131],\n",
       "         [ 0.0175,  0.0169, -0.0140,  ..., -0.0121,  0.0190,  0.0093],\n",
       "         [ 0.0025,  0.0054, -0.0025,  ..., -0.0125, -0.0115,  0.0009],\n",
       "         ...,\n",
       "         [-0.0194,  0.0195, -0.0187,  ...,  0.0188,  0.0012,  0.0115],\n",
       "         [ 0.0024,  0.0159, -0.0080,  ...,  0.0135,  0.0031, -0.0114],\n",
       "         [-0.0140,  0.0076, -0.0031,  ..., -0.0075, -0.0046, -0.0122]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight': tensor([[ 0.0138, -0.0105, -0.0162,  ...,  0.0080,  0.0071,  0.0175],\n",
       "         [-0.0026,  0.0041,  0.0195,  ..., -0.0064,  0.0136,  0.0181],\n",
       "         [-0.0157,  0.0002, -0.0040,  ...,  0.0096,  0.0041,  0.0039],\n",
       "         ...,\n",
       "         [ 0.0133,  0.0140, -0.0168,  ...,  0.0068, -0.0044, -0.0189],\n",
       "         [-0.0139,  0.0047, -0.0145,  ...,  0.0118,  0.0093, -0.0185],\n",
       "         [-0.0132, -0.0093,  0.0191,  ...,  0.0111, -0.0088,  0.0159]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight': tensor([[-0.0179,  0.0168,  0.0150,  ..., -0.0142, -0.0021,  0.0109],\n",
       "         [-0.0182, -0.0059, -0.0023,  ...,  0.0135, -0.0095,  0.0142],\n",
       "         [-0.0003, -0.0170,  0.0086,  ...,  0.0180,  0.0020,  0.0040],\n",
       "         ...,\n",
       "         [-0.0069, -0.0191, -0.0167,  ..., -0.0087, -0.0174, -0.0061],\n",
       "         [-0.0178, -0.0054, -0.0029,  ...,  0.0087, -0.0009,  0.0034],\n",
       "         [ 0.0024,  0.0127,  0.0102,  ..., -0.0118,  0.0029,  0.0065]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight': tensor([[-0.0023,  0.0119, -0.0017,  ..., -0.0055,  0.0054, -0.0057],\n",
       "         [ 0.0069,  0.0170, -0.0038,  ...,  0.0024,  0.0123,  0.0069],\n",
       "         [ 0.0026,  0.0068, -0.0015,  ..., -0.0079,  0.0144, -0.0086],\n",
       "         ...,\n",
       "         [ 0.0069, -0.0023,  0.0015,  ..., -0.0153, -0.0069,  0.0160],\n",
       "         [ 0.0023, -0.0041, -0.0081,  ..., -0.0103,  0.0062,  0.0010],\n",
       "         [-0.0139,  0.0087, -0.0108,  ..., -0.0177, -0.0090, -0.0007]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight': tensor([[ 1.2868e-02, -8.9761e-03, -1.0239e-02,  ...,  2.0680e-03,\n",
       "          -8.8112e-03,  9.9547e-03],\n",
       "         [-1.1156e-02,  4.6044e-04,  9.6251e-03,  ..., -3.2234e-03,\n",
       "          -4.4459e-03,  9.6679e-03],\n",
       "         [-2.8426e-03,  1.7009e-02,  1.4835e-02,  ..., -1.5877e-03,\n",
       "          -4.4759e-03, -2.3851e-03],\n",
       "         ...,\n",
       "         [ 2.1169e-03, -1.1584e-02, -7.9249e-03,  ..., -1.1628e-02,\n",
       "           1.3631e-02, -3.5299e-03],\n",
       "         [-1.4411e-02,  2.7183e-03,  2.7374e-03,  ...,  1.1407e-02,\n",
       "          -2.8099e-03, -1.6572e-02],\n",
       "         [ 1.6546e-02,  3.1247e-04,  1.7457e-02,  ...,  2.6798e-05,\n",
       "           5.8788e-03, -7.1700e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight': tensor([[ 0.0043, -0.0185, -0.0100,  ...,  0.0059,  0.0144, -0.0082],\n",
       "         [ 0.0141, -0.0099,  0.0193,  ...,  0.0137, -0.0172, -0.0050],\n",
       "         [-0.0041, -0.0069,  0.0107,  ...,  0.0107,  0.0154, -0.0096],\n",
       "         ...,\n",
       "         [ 0.0165,  0.0036,  0.0137,  ..., -0.0157,  0.0132, -0.0116],\n",
       "         [ 0.0057,  0.0087, -0.0172,  ...,  0.0077, -0.0073,  0.0097],\n",
       "         [ 0.0084,  0.0046,  0.0092,  ..., -0.0131, -0.0187,  0.0062]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight': tensor([[ 0.0137, -0.0145,  0.0014,  ...,  0.0110,  0.0066, -0.0140],\n",
       "         [-0.0141, -0.0088,  0.0094,  ..., -0.0133,  0.0064, -0.0015],\n",
       "         [ 0.0041, -0.0119, -0.0101,  ..., -0.0158, -0.0081,  0.0162],\n",
       "         ...,\n",
       "         [-0.0058,  0.0017,  0.0086,  ..., -0.0067,  0.0022, -0.0154],\n",
       "         [ 0.0136, -0.0131,  0.0064,  ...,  0.0152, -0.0037,  0.0019],\n",
       "         [ 0.0131,  0.0176,  0.0115,  ...,  0.0194,  0.0197,  0.0101]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight': tensor([[ 0.0123, -0.0053, -0.0146,  ..., -0.0156,  0.0170, -0.0196],\n",
       "         [-0.0062, -0.0024,  0.0085,  ..., -0.0154, -0.0140, -0.0154],\n",
       "         [ 0.0107, -0.0184,  0.0096,  ..., -0.0126, -0.0071, -0.0121],\n",
       "         ...,\n",
       "         [ 0.0050, -0.0108, -0.0137,  ..., -0.0094, -0.0115,  0.0109],\n",
       "         [-0.0170, -0.0024, -0.0179,  ..., -0.0181, -0.0182,  0.0166],\n",
       "         [-0.0056, -0.0034,  0.0094,  ..., -0.0121, -0.0176,  0.0098]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight': tensor([[ 3.8934e-03, -5.9288e-06,  1.1697e-02,  ...,  9.9246e-03,\n",
       "          -5.2438e-03,  8.7560e-03],\n",
       "         [-1.5490e-03,  2.6222e-03, -1.2751e-02,  ...,  3.9259e-03,\n",
       "           1.3323e-02,  9.7304e-03],\n",
       "         [ 3.5938e-03, -1.1453e-02, -1.3048e-02,  ...,  6.5085e-03,\n",
       "           3.0513e-03, -6.5141e-03],\n",
       "         ...,\n",
       "         [-1.5813e-02,  1.0715e-02, -1.0281e-02,  ...,  7.2947e-03,\n",
       "           6.7384e-03, -2.0164e-03],\n",
       "         [-1.8794e-02,  6.2840e-03,  4.3452e-03,  ..., -1.5489e-02,\n",
       "          -1.2159e-02,  1.2138e-02],\n",
       "         [-1.2220e-02, -9.0250e-03,  1.7922e-02,  ..., -1.4962e-02,\n",
       "           2.1299e-03, -7.3116e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight': tensor([[ 0.0089, -0.0082, -0.0097,  ...,  0.0130, -0.0077,  0.0024],\n",
       "         [-0.0054, -0.0004,  0.0108,  ...,  0.0093, -0.0187,  0.0182],\n",
       "         [-0.0140,  0.0144, -0.0032,  ..., -0.0158,  0.0027,  0.0037],\n",
       "         ...,\n",
       "         [ 0.0024,  0.0166, -0.0131,  ...,  0.0019, -0.0096, -0.0113],\n",
       "         [ 0.0126,  0.0176,  0.0073,  ...,  0.0093,  0.0130,  0.0018],\n",
       "         [-0.0074, -0.0115, -0.0091,  ...,  0.0059, -0.0072,  0.0063]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight': tensor([[-0.0036, -0.0041, -0.0010,  ...,  0.0006, -0.0069, -0.0025],\n",
       "         [-0.0114, -0.0093, -0.0005,  ...,  0.0156,  0.0046, -0.0031],\n",
       "         [-0.0020, -0.0052,  0.0025,  ...,  0.0088,  0.0133,  0.0042],\n",
       "         ...,\n",
       "         [-0.0023, -0.0012,  0.0105,  ..., -0.0195,  0.0043,  0.0039],\n",
       "         [ 0.0121,  0.0087,  0.0116,  ...,  0.0067, -0.0135, -0.0037],\n",
       "         [-0.0054, -0.0034, -0.0063,  ...,  0.0166,  0.0185,  0.0144]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight': tensor([[-0.0119,  0.0093,  0.0186,  ...,  0.0181,  0.0187, -0.0162],\n",
       "         [ 0.0195,  0.0034, -0.0059,  ..., -0.0136, -0.0110, -0.0045],\n",
       "         [ 0.0082, -0.0045,  0.0115,  ...,  0.0140,  0.0197, -0.0171],\n",
       "         ...,\n",
       "         [ 0.0143, -0.0136, -0.0022,  ...,  0.0110, -0.0196,  0.0005],\n",
       "         [-0.0124,  0.0049,  0.0170,  ...,  0.0018, -0.0182,  0.0089],\n",
       "         [-0.0122, -0.0120,  0.0019,  ..., -0.0128,  0.0045,  0.0044]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight': tensor([[-1.1186e-03,  1.8473e-02, -1.3449e-02,  ..., -7.6955e-03,\n",
       "           1.6376e-02, -8.2250e-03],\n",
       "         [-1.7267e-02, -1.6934e-02, -8.5463e-03,  ..., -1.7258e-02,\n",
       "           7.2462e-03,  1.5097e-02],\n",
       "         [-1.1578e-02, -1.1710e-02, -9.6221e-05,  ..., -4.8852e-03,\n",
       "          -7.3122e-03, -7.6225e-03],\n",
       "         ...,\n",
       "         [-1.5262e-02,  6.2400e-03, -7.8254e-04,  ..., -1.9239e-02,\n",
       "           1.8891e-02,  2.1059e-04],\n",
       "         [-5.4041e-03, -7.4826e-03, -2.9659e-03,  ..., -1.1169e-02,\n",
       "           1.3966e-02, -4.4662e-03],\n",
       "         [-7.3834e-03, -1.0001e-02, -1.0320e-02,  ..., -5.0415e-03,\n",
       "           2.2421e-03, -1.6744e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight': tensor([[-1.8482e-03, -1.1662e-02, -9.1223e-05,  ..., -1.3681e-02,\n",
       "          -1.9178e-02, -1.1715e-02],\n",
       "         [ 1.9193e-02, -1.4103e-02, -6.0867e-03,  ..., -9.4388e-03,\n",
       "           2.2715e-03, -1.9682e-02],\n",
       "         [-1.8547e-02, -8.8514e-03,  5.2421e-03,  ...,  1.4844e-02,\n",
       "          -1.5142e-02, -1.6828e-02],\n",
       "         ...,\n",
       "         [ 5.8498e-03, -1.5475e-02,  1.3537e-02,  ...,  1.4347e-02,\n",
       "          -3.2262e-03,  9.8662e-03],\n",
       "         [-5.9404e-03, -4.5129e-03, -5.4500e-04,  ..., -1.7264e-03,\n",
       "           5.9262e-03,  4.1465e-03],\n",
       "         [-1.0303e-02,  1.9750e-02, -4.6734e-03,  ...,  1.3800e-03,\n",
       "          -6.0978e-04,  5.9537e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight': tensor([[-0.0195, -0.0038,  0.0076,  ...,  0.0083, -0.0109,  0.0151],\n",
       "         [-0.0164, -0.0082, -0.0070,  ..., -0.0157,  0.0195,  0.0092],\n",
       "         [ 0.0097,  0.0037, -0.0071,  ...,  0.0004,  0.0084,  0.0161],\n",
       "         ...,\n",
       "         [-0.0181,  0.0063,  0.0167,  ..., -0.0020,  0.0139, -0.0133],\n",
       "         [ 0.0146,  0.0023, -0.0072,  ...,  0.0055, -0.0086, -0.0143],\n",
       "         [ 0.0140,  0.0078, -0.0196,  ...,  0.0139,  0.0162, -0.0086]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight': tensor([[ 0.0060, -0.0165,  0.0010,  ...,  0.0118, -0.0075,  0.0046],\n",
       "         [-0.0016, -0.0119, -0.0021,  ..., -0.0099,  0.0181, -0.0164],\n",
       "         [ 0.0122,  0.0003,  0.0183,  ...,  0.0055,  0.0065,  0.0166],\n",
       "         ...,\n",
       "         [ 0.0018,  0.0094, -0.0166,  ..., -0.0103,  0.0173, -0.0163],\n",
       "         [ 0.0078, -0.0089,  0.0181,  ...,  0.0192,  0.0162, -0.0032],\n",
       "         [ 0.0101,  0.0096,  0.0061,  ...,  0.0021,  0.0060, -0.0016]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight': tensor([[-0.0138, -0.0070, -0.0027,  ...,  0.0018,  0.0192, -0.0123],\n",
       "         [ 0.0118,  0.0185, -0.0129,  ..., -0.0167,  0.0153,  0.0178],\n",
       "         [ 0.0130, -0.0177,  0.0130,  ...,  0.0012,  0.0077,  0.0156],\n",
       "         ...,\n",
       "         [ 0.0178, -0.0142,  0.0019,  ...,  0.0191,  0.0034,  0.0092],\n",
       "         [ 0.0054, -0.0150,  0.0160,  ...,  0.0015,  0.0086, -0.0144],\n",
       "         [-0.0167, -0.0022,  0.0158,  ...,  0.0069,  0.0018, -0.0054]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight': tensor([[ 2.3028e-05,  1.0345e-02, -1.1703e-03,  ..., -1.9564e-02,\n",
       "           1.5273e-02, -8.2668e-03],\n",
       "         [ 6.6287e-03, -6.7351e-03,  1.3005e-02,  ...,  1.0590e-02,\n",
       "          -6.1847e-03, -1.6078e-02],\n",
       "         [-3.8481e-03,  3.7385e-03, -5.9733e-03,  ..., -1.0982e-02,\n",
       "           1.2098e-02,  1.1732e-02],\n",
       "         ...,\n",
       "         [ 1.3620e-03, -1.7536e-02, -3.3058e-04,  ..., -1.4779e-02,\n",
       "          -3.6710e-04, -1.5924e-02],\n",
       "         [-1.5107e-02,  1.1764e-02,  9.9406e-03,  ...,  8.6600e-03,\n",
       "           1.1995e-02,  5.7848e-03],\n",
       "         [-6.5420e-03, -4.1527e-03, -1.0183e-02,  ...,  4.1370e-04,\n",
       "           4.1437e-03,  1.5127e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight': tensor([[ 0.0128, -0.0084,  0.0023,  ..., -0.0028, -0.0075, -0.0137],\n",
       "         [-0.0186,  0.0103,  0.0015,  ..., -0.0107, -0.0063,  0.0037],\n",
       "         [-0.0147, -0.0178,  0.0021,  ..., -0.0005, -0.0040, -0.0147],\n",
       "         ...,\n",
       "         [ 0.0036,  0.0015, -0.0187,  ..., -0.0114, -0.0192, -0.0097],\n",
       "         [ 0.0036, -0.0195, -0.0195,  ..., -0.0178,  0.0182, -0.0078],\n",
       "         [ 0.0185, -0.0075, -0.0065,  ...,  0.0072,  0.0173, -0.0061]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight': tensor([[ 0.0093,  0.0158, -0.0114,  ...,  0.0181, -0.0017,  0.0032],\n",
       "         [-0.0177,  0.0047,  0.0067,  ...,  0.0020,  0.0143, -0.0161],\n",
       "         [-0.0135, -0.0118, -0.0126,  ..., -0.0191, -0.0019, -0.0154],\n",
       "         ...,\n",
       "         [ 0.0103,  0.0046, -0.0036,  ...,  0.0036,  0.0078,  0.0077],\n",
       "         [-0.0132, -0.0087,  0.0026,  ...,  0.0143,  0.0132,  0.0114],\n",
       "         [-0.0158,  0.0033, -0.0143,  ...,  0.0188, -0.0026, -0.0055]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight': tensor([[-0.0037,  0.0192,  0.0192,  ...,  0.0031, -0.0068,  0.0116],\n",
       "         [-0.0071,  0.0063, -0.0064,  ..., -0.0146,  0.0104, -0.0070],\n",
       "         [ 0.0166, -0.0061,  0.0154,  ...,  0.0179,  0.0079, -0.0002],\n",
       "         ...,\n",
       "         [ 0.0160,  0.0176, -0.0061,  ..., -0.0004,  0.0195,  0.0085],\n",
       "         [ 0.0148,  0.0193, -0.0113,  ...,  0.0131,  0.0016,  0.0134],\n",
       "         [-0.0106,  0.0054, -0.0074,  ...,  0.0104, -0.0137,  0.0186]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight': tensor([[-0.0146, -0.0163,  0.0131,  ..., -0.0072, -0.0085,  0.0088],\n",
       "         [ 0.0135, -0.0007,  0.0176,  ...,  0.0172,  0.0110, -0.0144],\n",
       "         [ 0.0081,  0.0013,  0.0129,  ..., -0.0038,  0.0038, -0.0143],\n",
       "         ...,\n",
       "         [-0.0152,  0.0006, -0.0170,  ..., -0.0028,  0.0053, -0.0035],\n",
       "         [-0.0043,  0.0015,  0.0010,  ...,  0.0107, -0.0134,  0.0055],\n",
       "         [ 0.0192,  0.0146,  0.0093,  ..., -0.0111, -0.0112,  0.0091]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight': tensor([[ 0.0186, -0.0060, -0.0062,  ...,  0.0114, -0.0017,  0.0126],\n",
       "         [ 0.0175, -0.0021,  0.0124,  ..., -0.0131, -0.0032,  0.0113],\n",
       "         [ 0.0009, -0.0055,  0.0105,  ...,  0.0134,  0.0044, -0.0014],\n",
       "         ...,\n",
       "         [ 0.0159, -0.0082, -0.0152,  ...,  0.0164, -0.0103, -0.0042],\n",
       "         [-0.0125,  0.0180,  0.0025,  ..., -0.0175, -0.0029,  0.0197],\n",
       "         [-0.0046, -0.0003, -0.0109,  ..., -0.0041,  0.0005, -0.0187]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight': tensor([[ 9.2750e-03, -1.8114e-02,  3.8951e-03,  ..., -9.4278e-05,\n",
       "           1.5961e-02, -7.1110e-03],\n",
       "         [ 1.2239e-03,  1.1494e-02, -1.1518e-02,  ...,  1.7007e-04,\n",
       "          -4.5548e-03,  1.4889e-02],\n",
       "         [-2.5354e-03,  1.6349e-02, -9.8392e-03,  ...,  7.7332e-03,\n",
       "          -3.0884e-03, -1.1719e-02],\n",
       "         ...,\n",
       "         [-7.1491e-03, -1.2009e-02, -8.6421e-04,  ...,  4.1179e-03,\n",
       "          -1.7439e-03, -6.4273e-03],\n",
       "         [-1.2700e-02,  1.7179e-02, -1.9212e-02,  ...,  4.7549e-03,\n",
       "          -5.5263e-03,  1.7718e-02],\n",
       "         [ 1.7798e-02,  1.4052e-02,  6.7571e-03,  ..., -7.0269e-03,\n",
       "          -8.3760e-03, -3.5925e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight': tensor([[ 0.0045, -0.0097, -0.0035,  ...,  0.0123, -0.0060, -0.0030],\n",
       "         [-0.0106, -0.0161,  0.0033,  ...,  0.0122, -0.0008,  0.0116],\n",
       "         [-0.0164, -0.0045,  0.0091,  ..., -0.0079,  0.0093,  0.0175],\n",
       "         ...,\n",
       "         [-0.0021,  0.0008, -0.0067,  ..., -0.0102, -0.0113, -0.0008],\n",
       "         [ 0.0175, -0.0053,  0.0028,  ...,  0.0154,  0.0112, -0.0087],\n",
       "         [-0.0125, -0.0106,  0.0029,  ..., -0.0181, -0.0118, -0.0036]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight': tensor([[-0.0134, -0.0100, -0.0028,  ..., -0.0127, -0.0002, -0.0091],\n",
       "         [-0.0126, -0.0109,  0.0027,  ...,  0.0020, -0.0058,  0.0139],\n",
       "         [-0.0057, -0.0112,  0.0171,  ..., -0.0166, -0.0094,  0.0030],\n",
       "         ...,\n",
       "         [-0.0026,  0.0188,  0.0027,  ...,  0.0059, -0.0110, -0.0144],\n",
       "         [-0.0195, -0.0164, -0.0122,  ..., -0.0042,  0.0126,  0.0057],\n",
       "         [-0.0119,  0.0183,  0.0058,  ...,  0.0037, -0.0196, -0.0015]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight': tensor([[ 1.0996e-02, -1.5532e-02,  6.3503e-04,  ...,  9.5928e-04,\n",
       "           7.3312e-03, -5.9132e-03],\n",
       "         [ 1.2896e-02, -1.4866e-03,  3.1732e-04,  ...,  1.2742e-02,\n",
       "           7.7115e-03, -1.8269e-02],\n",
       "         [ 9.8217e-03,  1.2347e-02,  1.7238e-02,  ..., -1.4976e-02,\n",
       "           1.9199e-03, -2.2244e-05],\n",
       "         ...,\n",
       "         [-1.2334e-02, -5.0529e-03,  1.6628e-02,  ...,  1.2777e-02,\n",
       "           2.3241e-03, -6.3684e-03],\n",
       "         [ 8.8336e-03,  1.2643e-02,  1.3732e-02,  ..., -2.5276e-03,\n",
       "           8.3080e-03, -1.4662e-02],\n",
       "         [-1.9313e-02, -1.6558e-02, -1.0176e-02,  ..., -1.4036e-02,\n",
       "          -1.2643e-02,  1.3990e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight': tensor([[ 0.0120, -0.0134, -0.0124,  ...,  0.0141,  0.0191,  0.0084],\n",
       "         [-0.0088,  0.0014, -0.0010,  ...,  0.0190,  0.0034, -0.0050],\n",
       "         [ 0.0027, -0.0172, -0.0008,  ...,  0.0050,  0.0162,  0.0188],\n",
       "         ...,\n",
       "         [-0.0147,  0.0147,  0.0054,  ..., -0.0100, -0.0013,  0.0139],\n",
       "         [ 0.0085, -0.0074, -0.0177,  ..., -0.0111,  0.0108,  0.0069],\n",
       "         [-0.0090, -0.0120, -0.0068,  ...,  0.0005,  0.0039,  0.0179]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight': tensor([[-0.0089, -0.0055,  0.0031,  ...,  0.0138,  0.0151, -0.0193],\n",
       "         [ 0.0174,  0.0051, -0.0085,  ...,  0.0057,  0.0177, -0.0095],\n",
       "         [-0.0195,  0.0134,  0.0193,  ..., -0.0157, -0.0026,  0.0090],\n",
       "         ...,\n",
       "         [-0.0064, -0.0069,  0.0053,  ...,  0.0176,  0.0061, -0.0125],\n",
       "         [ 0.0016,  0.0177,  0.0081,  ..., -0.0130,  0.0133,  0.0112],\n",
       "         [ 0.0051,  0.0189,  0.0104,  ..., -0.0092,  0.0075,  0.0045]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight': tensor([[-0.0130, -0.0017,  0.0053,  ...,  0.0005, -0.0111, -0.0120],\n",
       "         [-0.0190, -0.0033, -0.0027,  ...,  0.0026,  0.0103,  0.0112],\n",
       "         [-0.0150, -0.0024,  0.0177,  ..., -0.0038,  0.0129, -0.0009],\n",
       "         ...,\n",
       "         [ 0.0129, -0.0192,  0.0145,  ..., -0.0171, -0.0088, -0.0122],\n",
       "         [ 0.0038, -0.0095,  0.0190,  ..., -0.0180,  0.0137,  0.0169],\n",
       "         [-0.0021, -0.0096,  0.0020,  ...,  0.0141,  0.0050,  0.0112]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight': tensor([[ 0.0137, -0.0066,  0.0047,  ..., -0.0181, -0.0033, -0.0071],\n",
       "         [ 0.0062,  0.0147, -0.0080,  ...,  0.0024,  0.0082, -0.0139],\n",
       "         [ 0.0026,  0.0017, -0.0008,  ..., -0.0085, -0.0062,  0.0038],\n",
       "         ...,\n",
       "         [ 0.0196, -0.0016, -0.0160,  ..., -0.0020, -0.0059, -0.0075],\n",
       "         [ 0.0135,  0.0192, -0.0193,  ...,  0.0088, -0.0161,  0.0132],\n",
       "         [-0.0194,  0.0034,  0.0126,  ...,  0.0003, -0.0112, -0.0077]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight': tensor([[-0.0166, -0.0048,  0.0072,  ..., -0.0042, -0.0100, -0.0147],\n",
       "         [ 0.0097, -0.0111, -0.0165,  ...,  0.0172, -0.0004,  0.0195],\n",
       "         [-0.0183, -0.0181, -0.0096,  ...,  0.0084,  0.0164,  0.0058],\n",
       "         ...,\n",
       "         [-0.0114, -0.0133,  0.0049,  ..., -0.0137, -0.0070, -0.0046],\n",
       "         [ 0.0067,  0.0013, -0.0180,  ..., -0.0196,  0.0138,  0.0095],\n",
       "         [-0.0065, -0.0038,  0.0031,  ..., -0.0130,  0.0175, -0.0059]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight': tensor([[ 0.0101, -0.0196,  0.0035,  ..., -0.0185,  0.0068, -0.0117],\n",
       "         [ 0.0165, -0.0139,  0.0134,  ..., -0.0174, -0.0191, -0.0186],\n",
       "         [ 0.0046,  0.0044, -0.0034,  ...,  0.0046,  0.0025, -0.0115],\n",
       "         ...,\n",
       "         [-0.0061,  0.0151, -0.0038,  ...,  0.0159, -0.0178, -0.0069],\n",
       "         [ 0.0009, -0.0049,  0.0171,  ..., -0.0043,  0.0062, -0.0112],\n",
       "         [ 0.0161,  0.0028, -0.0094,  ..., -0.0112, -0.0121, -0.0077]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight': tensor([[-0.0014,  0.0050, -0.0033,  ...,  0.0146,  0.0144, -0.0124],\n",
       "         [ 0.0114, -0.0114,  0.0111,  ..., -0.0012, -0.0042, -0.0147],\n",
       "         [-0.0169, -0.0108,  0.0128,  ...,  0.0143,  0.0153,  0.0094],\n",
       "         ...,\n",
       "         [-0.0124,  0.0189,  0.0050,  ...,  0.0152, -0.0053, -0.0087],\n",
       "         [-0.0176,  0.0005,  0.0083,  ...,  0.0108, -0.0175,  0.0111],\n",
       "         [ 0.0063,  0.0159,  0.0097,  ..., -0.0021,  0.0189,  0.0054]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight': tensor([[ 0.0055,  0.0084,  0.0077,  ..., -0.0107,  0.0008,  0.0138],\n",
       "         [ 0.0084, -0.0011,  0.0013,  ...,  0.0148,  0.0048, -0.0005],\n",
       "         [ 0.0134, -0.0008,  0.0175,  ..., -0.0160,  0.0083,  0.0154],\n",
       "         ...,\n",
       "         [ 0.0178, -0.0102, -0.0118,  ...,  0.0149, -0.0128, -0.0065],\n",
       "         [-0.0045, -0.0009,  0.0016,  ..., -0.0035,  0.0075,  0.0086],\n",
       "         [-0.0163,  0.0064,  0.0043,  ..., -0.0075, -0.0134, -0.0017]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight': tensor([[ 0.0027, -0.0031, -0.0100,  ..., -0.0187, -0.0159, -0.0008],\n",
       "         [-0.0037,  0.0106, -0.0184,  ..., -0.0187,  0.0135,  0.0005],\n",
       "         [ 0.0139, -0.0133,  0.0034,  ..., -0.0129, -0.0068, -0.0156],\n",
       "         ...,\n",
       "         [ 0.0021, -0.0028, -0.0079,  ..., -0.0012,  0.0145, -0.0007],\n",
       "         [-0.0153,  0.0105,  0.0144,  ...,  0.0056, -0.0029,  0.0096],\n",
       "         [ 0.0004, -0.0124, -0.0182,  ...,  0.0078, -0.0038,  0.0081]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight': tensor([[ 0.0019, -0.0077, -0.0029,  ..., -0.0145,  0.0027, -0.0009],\n",
       "         [ 0.0149, -0.0013, -0.0057,  ..., -0.0196, -0.0156, -0.0055],\n",
       "         [-0.0091,  0.0079,  0.0099,  ...,  0.0164,  0.0056,  0.0131],\n",
       "         ...,\n",
       "         [ 0.0122, -0.0044,  0.0181,  ..., -0.0114,  0.0155, -0.0149],\n",
       "         [ 0.0186,  0.0124,  0.0188,  ...,  0.0056, -0.0044, -0.0095],\n",
       "         [-0.0067,  0.0026,  0.0050,  ...,  0.0006, -0.0089, -0.0176]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight': tensor([[ 0.0006, -0.0177,  0.0104,  ...,  0.0103,  0.0181,  0.0141],\n",
       "         [ 0.0136, -0.0173, -0.0139,  ..., -0.0116,  0.0086,  0.0020],\n",
       "         [-0.0167, -0.0045, -0.0028,  ...,  0.0191,  0.0191, -0.0106],\n",
       "         ...,\n",
       "         [-0.0139, -0.0124, -0.0003,  ..., -0.0097, -0.0160,  0.0008],\n",
       "         [ 0.0024, -0.0140,  0.0025,  ..., -0.0081, -0.0137, -0.0178],\n",
       "         [ 0.0108,  0.0117,  0.0129,  ...,  0.0059,  0.0083,  0.0038]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight': tensor([[ 0.0039,  0.0072,  0.0054,  ..., -0.0054,  0.0016,  0.0116],\n",
       "         [-0.0172, -0.0008, -0.0129,  ..., -0.0153,  0.0196, -0.0082],\n",
       "         [ 0.0130,  0.0048,  0.0142,  ..., -0.0127, -0.0190,  0.0141],\n",
       "         ...,\n",
       "         [-0.0100, -0.0149, -0.0109,  ..., -0.0106, -0.0104,  0.0044],\n",
       "         [ 0.0172,  0.0061,  0.0019,  ...,  0.0111,  0.0037, -0.0159],\n",
       "         [ 0.0155,  0.0077, -0.0050,  ...,  0.0038,  0.0123, -0.0013]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight': tensor([[-0.0141,  0.0189, -0.0157,  ..., -0.0121, -0.0024, -0.0177],\n",
       "         [ 0.0168,  0.0197,  0.0139,  ...,  0.0169, -0.0127, -0.0127],\n",
       "         [ 0.0019, -0.0121, -0.0103,  ..., -0.0064, -0.0014,  0.0072],\n",
       "         ...,\n",
       "         [ 0.0104, -0.0074,  0.0068,  ...,  0.0168,  0.0072, -0.0109],\n",
       "         [-0.0107,  0.0065,  0.0097,  ...,  0.0115,  0.0194,  0.0099],\n",
       "         [-0.0040,  0.0041,  0.0158,  ...,  0.0036, -0.0113, -0.0180]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.weight': tensor([[ 0.0036,  0.0043,  0.0090,  ...,  0.0065,  0.0139,  0.0122],\n",
       "         [ 0.0032, -0.0023, -0.0137,  ..., -0.0175, -0.0001, -0.0108],\n",
       "         [-0.0061,  0.0008,  0.0138,  ...,  0.0105,  0.0170, -0.0065],\n",
       "         ...,\n",
       "         [-0.0174,  0.0064, -0.0107,  ..., -0.0172,  0.0001, -0.0035],\n",
       "         [ 0.0004, -0.0105,  0.0025,  ...,  0.0047, -0.0046, -0.0182],\n",
       "         [-0.0185, -0.0133, -0.0065,  ...,  0.0046,  0.0099, -0.0156]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight': tensor([[-0.0030, -0.0014,  0.0029,  ..., -0.0104, -0.0193, -0.0185],\n",
       "         [ 0.0188,  0.0102, -0.0008,  ...,  0.0114, -0.0032, -0.0129],\n",
       "         [-0.0126,  0.0167,  0.0086,  ...,  0.0164,  0.0025, -0.0143],\n",
       "         ...,\n",
       "         [-0.0127,  0.0177,  0.0141,  ..., -0.0117, -0.0054, -0.0068],\n",
       "         [-0.0032,  0.0134, -0.0094,  ..., -0.0145, -0.0084,  0.0106],\n",
       "         [-0.0056,  0.0010, -0.0054,  ...,  0.0190,  0.0110,  0.0039]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.weight': tensor([[-0.0193,  0.0134, -0.0099,  ..., -0.0146,  0.0082, -0.0166],\n",
       "         [ 0.0105,  0.0093, -0.0174,  ..., -0.0066,  0.0057, -0.0026],\n",
       "         [ 0.0114,  0.0133, -0.0177,  ...,  0.0035,  0.0086, -0.0096],\n",
       "         ...,\n",
       "         [-0.0093,  0.0111,  0.0067,  ...,  0.0113, -0.0076, -0.0181],\n",
       "         [ 0.0096,  0.0048,  0.0021,  ...,  0.0155,  0.0010, -0.0029],\n",
       "         [-0.0077, -0.0009, -0.0139,  ..., -0.0137, -0.0175,  0.0168]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight': tensor([[-0.0081, -0.0143, -0.0074,  ...,  0.0127,  0.0089,  0.0117],\n",
       "         [ 0.0118, -0.0081, -0.0105,  ...,  0.0064, -0.0113, -0.0158],\n",
       "         [-0.0127, -0.0136,  0.0135,  ...,  0.0134, -0.0021, -0.0019],\n",
       "         ...,\n",
       "         [ 0.0102,  0.0070, -0.0020,  ..., -0.0027,  0.0171,  0.0158],\n",
       "         [-0.0177,  0.0154, -0.0052,  ...,  0.0023, -0.0033,  0.0027],\n",
       "         [-0.0021, -0.0096, -0.0145,  ...,  0.0124, -0.0128, -0.0031]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.weight': tensor([[-0.0052, -0.0087, -0.0002,  ..., -0.0166, -0.0194,  0.0193],\n",
       "         [ 0.0196, -0.0156,  0.0009,  ...,  0.0159, -0.0153, -0.0068],\n",
       "         [ 0.0027, -0.0124,  0.0096,  ...,  0.0045, -0.0089, -0.0080],\n",
       "         ...,\n",
       "         [-0.0138, -0.0175, -0.0097,  ..., -0.0132,  0.0057,  0.0059],\n",
       "         [-0.0111, -0.0081,  0.0196,  ..., -0.0159, -0.0006,  0.0058],\n",
       "         [ 0.0148, -0.0011,  0.0053,  ...,  0.0195, -0.0111,  0.0194]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight': tensor([[ 0.0079,  0.0116,  0.0032,  ..., -0.0141,  0.0058, -0.0051],\n",
       "         [-0.0047,  0.0084,  0.0020,  ...,  0.0104,  0.0067,  0.0098],\n",
       "         [ 0.0009,  0.0064, -0.0146,  ...,  0.0041,  0.0078,  0.0033],\n",
       "         ...,\n",
       "         [-0.0098, -0.0161,  0.0124,  ...,  0.0115,  0.0144,  0.0148],\n",
       "         [-0.0067, -0.0020, -0.0034,  ..., -0.0163, -0.0156,  0.0113],\n",
       "         [-0.0186,  0.0133,  0.0136,  ..., -0.0176,  0.0044,  0.0154]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.weight': tensor([[-0.0057,  0.0172,  0.0195,  ..., -0.0023,  0.0115,  0.0036],\n",
       "         [-0.0179,  0.0191,  0.0079,  ...,  0.0146, -0.0041, -0.0167],\n",
       "         [-0.0181,  0.0103, -0.0133,  ..., -0.0121,  0.0124,  0.0026],\n",
       "         ...,\n",
       "         [ 0.0036, -0.0133,  0.0166,  ..., -0.0094, -0.0146, -0.0129],\n",
       "         [-0.0028, -0.0048,  0.0060,  ..., -0.0088,  0.0011, -0.0061],\n",
       "         [-0.0036, -0.0165, -0.0175,  ...,  0.0114, -0.0016, -0.0021]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight': tensor([[ 0.0153,  0.0029,  0.0085,  ...,  0.0185, -0.0009,  0.0022],\n",
       "         [-0.0004, -0.0063,  0.0056,  ..., -0.0072, -0.0177,  0.0119],\n",
       "         [ 0.0035, -0.0133, -0.0085,  ..., -0.0151, -0.0022, -0.0037],\n",
       "         ...,\n",
       "         [ 0.0081,  0.0126, -0.0120,  ..., -0.0153,  0.0076, -0.0178],\n",
       "         [ 0.0042, -0.0179, -0.0012,  ..., -0.0160,  0.0175, -0.0122],\n",
       "         [ 0.0068, -0.0027, -0.0008,  ...,  0.0074, -0.0127, -0.0105]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.weight': tensor([[ 0.0194, -0.0193,  0.0080,  ..., -0.0083, -0.0112, -0.0138],\n",
       "         [ 0.0032, -0.0045, -0.0094,  ...,  0.0082, -0.0032, -0.0097],\n",
       "         [ 0.0107, -0.0012, -0.0042,  ..., -0.0041,  0.0097, -0.0179],\n",
       "         ...,\n",
       "         [ 0.0070, -0.0109,  0.0046,  ...,  0.0136, -0.0106, -0.0128],\n",
       "         [ 0.0104, -0.0092,  0.0117,  ..., -0.0040,  0.0109, -0.0017],\n",
       "         [-0.0004,  0.0001,  0.0063,  ..., -0.0078, -0.0013,  0.0167]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight': tensor([[-0.0160,  0.0179,  0.0118,  ...,  0.0149, -0.0197,  0.0125],\n",
       "         [ 0.0091, -0.0141,  0.0019,  ...,  0.0187, -0.0040,  0.0111],\n",
       "         [ 0.0134, -0.0072,  0.0080,  ..., -0.0101, -0.0172,  0.0022],\n",
       "         ...,\n",
       "         [ 0.0020, -0.0072,  0.0044,  ..., -0.0044, -0.0166, -0.0151],\n",
       "         [ 0.0030, -0.0096, -0.0108,  ...,  0.0192,  0.0145,  0.0164],\n",
       "         [-0.0076, -0.0153,  0.0009,  ...,  0.0089, -0.0071, -0.0065]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.weight': tensor([[ 0.0089, -0.0036,  0.0164,  ...,  0.0183, -0.0124,  0.0164],\n",
       "         [-0.0145,  0.0102,  0.0038,  ...,  0.0028, -0.0104, -0.0042],\n",
       "         [ 0.0167, -0.0086,  0.0195,  ..., -0.0067, -0.0021,  0.0120],\n",
       "         ...,\n",
       "         [ 0.0127,  0.0110,  0.0069,  ...,  0.0120, -0.0145,  0.0106],\n",
       "         [ 0.0160,  0.0133, -0.0151,  ...,  0.0178, -0.0172, -0.0095],\n",
       "         [ 0.0073, -0.0081, -0.0090,  ...,  0.0122,  0.0145,  0.0060]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight': tensor([[-3.7389e-03, -2.1499e-03,  5.8650e-03,  ...,  1.0877e-02,\n",
       "          -7.6997e-03,  1.0394e-02],\n",
       "         [-8.6380e-03,  8.4763e-03, -1.3345e-02,  ..., -9.1249e-03,\n",
       "           1.0928e-02,  2.2185e-03],\n",
       "         [-3.9198e-05, -1.0586e-02, -4.5521e-03,  ...,  3.5068e-03,\n",
       "           2.0743e-04,  1.1199e-02],\n",
       "         ...,\n",
       "         [ 1.1485e-02, -1.5960e-02,  9.1482e-03,  ..., -9.6459e-03,\n",
       "           5.6675e-03,  3.4994e-03],\n",
       "         [-1.8064e-02,  1.0246e-02, -4.0703e-03,  ...,  1.4169e-02,\n",
       "           3.3339e-03,  1.1284e-02],\n",
       "         [-9.6311e-03,  6.8417e-03,  1.5513e-02,  ..., -1.3035e-02,\n",
       "           1.3043e-02, -1.7105e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.weight': tensor([[ 0.0141,  0.0013, -0.0161,  ..., -0.0049,  0.0191, -0.0018],\n",
       "         [ 0.0051, -0.0084, -0.0027,  ...,  0.0144,  0.0066, -0.0061],\n",
       "         [-0.0031, -0.0189,  0.0089,  ...,  0.0187,  0.0035,  0.0132],\n",
       "         ...,\n",
       "         [-0.0138, -0.0139, -0.0112,  ...,  0.0169,  0.0147, -0.0034],\n",
       "         [-0.0130,  0.0010, -0.0192,  ..., -0.0095,  0.0031, -0.0034],\n",
       "         [-0.0086,  0.0023,  0.0156,  ...,  0.0021,  0.0086,  0.0032]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight': tensor([[ 0.0006,  0.0061, -0.0033,  ..., -0.0111,  0.0110, -0.0087],\n",
       "         [-0.0125,  0.0076, -0.0096,  ...,  0.0159,  0.0176,  0.0175],\n",
       "         [ 0.0095, -0.0195, -0.0035,  ...,  0.0141,  0.0066, -0.0146],\n",
       "         ...,\n",
       "         [ 0.0032, -0.0078,  0.0189,  ..., -0.0115,  0.0115,  0.0147],\n",
       "         [-0.0158,  0.0129,  0.0125,  ...,  0.0003,  0.0009,  0.0160],\n",
       "         [ 0.0123, -0.0167,  0.0126,  ..., -0.0144,  0.0163, -0.0078]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.weight': tensor([[ 0.0161,  0.0094,  0.0048,  ..., -0.0083,  0.0071,  0.0131],\n",
       "         [ 0.0040, -0.0047, -0.0191,  ..., -0.0014,  0.0055, -0.0117],\n",
       "         [-0.0021, -0.0145, -0.0060,  ..., -0.0114, -0.0063, -0.0035],\n",
       "         ...,\n",
       "         [ 0.0133,  0.0142, -0.0183,  ...,  0.0064, -0.0064, -0.0088],\n",
       "         [ 0.0150, -0.0086,  0.0083,  ...,  0.0157, -0.0121,  0.0095],\n",
       "         [-0.0191, -0.0184, -0.0006,  ...,  0.0037, -0.0165, -0.0103]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.q_proj.lora_A.weight': tensor([[ 0.0191,  0.0032,  0.0081,  ...,  0.0034,  0.0109,  0.0174],\n",
       "         [-0.0021,  0.0121,  0.0149,  ..., -0.0128,  0.0003, -0.0137],\n",
       "         [ 0.0075, -0.0109,  0.0070,  ..., -0.0099,  0.0163, -0.0107],\n",
       "         ...,\n",
       "         [ 0.0151, -0.0005, -0.0179,  ..., -0.0111,  0.0038, -0.0007],\n",
       "         [-0.0157,  0.0121,  0.0070,  ..., -0.0049, -0.0176,  0.0057],\n",
       "         [ 0.0045, -0.0153,  0.0037,  ...,  0.0083,  0.0145, -0.0088]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.k_proj.lora_A.weight': tensor([[ 0.0026, -0.0133,  0.0050,  ...,  0.0014, -0.0170,  0.0134],\n",
       "         [-0.0072,  0.0140,  0.0182,  ..., -0.0005, -0.0181,  0.0056],\n",
       "         [-0.0181,  0.0127, -0.0019,  ...,  0.0153,  0.0147, -0.0098],\n",
       "         ...,\n",
       "         [-0.0087,  0.0155, -0.0089,  ..., -0.0084,  0.0020, -0.0052],\n",
       "         [-0.0110,  0.0183,  0.0086,  ..., -0.0063,  0.0094,  0.0137],\n",
       "         [ 0.0138,  0.0141, -0.0033,  ..., -0.0078,  0.0192,  0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.v_proj.lora_A.weight': tensor([[-0.0038, -0.0068, -0.0170,  ..., -0.0047,  0.0046, -0.0158],\n",
       "         [ 0.0015,  0.0077,  0.0098,  ...,  0.0005,  0.0057, -0.0004],\n",
       "         [-0.0117,  0.0016,  0.0168,  ...,  0.0091, -0.0057, -0.0029],\n",
       "         ...,\n",
       "         [ 0.0137, -0.0149, -0.0182,  ...,  0.0115,  0.0147, -0.0011],\n",
       "         [ 0.0149,  0.0153,  0.0114,  ..., -0.0056,  0.0039, -0.0104],\n",
       "         [ 0.0099, -0.0056,  0.0052,  ...,  0.0180, -0.0165, -0.0068]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.o_proj.lora_A.weight': tensor([[ 0.0040, -0.0069,  0.0024,  ...,  0.0140, -0.0005,  0.0163],\n",
       "         [-0.0184, -0.0019, -0.0160,  ..., -0.0090, -0.0137,  0.0093],\n",
       "         [ 0.0103,  0.0109,  0.0061,  ..., -0.0074,  0.0104,  0.0037],\n",
       "         ...,\n",
       "         [ 0.0192,  0.0071,  0.0083,  ...,  0.0188, -0.0124,  0.0135],\n",
       "         [ 0.0165, -0.0083,  0.0041,  ...,  0.0029, -0.0026,  0.0065],\n",
       "         [-0.0108, -0.0093, -0.0082,  ...,  0.0145,  0.0054, -0.0134]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.32.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.q_proj.lora_A.weight': tensor([[ 0.0033, -0.0177,  0.0196,  ...,  0.0134, -0.0010, -0.0091],\n",
       "         [-0.0153, -0.0055, -0.0089,  ...,  0.0025, -0.0005, -0.0148],\n",
       "         [-0.0042, -0.0173,  0.0004,  ...,  0.0095, -0.0044,  0.0121],\n",
       "         ...,\n",
       "         [ 0.0002,  0.0029, -0.0175,  ..., -0.0027, -0.0022,  0.0076],\n",
       "         [ 0.0147, -0.0183,  0.0178,  ...,  0.0173, -0.0078,  0.0026],\n",
       "         [-0.0147,  0.0034, -0.0080,  ..., -0.0021,  0.0128, -0.0126]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.k_proj.lora_A.weight': tensor([[ 0.0159,  0.0048,  0.0100,  ...,  0.0025,  0.0113,  0.0123],\n",
       "         [ 0.0105, -0.0042,  0.0181,  ...,  0.0157, -0.0060,  0.0151],\n",
       "         [-0.0136, -0.0031,  0.0142,  ...,  0.0051,  0.0192, -0.0113],\n",
       "         ...,\n",
       "         [-0.0163,  0.0116,  0.0108,  ..., -0.0180, -0.0136, -0.0115],\n",
       "         [ 0.0071,  0.0108, -0.0017,  ..., -0.0119, -0.0074, -0.0164],\n",
       "         [ 0.0121,  0.0183, -0.0069,  ...,  0.0117,  0.0087, -0.0048]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.v_proj.lora_A.weight': tensor([[-0.0088, -0.0150,  0.0158,  ...,  0.0164, -0.0067,  0.0154],\n",
       "         [-0.0013, -0.0014,  0.0080,  ...,  0.0088,  0.0188,  0.0031],\n",
       "         [ 0.0106,  0.0017, -0.0029,  ...,  0.0046, -0.0035, -0.0037],\n",
       "         ...,\n",
       "         [ 0.0090,  0.0033,  0.0038,  ..., -0.0074,  0.0086, -0.0155],\n",
       "         [-0.0016, -0.0024, -0.0067,  ..., -0.0137, -0.0082,  0.0088],\n",
       "         [-0.0077, -0.0166, -0.0111,  ..., -0.0108,  0.0105, -0.0176]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.o_proj.lora_A.weight': tensor([[ 0.0005, -0.0027,  0.0007,  ..., -0.0063, -0.0087, -0.0131],\n",
       "         [ 0.0123, -0.0119, -0.0126,  ..., -0.0072,  0.0120, -0.0193],\n",
       "         [-0.0074,  0.0186,  0.0162,  ...,  0.0062,  0.0111,  0.0104],\n",
       "         ...,\n",
       "         [-0.0078, -0.0006,  0.0045,  ..., -0.0007,  0.0091, -0.0102],\n",
       "         [-0.0098,  0.0137, -0.0194,  ..., -0.0051,  0.0091,  0.0148],\n",
       "         [-0.0086, -0.0092, -0.0188,  ...,  0.0055,  0.0197, -0.0011]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.33.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.q_proj.lora_A.weight': tensor([[-0.0130, -0.0116, -0.0169,  ...,  0.0100,  0.0112, -0.0130],\n",
       "         [-0.0084, -0.0131, -0.0032,  ..., -0.0123,  0.0125, -0.0044],\n",
       "         [ 0.0074,  0.0035,  0.0100,  ...,  0.0181,  0.0180,  0.0165],\n",
       "         ...,\n",
       "         [-0.0174,  0.0062,  0.0126,  ...,  0.0062,  0.0178,  0.0006],\n",
       "         [ 0.0037,  0.0061, -0.0163,  ..., -0.0149, -0.0065,  0.0014],\n",
       "         [ 0.0039,  0.0197,  0.0178,  ..., -0.0131, -0.0017, -0.0074]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.k_proj.lora_A.weight': tensor([[ 0.0113, -0.0165,  0.0109,  ..., -0.0079,  0.0066,  0.0167],\n",
       "         [-0.0015, -0.0168,  0.0180,  ...,  0.0078, -0.0139, -0.0178],\n",
       "         [-0.0086,  0.0088, -0.0165,  ..., -0.0076,  0.0142, -0.0170],\n",
       "         ...,\n",
       "         [ 0.0196,  0.0189, -0.0037,  ..., -0.0163, -0.0131,  0.0127],\n",
       "         [ 0.0133,  0.0174, -0.0038,  ...,  0.0102, -0.0114, -0.0164],\n",
       "         [-0.0184, -0.0002, -0.0078,  ..., -0.0116, -0.0041,  0.0003]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.v_proj.lora_A.weight': tensor([[ 0.0096, -0.0099,  0.0131,  ...,  0.0051, -0.0114, -0.0178],\n",
       "         [ 0.0093, -0.0110,  0.0069,  ...,  0.0072, -0.0155,  0.0197],\n",
       "         [-0.0077,  0.0051, -0.0054,  ...,  0.0123, -0.0073,  0.0138],\n",
       "         ...,\n",
       "         [ 0.0015, -0.0023, -0.0103,  ..., -0.0147, -0.0076, -0.0091],\n",
       "         [ 0.0045, -0.0093, -0.0119,  ...,  0.0066, -0.0142, -0.0194],\n",
       "         [-0.0021, -0.0108,  0.0042,  ..., -0.0001, -0.0044, -0.0139]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.o_proj.lora_A.weight': tensor([[-9.9465e-03, -1.1726e-02,  6.8423e-03,  ...,  1.3087e-02,\n",
       "           1.3596e-02,  3.0961e-05],\n",
       "         [-1.3451e-04,  1.6240e-02,  1.3968e-02,  ...,  5.5177e-03,\n",
       "           4.9870e-03, -1.5637e-02],\n",
       "         [ 9.8787e-03, -6.6984e-03, -7.6029e-04,  ...,  1.2515e-02,\n",
       "           7.8383e-03, -6.3189e-03],\n",
       "         ...,\n",
       "         [-4.9077e-03,  1.6978e-02, -1.0404e-02,  ...,  1.9011e-02,\n",
       "          -1.0924e-02, -9.8377e-03],\n",
       "         [-1.3559e-02,  1.3423e-02, -1.4751e-02,  ..., -1.9370e-02,\n",
       "          -1.3104e-02,  1.7428e-02],\n",
       "         [-1.7528e-02,  1.1681e-02,  9.3547e-03,  ..., -1.9523e-02,\n",
       "           3.7837e-04,  5.9993e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.34.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.q_proj.lora_A.weight': tensor([[-0.0190, -0.0112,  0.0161,  ...,  0.0031,  0.0101, -0.0197],\n",
       "         [-0.0192, -0.0087, -0.0124,  ..., -0.0001, -0.0122,  0.0119],\n",
       "         [-0.0061, -0.0153, -0.0165,  ...,  0.0066,  0.0013,  0.0024],\n",
       "         ...,\n",
       "         [ 0.0050,  0.0102,  0.0071,  ..., -0.0139,  0.0039,  0.0194],\n",
       "         [ 0.0001,  0.0132,  0.0173,  ...,  0.0108,  0.0064, -0.0179],\n",
       "         [ 0.0164,  0.0030,  0.0193,  ...,  0.0074,  0.0041, -0.0066]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.k_proj.lora_A.weight': tensor([[-0.0012, -0.0082,  0.0153,  ...,  0.0105, -0.0117, -0.0145],\n",
       "         [ 0.0132,  0.0161, -0.0143,  ...,  0.0003, -0.0061, -0.0085],\n",
       "         [-0.0137, -0.0194, -0.0112,  ..., -0.0182,  0.0138,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0121, -0.0020,  0.0145,  ..., -0.0184, -0.0039,  0.0072],\n",
       "         [ 0.0153,  0.0183, -0.0191,  ...,  0.0167,  0.0146, -0.0049],\n",
       "         [ 0.0137, -0.0092,  0.0151,  ..., -0.0035, -0.0146, -0.0168]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.v_proj.lora_A.weight': tensor([[-0.0085, -0.0031,  0.0034,  ..., -0.0022,  0.0053,  0.0153],\n",
       "         [ 0.0058,  0.0114,  0.0181,  ...,  0.0119, -0.0065, -0.0064],\n",
       "         [ 0.0117,  0.0006, -0.0176,  ..., -0.0156,  0.0043, -0.0084],\n",
       "         ...,\n",
       "         [-0.0020,  0.0053, -0.0112,  ..., -0.0118,  0.0100, -0.0044],\n",
       "         [ 0.0167, -0.0164,  0.0134,  ...,  0.0011, -0.0146, -0.0100],\n",
       "         [ 0.0050,  0.0062,  0.0123,  ..., -0.0174, -0.0132, -0.0032]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.o_proj.lora_A.weight': tensor([[ 0.0171,  0.0052,  0.0043,  ..., -0.0142,  0.0062, -0.0179],\n",
       "         [-0.0017, -0.0033,  0.0110,  ..., -0.0103,  0.0032, -0.0027],\n",
       "         [-0.0095, -0.0100, -0.0183,  ...,  0.0147, -0.0024, -0.0111],\n",
       "         ...,\n",
       "         [-0.0101,  0.0171,  0.0179,  ..., -0.0113, -0.0025, -0.0060],\n",
       "         [-0.0130,  0.0097, -0.0146,  ...,  0.0034, -0.0194,  0.0008],\n",
       "         [ 0.0137, -0.0133,  0.0006,  ...,  0.0107, -0.0086, -0.0154]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.35.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.q_proj.lora_A.weight': tensor([[-0.0157,  0.0096,  0.0168,  ..., -0.0141,  0.0123,  0.0092],\n",
       "         [ 0.0133, -0.0139, -0.0150,  ...,  0.0039,  0.0092, -0.0064],\n",
       "         [-0.0110,  0.0031,  0.0186,  ...,  0.0096, -0.0016, -0.0032],\n",
       "         ...,\n",
       "         [ 0.0070, -0.0191, -0.0190,  ...,  0.0025, -0.0025, -0.0009],\n",
       "         [-0.0031,  0.0046, -0.0098,  ..., -0.0011, -0.0052, -0.0158],\n",
       "         [-0.0033, -0.0102,  0.0038,  ...,  0.0047, -0.0051,  0.0024]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.k_proj.lora_A.weight': tensor([[-0.0164,  0.0112, -0.0141,  ...,  0.0185, -0.0126,  0.0047],\n",
       "         [ 0.0172,  0.0118, -0.0141,  ..., -0.0102, -0.0129,  0.0066],\n",
       "         [ 0.0081,  0.0068,  0.0113,  ..., -0.0161, -0.0137, -0.0037],\n",
       "         ...,\n",
       "         [-0.0070,  0.0187, -0.0179,  ..., -0.0191, -0.0110,  0.0141],\n",
       "         [ 0.0102, -0.0076,  0.0116,  ..., -0.0003, -0.0112,  0.0124],\n",
       "         [ 0.0153, -0.0033, -0.0036,  ..., -0.0046,  0.0118,  0.0022]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.v_proj.lora_A.weight': tensor([[-0.0114,  0.0165,  0.0092,  ..., -0.0195, -0.0132, -0.0055],\n",
       "         [ 0.0103, -0.0142, -0.0136,  ..., -0.0099,  0.0110,  0.0128],\n",
       "         [-0.0148, -0.0026,  0.0041,  ...,  0.0017,  0.0073,  0.0155],\n",
       "         ...,\n",
       "         [ 0.0139, -0.0041, -0.0086,  ..., -0.0040,  0.0170,  0.0099],\n",
       "         [ 0.0190,  0.0097,  0.0156,  ...,  0.0064, -0.0034,  0.0185],\n",
       "         [ 0.0144, -0.0189, -0.0049,  ..., -0.0029,  0.0058,  0.0057]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.o_proj.lora_A.weight': tensor([[ 0.0135, -0.0143,  0.0077,  ..., -0.0097, -0.0141, -0.0135],\n",
       "         [ 0.0178, -0.0102, -0.0051,  ...,  0.0037,  0.0189,  0.0039],\n",
       "         [-0.0114,  0.0144, -0.0127,  ..., -0.0165, -0.0068, -0.0061],\n",
       "         ...,\n",
       "         [-0.0031,  0.0085,  0.0048,  ...,  0.0163, -0.0033, -0.0189],\n",
       "         [ 0.0196, -0.0004,  0.0181,  ...,  0.0094, -0.0159, -0.0107],\n",
       "         [ 0.0138, -0.0092,  0.0096,  ..., -0.0029, -0.0083, -0.0123]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.36.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.q_proj.lora_A.weight': tensor([[-0.0059,  0.0131, -0.0029,  ...,  0.0150, -0.0153,  0.0159],\n",
       "         [ 0.0113,  0.0019,  0.0167,  ..., -0.0168, -0.0064,  0.0036],\n",
       "         [ 0.0142, -0.0044, -0.0092,  ...,  0.0138, -0.0067,  0.0175],\n",
       "         ...,\n",
       "         [-0.0155,  0.0184,  0.0170,  ..., -0.0011,  0.0025, -0.0132],\n",
       "         [-0.0093,  0.0056,  0.0120,  ..., -0.0163,  0.0143,  0.0102],\n",
       "         [ 0.0061,  0.0057,  0.0137,  ..., -0.0135, -0.0150, -0.0012]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.k_proj.lora_A.weight': tensor([[-0.0066, -0.0167, -0.0164,  ...,  0.0066, -0.0098, -0.0041],\n",
       "         [ 0.0060,  0.0191,  0.0176,  ..., -0.0087, -0.0029, -0.0135],\n",
       "         [ 0.0023,  0.0036, -0.0140,  ...,  0.0144,  0.0001,  0.0137],\n",
       "         ...,\n",
       "         [-0.0090, -0.0169,  0.0018,  ...,  0.0082,  0.0099, -0.0128],\n",
       "         [-0.0006, -0.0141, -0.0018,  ..., -0.0008, -0.0147, -0.0184],\n",
       "         [-0.0055, -0.0071, -0.0178,  ..., -0.0003,  0.0105,  0.0036]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.v_proj.lora_A.weight': tensor([[-1.5018e-02, -1.5286e-02,  1.6424e-02,  ...,  8.5637e-03,\n",
       "          -9.7683e-05, -1.7185e-02],\n",
       "         [ 1.4979e-02,  6.1607e-03, -1.9183e-02,  ..., -1.9255e-02,\n",
       "          -8.7858e-03,  2.7702e-03],\n",
       "         [ 1.2278e-02,  3.0655e-03, -7.9224e-03,  ..., -1.2390e-02,\n",
       "          -2.0279e-03, -1.3933e-02],\n",
       "         ...,\n",
       "         [ 1.6539e-02, -1.4664e-02,  6.2520e-03,  ...,  1.5633e-02,\n",
       "          -3.4640e-04,  6.7712e-03],\n",
       "         [-4.0496e-03, -1.6672e-03, -6.7988e-03,  ..., -1.7420e-02,\n",
       "           1.5305e-02,  1.0893e-02],\n",
       "         [-1.9401e-02,  1.3656e-02, -7.9994e-03,  ...,  1.9046e-02,\n",
       "          -2.6230e-03,  1.0453e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.o_proj.lora_A.weight': tensor([[-0.0099,  0.0085, -0.0022,  ..., -0.0181, -0.0115,  0.0017],\n",
       "         [-0.0150,  0.0107, -0.0017,  ..., -0.0145, -0.0171,  0.0062],\n",
       "         [-0.0174, -0.0173,  0.0017,  ...,  0.0008, -0.0047, -0.0031],\n",
       "         ...,\n",
       "         [-0.0054, -0.0113, -0.0167,  ..., -0.0030, -0.0007,  0.0027],\n",
       "         [-0.0013, -0.0084,  0.0075,  ..., -0.0090, -0.0161, -0.0077],\n",
       "         [ 0.0189,  0.0154,  0.0176,  ...,  0.0159,  0.0151, -0.0067]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.37.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.q_proj.lora_A.weight': tensor([[ 5.9638e-03,  4.6688e-03,  1.6872e-02,  ...,  2.1942e-03,\n",
       "          -8.0096e-03, -4.9937e-03],\n",
       "         [ 3.3368e-03,  4.2317e-03, -9.3774e-03,  ...,  1.1263e-02,\n",
       "           1.7229e-02,  9.5720e-03],\n",
       "         [ 1.6560e-02,  1.7448e-02, -1.0296e-02,  ..., -3.1366e-03,\n",
       "          -1.5730e-02, -3.6309e-03],\n",
       "         ...,\n",
       "         [ 1.1178e-02, -1.3223e-02,  2.5848e-03,  ...,  1.4389e-02,\n",
       "          -7.1653e-03,  7.9665e-06],\n",
       "         [-3.0885e-03, -6.9899e-03,  1.4419e-02,  ...,  1.8302e-02,\n",
       "          -1.5651e-02,  6.0126e-04],\n",
       "         [-1.2146e-02, -1.6010e-02,  7.8487e-03,  ...,  1.1389e-03,\n",
       "          -1.1350e-02, -1.7559e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.k_proj.lora_A.weight': tensor([[ 0.0051,  0.0180, -0.0170,  ...,  0.0155, -0.0064, -0.0062],\n",
       "         [-0.0033,  0.0051, -0.0023,  ..., -0.0086,  0.0010,  0.0066],\n",
       "         [ 0.0081, -0.0007, -0.0064,  ...,  0.0103, -0.0130, -0.0126],\n",
       "         ...,\n",
       "         [ 0.0013, -0.0012, -0.0187,  ..., -0.0138,  0.0197,  0.0142],\n",
       "         [-0.0026, -0.0017, -0.0069,  ..., -0.0133,  0.0027,  0.0174],\n",
       "         [ 0.0060, -0.0018, -0.0088,  ..., -0.0196, -0.0194,  0.0187]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.v_proj.lora_A.weight': tensor([[-0.0064, -0.0147, -0.0070,  ...,  0.0063, -0.0124, -0.0041],\n",
       "         [-0.0076, -0.0194,  0.0171,  ..., -0.0179, -0.0033,  0.0052],\n",
       "         [-0.0131,  0.0165, -0.0110,  ...,  0.0141, -0.0101, -0.0090],\n",
       "         ...,\n",
       "         [-0.0133, -0.0108,  0.0028,  ..., -0.0111,  0.0079,  0.0146],\n",
       "         [ 0.0167, -0.0193, -0.0197,  ...,  0.0183,  0.0071, -0.0013],\n",
       "         [ 0.0013,  0.0133, -0.0029,  ..., -0.0118,  0.0044,  0.0025]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.o_proj.lora_A.weight': tensor([[ 0.0189, -0.0128, -0.0081,  ..., -0.0032,  0.0094,  0.0074],\n",
       "         [ 0.0007, -0.0126, -0.0154,  ..., -0.0130,  0.0136,  0.0024],\n",
       "         [-0.0142, -0.0079,  0.0078,  ..., -0.0148,  0.0036,  0.0122],\n",
       "         ...,\n",
       "         [ 0.0109,  0.0118, -0.0188,  ...,  0.0009, -0.0069,  0.0110],\n",
       "         [ 0.0066, -0.0072, -0.0118,  ...,  0.0096, -0.0171, -0.0130],\n",
       "         [-0.0149, -0.0075,  0.0167,  ..., -0.0138,  0.0001,  0.0171]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.38.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.q_proj.lora_A.weight': tensor([[ 0.0026,  0.0157,  0.0126,  ..., -0.0026, -0.0017, -0.0107],\n",
       "         [ 0.0148,  0.0127,  0.0052,  ...,  0.0140,  0.0057, -0.0056],\n",
       "         [-0.0005,  0.0163, -0.0121,  ...,  0.0116,  0.0157, -0.0103],\n",
       "         ...,\n",
       "         [ 0.0133, -0.0159,  0.0003,  ..., -0.0139,  0.0197, -0.0097],\n",
       "         [ 0.0158,  0.0120, -0.0038,  ..., -0.0158,  0.0035, -0.0007],\n",
       "         [-0.0100, -0.0091, -0.0098,  ..., -0.0013,  0.0084,  0.0195]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.q_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.k_proj.lora_A.weight': tensor([[-0.0078,  0.0111,  0.0162,  ...,  0.0004,  0.0035,  0.0144],\n",
       "         [ 0.0142, -0.0014,  0.0151,  ...,  0.0003, -0.0031,  0.0013],\n",
       "         [ 0.0002, -0.0115,  0.0192,  ...,  0.0122, -0.0130,  0.0166],\n",
       "         ...,\n",
       "         [-0.0192, -0.0016, -0.0028,  ..., -0.0036,  0.0126, -0.0052],\n",
       "         [-0.0127,  0.0180,  0.0040,  ...,  0.0096,  0.0182,  0.0152],\n",
       "         [-0.0181,  0.0088,  0.0119,  ...,  0.0025,  0.0025,  0.0166]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.k_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.v_proj.lora_A.weight': tensor([[-1.1851e-02, -6.1607e-05,  1.8923e-02,  ..., -1.2246e-02,\n",
       "           2.4219e-04,  1.5129e-02],\n",
       "         [ 1.8183e-02,  1.1009e-02, -5.8305e-03,  ...,  1.9048e-02,\n",
       "          -1.1584e-02,  5.9901e-03],\n",
       "         [-1.4506e-02, -4.2880e-03,  1.4058e-02,  ...,  1.7515e-02,\n",
       "          -1.0231e-02, -8.4731e-03],\n",
       "         ...,\n",
       "         [ 6.5435e-03,  4.3040e-03,  5.5191e-03,  ...,  3.7101e-04,\n",
       "           1.3551e-02, -1.3088e-02],\n",
       "         [-2.6032e-03,  1.0429e-02, -4.0871e-03,  ..., -1.0626e-02,\n",
       "           1.8327e-02,  1.4335e-02],\n",
       "         [ 1.1985e-02,  1.5730e-02,  1.9642e-02,  ...,  6.6279e-04,\n",
       "           1.2936e-02, -6.4051e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.v_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.o_proj.lora_A.weight': tensor([[-9.8631e-03, -6.9447e-03, -8.7484e-03,  ..., -4.2343e-03,\n",
       "           1.3054e-02, -1.6153e-02],\n",
       "         [-1.2456e-02, -1.0506e-03,  7.9150e-03,  ..., -5.6135e-03,\n",
       "           1.0388e-05, -7.9646e-03],\n",
       "         [ 1.4208e-02, -8.2349e-03,  1.3736e-02,  ...,  6.5304e-03,\n",
       "           1.7587e-02, -1.1248e-02],\n",
       "         ...,\n",
       "         [-4.3668e-03,  1.4857e-02,  8.5843e-03,  ...,  1.4146e-02,\n",
       "           1.6418e-02, -3.4860e-03],\n",
       "         [-5.8362e-03, -5.2470e-03,  1.4900e-03,  ..., -1.7307e-02,\n",
       "           5.9133e-03,  1.8537e-02],\n",
       "         [ 1.5060e-02, -1.2771e-02,  4.5073e-03,  ...,  9.7712e-03,\n",
       "           6.0589e-03,  1.2735e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.39.self_attn.o_proj.lora_B.weight': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_peft_model_state_dict(model_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-7: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train = TrainingArguments(\n",
    "    output_dir=os.path.join(path_output, \"model_sft\"),\n",
    "    num_train_epochs=config_model.get(\"epochs\"),\n",
    "    per_device_train_batch_size=config_model.get(\"batch_size\"),\n",
    "    per_device_eval_batch_size=config_model.get(\"batch_size\"),\n",
    "    gradient_accumulation_steps=config_model.get(\"gradient_steps\"),\n",
    "    gradient_checkpointing=True, \n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=config_model.get(\"learning_rate\"),\n",
    "    weight_decay=config_model.get(\"weight_decay\"),\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer, mlm=False) \n",
    "# collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "# collate_fn = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "# collate_fn = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a626204ded7743c9892ea4cbbb122f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e6085f80b046afa8ed2ef81ce7dca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model_lora,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args_train,\n",
    "    peft_config=config_lora,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    dataset_text_field=\"text\", \n",
    "    packing=True,\n",
    "    max_seq_length=config_model.get(\"max_seq_length\"),\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4383cd710ae74a6fbc641dfd7b679105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4522, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113d71f53ab49dca5ce234f9a357d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.179024338722229, 'eval_runtime': 6.7116, 'eval_samples_per_second': 0.447, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n",
      "{'loss': 1.3085, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e662af8e89440ba5169188a416579b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1574249267578125, 'eval_runtime': 6.736, 'eval_samples_per_second': 0.445, 'eval_steps_per_second': 0.148, 'epoch': 2.0}\n",
      "{'train_runtime': 202.8191, 'train_samples_per_second': 0.148, 'train_steps_per_second': 0.039, 'train_loss': 1.3803266286849976, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "res_train = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-8: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89244b19f66146f3ad7ea462298e00ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1574249267578125, 'eval_runtime': 6.8337, 'eval_samples_per_second': 0.439, 'eval_steps_per_second': 0.146, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "res_eval = trainer.evaluate()\n",
    "# res_eval = trainer.evaluate(dataset_train)\n",
    "# res_eval = trainer.evaluate(dataset_test)\n",
    "print(res_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-9: 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=os.path.join(path_model, \"model_sft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-10: 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3785 MiB |  11811 MiB |   3465 GiB |   3461 GiB |\n",
      "|       from large pool |   3660 MiB |  11686 MiB |   3461 GiB |   3458 GiB |\n",
      "|       from small pool |    124 MiB |    199 MiB |      3 GiB |      3 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3785 MiB |  11811 MiB |   3465 GiB |   3461 GiB |\n",
      "|       from large pool |   3660 MiB |  11686 MiB |   3461 GiB |   3458 GiB |\n",
      "|       from small pool |    124 MiB |    199 MiB |      3 GiB |      3 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3776 MiB |  11802 MiB |   3465 GiB |   3461 GiB |\n",
      "|       from large pool |   3652 MiB |  11677 MiB |   3461 GiB |   3458 GiB |\n",
      "|       from small pool |    124 MiB |    199 MiB |      3 GiB |      3 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  14624 MiB |  14624 MiB |  14658 MiB |  34816 KiB |\n",
      "|       from large pool |  14414 MiB |  14414 MiB |  14448 MiB |  34816 KiB |\n",
      "|       from small pool |    210 MiB |    210 MiB |    210 MiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 279546 KiB |   2271 MiB |   3166 GiB |   3166 GiB |\n",
      "|       from large pool | 273792 KiB |   2266 MiB |   3162 GiB |   3162 GiB |\n",
      "|       from small pool |   5754 KiB |     13 MiB |      4 GiB |      4 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    2686    |    3651    |  128383    |  125697    |\n",
      "|       from large pool |     364    |     434    |   90235    |   89871    |\n",
      "|       from small pool |    2322    |    3287    |   38148    |   35826    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    2686    |    3651    |  128383    |  125697    |\n",
      "|       from large pool |     364    |     434    |   90235    |   89871    |\n",
      "|       from small pool |    2322    |    3287    |   38148    |   35826    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     243    |     243    |     244    |       1    |\n",
      "|       from large pool |     138    |     138    |     139    |       1    |\n",
      "|       from small pool |     105    |     105    |     105    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     126    |     311    |   59776    |   59650    |\n",
      "|       from large pool |      80    |     103    |   48987    |   48907    |\n",
      "|       from small pool |      46    |     231    |   10789    |   10743    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(th.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocated_memory = th.cuda.memory_allocated()\n",
    "# cached_memory = th.cuda.memory_cached()\n",
    "# print(f\"已分配的GPU内存：{allocated_memory / 1024**3:.2f}G, 已缓存的GPU内存：{cached_memory / 1024**3:.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 释放不再使用的GPU内存\n",
    "model_base.cpu()\n",
    "del model_base\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe81e16928f45efba7fc7c4f32731f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reload model_base\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=th.bfloat16,\n",
    "    quantization_config=config_bnb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model_sft\n",
    "model_sft = PeftModel.from_pretrained(\n",
    "    model=model_base,\n",
    "    model_id=os.path.join(path_model, \"model_sft\"),\n",
    "    is_trainable=False\n",
    ")\n",
    "model_sft = model_sft.merge_and_unload()  # W + BA, speed up, but errors when use 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged model to local\n",
    "model_sft.save_pretrained(save_directory=os.path.join(path_model, \"model_sft_merged\"), max_shard_size=\"4GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged model to hf\n",
    "# model_sft.save_pretrained_merged(\"model_sft\", tokenizer, save_method=\"merged_16bit\")  # merged_16bit, merged_4bit, lora\n",
    "# model_sft.push_to_hub_merged(\"hf/model_sft\", tokenizer, save_method=\"merged_16bit\", token=your_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged model as gguf\n",
    "# model_sft.save_pretrained_gguf(\"model_sft\", tokenizer, quantization_method=\"f16\")  # f16, q4_k_m\n",
    "# model_sft.push_to_hub_gguf(\"hf/model_sft\", tokenizer, quantization_method=\"f16\", token=your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-11: 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_usr = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Design a database to record employee salaries.\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": content_usr}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "# model_inputs = tokenizer([text1, text2], return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:12.542673\n",
      "To design a database to record employee salaries, we need to consider the following entities and their attributes:\n",
      "\n",
      "1. Employee: This entity will have the following attributes:\n",
      "   - ID (primary key)\n",
      "   - First Name\n",
      "   - Last Name\n",
      "   - Date of Birth\n",
      "   - Gender\n",
      "   - Position\n",
      "   - Department\n",
      "   - Salary\n",
      "\n",
      "2. Salary: This entity will have the following attributes:\n",
      "   - ID (primary key)\n",
      "   - Employee ID (foreign key referencing Employee table)\n",
      "   - Amount\n",
      "   - Date\n",
      "\n",
      "We can create two tables for this database: Employee and Salary. The Employee table will have one\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "max_new_tokens = 128  # 取训练样本答案的最长值\n",
    "top_p = 0.9\n",
    "temperature = 0.1  # 0.5，0.35，0.1，0.01\n",
    "# repetition_penalty = 1.5\n",
    "\n",
    "t0 = pd.Timestamp.now()\n",
    "model_sft.eval()\n",
    "with th.inference_mode():\n",
    "    complete_ids = model_sft.generate(\n",
    "        input_ids=model_inputs.input_ids,  # 针对 tokenizer.padding_side\n",
    "        attention_mask=model_inputs.attention_mask,  # 针对 tokenizer.padding_side\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    # also OK\n",
    "    # complete_ids = model_sft.generate(\n",
    "    #     max_new_tokens=max_new_tokens,\n",
    "    #     top_p=top_p,\n",
    "    #     temperature=temperature,\n",
    "    #     **model_inputs  # 针对 tokenizer.padding_side\n",
    "    # )\n",
    "t1 = pd.Timestamp.now()\n",
    "print(t1 - t0)\n",
    "\n",
    "input_ids = model_inputs.input_ids\n",
    "generated_ids = [O[len(I): ] for (I, O) in zip(input_ids, complete_ids)]\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-12: ollama模型转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备好微调好的模型文件夹（其中模型文件为safetensors格式，其余文件与基模型文件夹对齐）\n",
    "- 在ollama中查看基模型的Modelfile格式（如：ollama show qwen2:7b --modelfile）\n",
    "- 在微调文件夹中创建一致的Modelfile文件\n",
    "- 执行转化与量化指令（如：ollama create --quantize Q4_K_M -f Modelfile Qwen1.5-4B-Chat-SFT-Q4_K_M）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama show qwen2:7b --modelfile\n",
    "'''\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM qwen2:7b\n",
    "\n",
    "FROM F:\\LLM\\ollama\\blobs\\sha256-43f7a214e5329f672bb05404cfba1913cbb70fdaa1a17497224e1925046b0ed5\n",
    "TEMPLATE \"{{ if .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}{{ if .Prompt }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "{{ end }}<|im_start|>assistant\n",
    "{{ .Response }}<|im_end|>\n",
    "\"\n",
    "PARAMETER stop <|im_start|>\n",
    "PARAMETER stop <|im_end|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama create --quantize Q4_K_M -f Modelfile Qwen1.5-4B-Chat-SFT-Q4_K_M\n",
    "'''\n",
    "transferring model data\n",
    "unpacking model metadata\n",
    "Error: Models based on 'Qwen2ForCausalLM' are not yet supported\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
