{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from datasets import (load_dataset, load_from_disk, Dataset)\n",
    "from transformers import (AutoTokenizer, AutoModel, BertTokenizer, BertModel,\n",
    "                          AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "                          BitsAndBytesConfig, TrainingArguments,\n",
    "                          DataCollatorWithPadding, DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForSeq2Seq, DataCollatorForTokenClassification,\n",
    "                          pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda; devive_cnt = 1\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "devive_cnt = th.cuda.device_count()\n",
    "print(f\"device = {device}; devive_cnt = {devive_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = os.getcwd()\n",
    "path_data = os.path.join(os.path.dirname(path_project), \"data\")\n",
    "path_model = os.path.join(os.path.dirname(path_project), \"model\")\n",
    "path_output = os.path.join(os.path.dirname(path_project), \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-1: 载入数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"axb/super_glue-test.arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 Dataset 类\n",
    "class Dataset(th.utils.data.Dataset):\n",
    "    def __init__(self, filename, data_type):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = load_dataset(\n",
    "            path=data_type,\n",
    "            data_files=os.path.join(path_data, filename),\n",
    "            split=\"all\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sentence1 = self.dataset[i][\"sentence1\"]\n",
    "        sentence2 = self.dataset[i][\"sentence2\"]\n",
    "        label = self.dataset[i][\"label\"]\n",
    "        return sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(filename, data_type=\"arrow\")\n",
    "dataset_train, dataset_test = random_split(dataset, lengths=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Notorious B.I.G. passed away.',\n",
       " 'During Notorious B.I.G.\\'s funeral procession through the streets of Brooklyn, someone interrupted the somber atmosphere by playing \"Hyponotize\" at full volume, which prompted the public to dance and sing along.',\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-2: tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-large-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-3: 配置量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bnb = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=th.bfloat16,\n",
    "    # bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-4: 载入基础大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "model_base = BertModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(path_model, checkpoint),\n",
    "    cache_dir=path_model,\n",
    "    force_download=False,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\",\n",
    "    # torch_dtype=th.float16,\n",
    "    # quantization_config=config_bnb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  name: embeddings.word_embeddings.weight;  shape: torch.Size([30522, 1024]);  dtype: torch.float32;  device: cpu\n",
      "1  name: embeddings.position_embeddings.weight;  shape: torch.Size([512, 1024]);  dtype: torch.float32;  device: cpu\n",
      "2  name: embeddings.token_type_embeddings.weight;  shape: torch.Size([2, 1024]);  dtype: torch.float32;  device: cpu\n",
      "3  name: embeddings.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "4  name: embeddings.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "5  name: encoder.layer.0.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "6  name: encoder.layer.0.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "7  name: encoder.layer.0.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "8  name: encoder.layer.0.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "9  name: encoder.layer.0.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "10  name: encoder.layer.0.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "11  name: encoder.layer.0.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "12  name: encoder.layer.0.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "13  name: encoder.layer.0.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "14  name: encoder.layer.0.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "15  name: encoder.layer.0.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "16  name: encoder.layer.0.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "17  name: encoder.layer.0.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "18  name: encoder.layer.0.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "19  name: encoder.layer.0.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "20  name: encoder.layer.0.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "21  name: encoder.layer.1.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "22  name: encoder.layer.1.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "23  name: encoder.layer.1.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "24  name: encoder.layer.1.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "25  name: encoder.layer.1.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "26  name: encoder.layer.1.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "27  name: encoder.layer.1.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "28  name: encoder.layer.1.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "29  name: encoder.layer.1.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "30  name: encoder.layer.1.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "31  name: encoder.layer.1.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "32  name: encoder.layer.1.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "33  name: encoder.layer.1.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "34  name: encoder.layer.1.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "35  name: encoder.layer.1.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "36  name: encoder.layer.1.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "37  name: encoder.layer.2.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "38  name: encoder.layer.2.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "39  name: encoder.layer.2.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "40  name: encoder.layer.2.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "41  name: encoder.layer.2.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "42  name: encoder.layer.2.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "43  name: encoder.layer.2.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "44  name: encoder.layer.2.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "45  name: encoder.layer.2.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "46  name: encoder.layer.2.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "47  name: encoder.layer.2.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "48  name: encoder.layer.2.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "49  name: encoder.layer.2.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "50  name: encoder.layer.2.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "51  name: encoder.layer.2.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "52  name: encoder.layer.2.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "53  name: encoder.layer.3.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "54  name: encoder.layer.3.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "55  name: encoder.layer.3.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "56  name: encoder.layer.3.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "57  name: encoder.layer.3.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "58  name: encoder.layer.3.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "59  name: encoder.layer.3.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "60  name: encoder.layer.3.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "61  name: encoder.layer.3.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "62  name: encoder.layer.3.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "63  name: encoder.layer.3.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "64  name: encoder.layer.3.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "65  name: encoder.layer.3.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "66  name: encoder.layer.3.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "67  name: encoder.layer.3.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "68  name: encoder.layer.3.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "69  name: encoder.layer.4.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "70  name: encoder.layer.4.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "71  name: encoder.layer.4.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "72  name: encoder.layer.4.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "73  name: encoder.layer.4.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "74  name: encoder.layer.4.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "75  name: encoder.layer.4.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "76  name: encoder.layer.4.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "77  name: encoder.layer.4.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "78  name: encoder.layer.4.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "79  name: encoder.layer.4.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "80  name: encoder.layer.4.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "81  name: encoder.layer.4.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "82  name: encoder.layer.4.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "83  name: encoder.layer.4.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "84  name: encoder.layer.4.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "85  name: encoder.layer.5.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "86  name: encoder.layer.5.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "87  name: encoder.layer.5.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "88  name: encoder.layer.5.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "89  name: encoder.layer.5.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "90  name: encoder.layer.5.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "91  name: encoder.layer.5.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "92  name: encoder.layer.5.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "93  name: encoder.layer.5.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "94  name: encoder.layer.5.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "95  name: encoder.layer.5.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "96  name: encoder.layer.5.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "97  name: encoder.layer.5.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "98  name: encoder.layer.5.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "99  name: encoder.layer.5.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "100  name: encoder.layer.5.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "101  name: encoder.layer.6.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "102  name: encoder.layer.6.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "103  name: encoder.layer.6.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "104  name: encoder.layer.6.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "105  name: encoder.layer.6.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "106  name: encoder.layer.6.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "107  name: encoder.layer.6.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "108  name: encoder.layer.6.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "109  name: encoder.layer.6.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "110  name: encoder.layer.6.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "111  name: encoder.layer.6.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "112  name: encoder.layer.6.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "113  name: encoder.layer.6.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "114  name: encoder.layer.6.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "115  name: encoder.layer.6.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "116  name: encoder.layer.6.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "117  name: encoder.layer.7.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "118  name: encoder.layer.7.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "119  name: encoder.layer.7.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "120  name: encoder.layer.7.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "121  name: encoder.layer.7.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "122  name: encoder.layer.7.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "123  name: encoder.layer.7.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "124  name: encoder.layer.7.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "125  name: encoder.layer.7.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "126  name: encoder.layer.7.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "127  name: encoder.layer.7.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "128  name: encoder.layer.7.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "129  name: encoder.layer.7.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "130  name: encoder.layer.7.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "131  name: encoder.layer.7.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "132  name: encoder.layer.7.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "133  name: encoder.layer.8.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "134  name: encoder.layer.8.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "135  name: encoder.layer.8.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "136  name: encoder.layer.8.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "137  name: encoder.layer.8.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "138  name: encoder.layer.8.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "139  name: encoder.layer.8.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "140  name: encoder.layer.8.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "141  name: encoder.layer.8.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "142  name: encoder.layer.8.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "143  name: encoder.layer.8.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "144  name: encoder.layer.8.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "145  name: encoder.layer.8.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "146  name: encoder.layer.8.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "147  name: encoder.layer.8.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "148  name: encoder.layer.8.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "149  name: encoder.layer.9.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "150  name: encoder.layer.9.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "151  name: encoder.layer.9.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "152  name: encoder.layer.9.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "153  name: encoder.layer.9.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "154  name: encoder.layer.9.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "155  name: encoder.layer.9.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "156  name: encoder.layer.9.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "157  name: encoder.layer.9.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "158  name: encoder.layer.9.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "159  name: encoder.layer.9.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "160  name: encoder.layer.9.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "161  name: encoder.layer.9.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "162  name: encoder.layer.9.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "163  name: encoder.layer.9.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "164  name: encoder.layer.9.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "165  name: encoder.layer.10.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "166  name: encoder.layer.10.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "167  name: encoder.layer.10.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "168  name: encoder.layer.10.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "169  name: encoder.layer.10.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "170  name: encoder.layer.10.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "171  name: encoder.layer.10.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "172  name: encoder.layer.10.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "173  name: encoder.layer.10.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "174  name: encoder.layer.10.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "175  name: encoder.layer.10.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "176  name: encoder.layer.10.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "177  name: encoder.layer.10.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "178  name: encoder.layer.10.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "179  name: encoder.layer.10.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "180  name: encoder.layer.10.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "181  name: encoder.layer.11.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "182  name: encoder.layer.11.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "183  name: encoder.layer.11.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "184  name: encoder.layer.11.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "185  name: encoder.layer.11.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "186  name: encoder.layer.11.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "187  name: encoder.layer.11.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "188  name: encoder.layer.11.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "189  name: encoder.layer.11.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "190  name: encoder.layer.11.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "191  name: encoder.layer.11.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "192  name: encoder.layer.11.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "193  name: encoder.layer.11.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "194  name: encoder.layer.11.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "195  name: encoder.layer.11.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "196  name: encoder.layer.11.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "197  name: encoder.layer.12.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "198  name: encoder.layer.12.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "199  name: encoder.layer.12.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "200  name: encoder.layer.12.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "201  name: encoder.layer.12.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "202  name: encoder.layer.12.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "203  name: encoder.layer.12.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "204  name: encoder.layer.12.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "205  name: encoder.layer.12.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "206  name: encoder.layer.12.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "207  name: encoder.layer.12.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "208  name: encoder.layer.12.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "209  name: encoder.layer.12.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "210  name: encoder.layer.12.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "211  name: encoder.layer.12.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "212  name: encoder.layer.12.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "213  name: encoder.layer.13.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "214  name: encoder.layer.13.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "215  name: encoder.layer.13.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "216  name: encoder.layer.13.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "217  name: encoder.layer.13.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "218  name: encoder.layer.13.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "219  name: encoder.layer.13.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "220  name: encoder.layer.13.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "221  name: encoder.layer.13.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "222  name: encoder.layer.13.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "223  name: encoder.layer.13.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "224  name: encoder.layer.13.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "225  name: encoder.layer.13.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "226  name: encoder.layer.13.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "227  name: encoder.layer.13.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "228  name: encoder.layer.13.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "229  name: encoder.layer.14.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "230  name: encoder.layer.14.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "231  name: encoder.layer.14.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "232  name: encoder.layer.14.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "233  name: encoder.layer.14.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "234  name: encoder.layer.14.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "235  name: encoder.layer.14.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "236  name: encoder.layer.14.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "237  name: encoder.layer.14.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "238  name: encoder.layer.14.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "239  name: encoder.layer.14.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "240  name: encoder.layer.14.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "241  name: encoder.layer.14.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "242  name: encoder.layer.14.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "243  name: encoder.layer.14.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "244  name: encoder.layer.14.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "245  name: encoder.layer.15.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "246  name: encoder.layer.15.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "247  name: encoder.layer.15.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "248  name: encoder.layer.15.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "249  name: encoder.layer.15.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "250  name: encoder.layer.15.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "251  name: encoder.layer.15.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "252  name: encoder.layer.15.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "253  name: encoder.layer.15.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "254  name: encoder.layer.15.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "255  name: encoder.layer.15.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "256  name: encoder.layer.15.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "257  name: encoder.layer.15.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "258  name: encoder.layer.15.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "259  name: encoder.layer.15.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "260  name: encoder.layer.15.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "261  name: encoder.layer.16.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "262  name: encoder.layer.16.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "263  name: encoder.layer.16.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "264  name: encoder.layer.16.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "265  name: encoder.layer.16.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "266  name: encoder.layer.16.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "267  name: encoder.layer.16.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "268  name: encoder.layer.16.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "269  name: encoder.layer.16.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "270  name: encoder.layer.16.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "271  name: encoder.layer.16.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "272  name: encoder.layer.16.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "273  name: encoder.layer.16.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "274  name: encoder.layer.16.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "275  name: encoder.layer.16.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "276  name: encoder.layer.16.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "277  name: encoder.layer.17.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "278  name: encoder.layer.17.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "279  name: encoder.layer.17.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "280  name: encoder.layer.17.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "281  name: encoder.layer.17.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "282  name: encoder.layer.17.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "283  name: encoder.layer.17.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "284  name: encoder.layer.17.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "285  name: encoder.layer.17.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "286  name: encoder.layer.17.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "287  name: encoder.layer.17.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "288  name: encoder.layer.17.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "289  name: encoder.layer.17.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "290  name: encoder.layer.17.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "291  name: encoder.layer.17.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "292  name: encoder.layer.17.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "293  name: encoder.layer.18.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "294  name: encoder.layer.18.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "295  name: encoder.layer.18.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "296  name: encoder.layer.18.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "297  name: encoder.layer.18.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "298  name: encoder.layer.18.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "299  name: encoder.layer.18.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "300  name: encoder.layer.18.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "301  name: encoder.layer.18.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "302  name: encoder.layer.18.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "303  name: encoder.layer.18.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "304  name: encoder.layer.18.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "305  name: encoder.layer.18.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "306  name: encoder.layer.18.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "307  name: encoder.layer.18.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "308  name: encoder.layer.18.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "309  name: encoder.layer.19.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "310  name: encoder.layer.19.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "311  name: encoder.layer.19.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "312  name: encoder.layer.19.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "313  name: encoder.layer.19.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "314  name: encoder.layer.19.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "315  name: encoder.layer.19.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "316  name: encoder.layer.19.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "317  name: encoder.layer.19.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "318  name: encoder.layer.19.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "319  name: encoder.layer.19.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "320  name: encoder.layer.19.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "321  name: encoder.layer.19.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "322  name: encoder.layer.19.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "323  name: encoder.layer.19.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "324  name: encoder.layer.19.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "325  name: encoder.layer.20.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "326  name: encoder.layer.20.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "327  name: encoder.layer.20.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "328  name: encoder.layer.20.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "329  name: encoder.layer.20.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "330  name: encoder.layer.20.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "331  name: encoder.layer.20.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "332  name: encoder.layer.20.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "333  name: encoder.layer.20.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "334  name: encoder.layer.20.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "335  name: encoder.layer.20.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "336  name: encoder.layer.20.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "337  name: encoder.layer.20.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "338  name: encoder.layer.20.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "339  name: encoder.layer.20.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "340  name: encoder.layer.20.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "341  name: encoder.layer.21.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "342  name: encoder.layer.21.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "343  name: encoder.layer.21.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "344  name: encoder.layer.21.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "345  name: encoder.layer.21.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "346  name: encoder.layer.21.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "347  name: encoder.layer.21.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "348  name: encoder.layer.21.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "349  name: encoder.layer.21.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "350  name: encoder.layer.21.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "351  name: encoder.layer.21.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "352  name: encoder.layer.21.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "353  name: encoder.layer.21.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "354  name: encoder.layer.21.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "355  name: encoder.layer.21.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "356  name: encoder.layer.21.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "357  name: encoder.layer.22.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "358  name: encoder.layer.22.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "359  name: encoder.layer.22.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "360  name: encoder.layer.22.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "361  name: encoder.layer.22.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "362  name: encoder.layer.22.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "363  name: encoder.layer.22.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "364  name: encoder.layer.22.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "365  name: encoder.layer.22.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "366  name: encoder.layer.22.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "367  name: encoder.layer.22.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "368  name: encoder.layer.22.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "369  name: encoder.layer.22.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "370  name: encoder.layer.22.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "371  name: encoder.layer.22.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "372  name: encoder.layer.22.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "373  name: encoder.layer.23.attention.self.query.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "374  name: encoder.layer.23.attention.self.query.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "375  name: encoder.layer.23.attention.self.key.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "376  name: encoder.layer.23.attention.self.key.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "377  name: encoder.layer.23.attention.self.value.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "378  name: encoder.layer.23.attention.self.value.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "379  name: encoder.layer.23.attention.output.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "380  name: encoder.layer.23.attention.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "381  name: encoder.layer.23.attention.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "382  name: encoder.layer.23.attention.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "383  name: encoder.layer.23.intermediate.dense.weight;  shape: torch.Size([4096, 1024]);  dtype: torch.float32;  device: cpu\n",
      "384  name: encoder.layer.23.intermediate.dense.bias;  shape: torch.Size([4096]);  dtype: torch.float32;  device: cpu\n",
      "385  name: encoder.layer.23.output.dense.weight;  shape: torch.Size([1024, 4096]);  dtype: torch.float32;  device: cpu\n",
      "386  name: encoder.layer.23.output.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "387  name: encoder.layer.23.output.LayerNorm.weight;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "388  name: encoder.layer.23.output.LayerNorm.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n",
      "389  name: pooler.dense.weight;  shape: torch.Size([1024, 1024]);  dtype: torch.float32;  device: cpu\n",
      "390  name: pooler.dense.bias;  shape: torch.Size([1024]);  dtype: torch.float32;  device: cpu\n"
     ]
    }
   ],
   "source": [
    "for i, (name, parm) in enumerate(model_base.named_parameters()):\n",
    "    print(f\"{i}  name: {name};  shape: {parm.shape};  dtype: {parm.dtype};  device: {parm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_base.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-5: 定义整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(dataset):\n",
    "    sents = [x[0:2] for x in dataset]\n",
    "    labels = [x[2] for x in dataset]\n",
    "    # max_length = max(len(x[0]) + len(x[1]) for x in sents) + 3\n",
    "\n",
    "    # 编码\n",
    "    inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                         truncation=True,\n",
    "                                         padding=\"max_length\",\n",
    "                                        #  max_length=max_length,\n",
    "                                         max_length=512,\n",
    "                                         add_special_tokens=True,\n",
    "                                         return_token_type_ids=True,\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         return_tensors=\"pt\",\n",
    "                                         return_length=True)\n",
    "\n",
    "    labels = th.LongTensor(labels)  # torch.int64\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-6: 配置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = {\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"dropout\": 0.2,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    # \"gradient_steps\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.01,\n",
    "    # \"max_seq_lenght\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-7: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(th.nn.Module):\n",
    "    def __init__(self, model_base, config_model):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_base = model_base\n",
    "        self.embedding_dim = config_model.get(\"embedding_dim\")\n",
    "        self.hidden_dim = config_model.get(\"hidden_dim\")\n",
    "        self.dropout = config_model.get(\"dropout\")\n",
    "\n",
    "        self.lstm = th.nn.LSTM(input_size=self.embedding_dim,\n",
    "                               hidden_size=self.hidden_dim,\n",
    "                               num_layers=1,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)\n",
    "\n",
    "        self.mlp = th.nn.Sequential(\n",
    "            # layer-1\n",
    "            th.nn.Linear(in_features=self.hidden_dim * 2, out_features=256),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Dropout(p=self.dropout),\n",
    "            th.nn.LayerNorm(normalized_shape=256),\n",
    "            # layer-2\n",
    "            th.nn.Linear(in_features=256, out_features=128),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Dropout(p=self.dropout),\n",
    "            th.nn.LayerNorm(normalized_shape=128),\n",
    "            # layer-3\n",
    "            th.nn.Linear(in_features=128, out_features=64),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Dropout(p=self.dropout),\n",
    "            th.nn.LayerNorm(normalized_shape=64),\n",
    "            # layer-out\n",
    "            th.nn.Linear(in_features=64, out_features=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # bert layer\n",
    "        tokens = inputs[\"input_ids\"]\n",
    "        segments = inputs[\"token_type_ids\"]\n",
    "        valid_lens = inputs[\"attention_mask\"]\n",
    "        \n",
    "        output_bert = self.model_base(\n",
    "            input_ids=tokens,\n",
    "            token_type_ids=segments,\n",
    "            attention_mask=valid_lens\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # lstm layer\n",
    "        output_lstm, [ht, ct] = self.lstm(output_bert)\n",
    "\n",
    "        # mlp_layer\n",
    "        out_mlp = self.mlp(output_lstm[:, 0, :])\n",
    "        return out_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sft = Model(model_base, config_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6604226 || all params: 341746114 || trainable%: 1.9325\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for param in model_sft.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "    all_params += param.numel()\n",
    "\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = optim.AdamW(params=model_sft.parameters(), \n",
    "                   lr=config_model.get(\"learning_rate\"), \n",
    "                   betas=(0.9, 0.999), \n",
    "                   eps=10**-8, \n",
    "                   weight_decay=config_model.get(\"weight_decay\"))\n",
    "objt = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "epochs = config_model.get(\"epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = th.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                        batch_size=config_model.get(\"batch_size\"),\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        shuffle=True,\n",
    "                                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_test = th.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                       batch_size=config_model.get(\"batch_size\"),\n",
    "                                       collate_fn=collate_fn,\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  loss_train 0.7302  loss_test 0.6561\n",
      "epoch 1  loss_train 0.6971  loss_test 0.6740\n",
      "epoch 2  loss_train 0.6897  loss_test 0.6545\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    loss_train_tmp = 0\n",
    "    model_sft.train()\n",
    "    for (i, (inputs, labels)) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output_mlp = model_sft(inputs)\n",
    "        loss = objt(output_mlp, labels)\n",
    "        loss_train_tmp += loss.item()\n",
    "\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "    loss_train = loss_train_tmp / (i + 1)\n",
    "    \n",
    "    # test\n",
    "    loss_test_tmp = 0\n",
    "    model_sft.eval()\n",
    "    for (i, (inputs, labels)) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        output_mlp = model_sft(inputs)\n",
    "        loss = objt(output_mlp, labels)\n",
    "        loss_test_tmp += loss.item()\n",
    "    loss_test = loss_test_tmp / (i + 1)\n",
    "    \n",
    "    print(f\"epoch {epoch}  loss_train {loss_train:.4f}  loss_test {loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step-8: 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Missouri lawmakers are considering a boycott of companies that boycott Israel.\"\n",
    "sent2 = \"Missouri lawmakers are considering a government boycott of companies that boycott Israel.\"\n",
    "sents = [(sent1, sent2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                     truncation=True,\n",
    "                                     padding=\"max_length\",\n",
    "                                     max_length=512,\n",
    "                                     add_special_tokens=True,\n",
    "                                     return_token_type_ids=True,\n",
    "                                     return_attention_mask=True,\n",
    "                                     return_special_tokens_mask=True,\n",
    "                                     return_tensors=\"pt\",\n",
    "                                     return_length=True)\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_sft.eval()\n",
    "with th.inference_mode():\n",
    "    out_mlp = model_sft(inputs)\n",
    "    y_hat = th.softmax(out_mlp, dim=1)\n",
    "    y_pred = th.argmax(y_hat, dim=1)\n",
    "\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
