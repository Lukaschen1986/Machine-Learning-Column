# 自然语言处理
- 在进入 Transformer 模型之前，让我们快速概述一下自然语言处理是什么以及我们为什么这么重视它。

# 什么是自然语言处理？
- NLP 是语言学和机器学习交叉领域，专注于理解与人类语言相关的一切。 
- NLP 任务的目标不仅是单独理解单个单词，而且是能够理解这些单词的上下文。

# 以下是常见 NLP 任务的列表，每个任务都有一些示例：
对整个句子进行分类: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关
对句子中的每个词进行分类: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）
生成文本内容: 用自动生成的文本完成提示，用屏蔽词填充文本中的空白
从文本中提取答案: 给定问题和上下文，根据上下文中提供的信息提取问题的答案
从输入文本生成新句子: 将文本翻译成另一种语言，总结文本
NLP 不仅限于书面文本。它还解决了语音识别和计算机视觉中的复杂挑战，例如生成音频样本的转录或图像描述。

# 为什么具有挑战性？
计算机处理信息的方式与人类不同。例如，当我们读到“我饿了”这句话时，我们很容易理解它的意思。
同样，给定两个句子，例如“我很饿”和“我很伤心”，我们可以轻松确定它们的相似程度。
对于机器学习 (ML) 模型，此类任务更加困难。文本需要以一种使模型能够从中学习的方式进行处理。
而且由于语言很复杂，我们需要仔细考虑必须如何进行这种处理。
关于如何表示文本已经做了很多研究，我们将在下一章中介绍一些方法。

# 一点Transformers的发展历史
Transformer 架构 于 2017 年 6 月推出。原本研究的重点是翻译任务。随后推出了几个有影响力的模型，包括
2018 年 6 月: GPT, 第一个预训练的 Transformer 模型，用于各种 NLP 任务并获得极好的结果
2018 年 10 月: BERT, 另一个大型预训练模型，该模型旨在生成更好的句子摘要（下一章将详细介绍！）
2019 年 2 月: GPT-2, GPT 的改进（并且更大）版本，由于道德问题没有立即公开发布
2019 年 10 月: DistilBERT, BERT 的提炼版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能
2019 年 10 月: BART 和 T5, 两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）
2020 年 5 月, GPT-3, GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习）

这个列表并不全面，只是为了突出一些不同类型的 Transformer 模型。大体上，它们可以分为三类：
GPT-like (也被称作自回归Transformer模型)
BERT-like (也被称作自动编码Transformer模型)
BART/T5-like (也被称作序列到序列的 Transformer模型)

# Transformers是语言模型
上面提到的所有 Transformer 模型（GPT、BERT、BART、T5 等）都被训练为语言模型。
这意味着他们已经以无监督学习的方式接受了大量原始文本的训练。
无监督学习是一种训练类型，其中目标是根据模型的输入自动计算的。
这意味着不需要人工来标记数据！

这种类型的模型可以对其训练过的语言进行统计理解，但对于特定的实际任务并不是很有用。
因此，一般的预训练模型会经历一个称为迁移学习的过程。
在此过程中，模型在给定任务上以监督方式（即使用人工注释标签）进行微调。

任务的一个例子是阅读 n 个单词的句子，预测下一个单词。
这被称为因果语言建模，因为输出取决于过去和现在的输入。

另一个例子是遮罩语言建模，该模型预测句子中的遮住的词。

Encoder-only models: 适用于需要理解输入的任务，如句子分类和命名实体识别。
Decoder-only models: 适用于生成任务，如文本生成。
Encoder-decoder models 或者 sequence-to-sequence models: 适用于需要根据输入进行生成的任务，如翻译或摘要。

# 架构与参数
在本课程中，当我们深入探讨Transformers模型时，您将看到 架构、参数和模型 。 这些术语的含义略有不同：
架构: 这是模型的骨架 — 每个层的定义以及模型中发生的每个操作。
Checkpoints: 这些是将在给架构中结构中加载的权重。
模型: 这是一个笼统的术语，没有“架构”或“参数”那么精确：它可以指两者。为了避免歧义，本课程使用将使用架构和参数。

例如，BERT是一个架构，而 bert-base-cased， 这是谷歌团队为BERT的第一个版本训练的一组权重参数，是一个参数。
我们可以说“BERT模型”和”bert-base-cased模型.”

# Bias and limitations
如果您打算在正式的项目中使用经过预训练或经过微调的模型。
请注意：虽然这些模型是很强大，但它们也有局限性。
其中最大的一个问题是，为了对大量数据进行预训练，研究人员通常会搜集所有他们能找到的内容，
中间可能夹带一些意识形态或者价值观的刻板印象。

为了快速解释清楚这个问题，让我们回到一个使用BERT模型的pipeline的例子：

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])

['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']

# 使用分词器进行预处理
与其他神经网络一样，Transformer模型无法直接处理原始文本， 
因此我们管道的第一步是将文本输入转换为模型能够理解的数字。 
为此，我们使用tokenizer(标记器)，负责：
- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(token)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

# 高维向量
Transformers模块的矢量输出通常较大。它通常有三个维度：
- Batch size: 一次处理的序列数（在我们的示例中为2）。
- Sequence length: 序列的数值表示的长度（在我们的示例中为16）。
- Hidden size: 每个模型输入的向量维度。







